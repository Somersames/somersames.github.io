{"pages":[],"posts":[{"title":"2019年07月第一周总结","text":"这一周主要学习如下： Vue在使用ElementUI的时候有一个el-select标签，我们有一个需求就是需要在下拉列表的时候还需要过滤出特定的选项，但是ElementUI官方默认的过滤却只能支持 label 的筛选，所以这个时候就需要重新filter-method方法来过滤出所需要的选项。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;template&gt; &lt;div&gt; &lt;el-form v-model=&quot;selectData&quot;&gt; &lt;el-select v-model=&quot;selectData.value&quot; placeholder=&quot;请选择&quot; filterable :filter-method=&quot;search&quot; prop=&quot;value&quot;&gt; &lt;el-option v-for=&quot;item in options&quot; :key=&quot;item.value&quot; :label=&quot;item.label&quot; :value=&quot;item.value&quot;&gt; &lt;span style=&quot;float: left&quot;&gt;{{ item.label }}&lt;/span&gt; &lt;span style=&quot;float: right; font-size: 15px&quot;&gt;{{ item.value }}&lt;/span&gt; &lt;/el-option&gt; &lt;/el-select&gt; &lt;/el-form&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { data () { return { options: [{ value: '选项1', label: '黄金糕' }, { value: '选项2', label: '双皮奶' }, { value: '选项3', label: '蚵仔煎' }, { value: '选项4', label: '龙须面' }, { value: '选项5', label: '北京烤鸭' }], selectData: { value: '' } }; }, methods: { search (val) { let copyObj2 = JSON.parse(JSON.stringify(this.options)); if (val != null &amp;&amp; val !== '') { this.options = copyObj2.filter((item) =&gt; { if (item.value.indexOf(val) &gt; -1) { debugger; return true; } else { return false; } }); } else { this.options = copyObj2; } } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 但是后来又说要把选择的值给另一个字段…于是我就在search方法下面加了一行this.otherField = val。。后来前端把那一行给去除了。。。然后修改为如下代码。 123456&lt;el-select v-model=&quot;selectData.value&quot; placeholder=&quot;请选择&quot; filterable :filter-method=&quot;(val)=&gt;search(val,()=&gt;{this.otherField=val})&quot; prop=&quot;value&quot;&gt; search (val, callback) { callback(); // 一样的代码 } 后来就没管了，不过现在觉得这样写的话有好处也有坏处。好处就是对于如果只有很少的字段变动的话，这样改无疑的好的，因为可以避免在方法里面写太多的字段，但是一旦重复的多起来，我觉得在方法里面写起来比较好。 ElasticSearch的乐观锁机制（同步的坑）上周由于在做补偿机制的时候，需要对Es一些数据进行频繁的更新，但是在测试的时候 ES 经常会返回version conflict ，后来搜索了一下，发现 ES 对于每一个数据都有一个_version字段 而我们对一个数据如果频繁更新的话，就会导致ES的乐观锁生效，从而更新失败。如下： 由于我使用的ES版本是ES6.7.1，这时候只能通过if_seq_no来解决了，因为当我尝试使用外部版本号控制的时候，突然发现。。。ES官方竟然不推荐了 这部分后续写文章继续分析了。。。 ES的升级以及基础排序方法由于需要列表的数据进行综合查询然后排序，这个时候就只能手写排序方法了，好在ES的官方文档倒也详细，所以参照官方的文档，自己摸索了两个小时倒也写出来了。 12345String script = &quot;你的排序脚本&quot;;Script inline = new Script(script);SearchRequest searchRequest = new SearchRequest(&quot;你的索引&quot;);SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();sourceBuilder.sort(new ScriptSortBuilder(inline,ScriptSortBuilder.ScriptSortType.NUMBER).order(SortOrder.DESC)); Rabbitmq由于Mq环境需要统一下，所以这段时间也把自己负责的Mq又fanout改为direct了。改动起来到也没啥难度，就是在convertAndSend方法里面多加了一个routeKey参数而已 算法这周主要是在 Leetcode 上写了点Array的一些算法题，因为都是简单的题目，暂时还未遇到一些比较经典的题目 总结周三才想起来试试记录下，下周估计会详细点","link":"/2019/07/07/2019%E5%B9%B407%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93/"},{"title":"CountDownLatch详解","text":"CountDownLatchCountDownLatch 只有一个构造函数：CountDownLatch(int count) 其中 count 表示该信号量的数量，其中具体的实现类是 Sync，而 Sycn 又是继承自 AQS，实现了几个 AQS 的方法 在生产环境中可以用于分治思想，讲一些复杂的处理分成一些子任务，等所有处理任务处理完毕以后，主线程才会执行 还可以用于一些任务的流程检查，例如只有所有的检查都完毕以后，主线程才可以获取数据然后执行 具体的实现，是利用了 AQS 的一个 volatile 变量 state countDown该方法的具体流程就是将 state 通过 CAS 操作进行原子性的 -1，然后判断 -1 后的 state 是不是变成了 0 ，如果是的话，则会调用 doReleaseShared 方法唤醒后续的一个节点。 关于 doReleaseShared，因为是 AQS 来负责具体实现的，所以在这里先不做说明，在文章后半部分会进行说明 await如果调用 await 的时候，state 已经是0了，那么此时就会直接返回，当前线程也不会被挂起，而如果 state 不是0，那么当前线程就会被阻塞 如果调用该方法的时候，线程已经被设置了中断，那么会直接抛出 InterruptedException ##实现原理 CountDownLatch 通过重写 tryAcquireShared和tryReleaseShared来实现，前面说过 调用 await 方法以后，如果判断 state &gt; 0，则会调用 AQS 里面的 doAcquireSharedInterruptibly 方法 这个方法首先会将当前线程包装成一个 node 并且初始化一个 FIFO 队列，如果只有一个线程调用 await 方法，那么通过 addWaiter 方法会形成一个只有两个节点的队列 head -&gt; curr（当前线程） 回到第一个图，当队列形成以后，就会调用 tryAcquireShared 来判断该线程是否允许执行 tryAcquireShared这个方法的含义就是在共享模式下，判断当前线程是否获取到执行的条件，注释上的英文简单翻译就是：这个方法每次都应该被调用，如果这个方法提示失败了，则当前线程就应该被入队然后等待其他线程唤醒该线程 而一旦满足了执行条件 &gt;0 则会执行 setHeadAndPropagate 函数 这个函数也比较简单，首先就是将当前线程设置为 head 节点，然后判断是否需要唤醒后续节点，这里的 propagate 变量就是 tryAcquireShared 这个方法返回的，因此只要返回的是 &gt; 0，那么就可以认为当前节点的任务已经结束了，可以唤醒后续的节点了 在这个方法里面，如果只有一个线程调用了 await 方法，那么第一个 其实 h == tail，而且 h == head，所以无需唤醒任何人，直接return 即可 而如果是多个线程都调用了 await 方法，那么此时该 node 后面还有 node，此时就需要唤醒后续节点了，而在 addWaiter 方法用的是 Node(Thread thread, Node mode) 构造函数，因此 ws 的值是 0 ，然后执行一个 CAS 操作，将 ws 的状态改成 PROPAGATE 上面几个状态值的大致意思如下： 1：已经取消 2：线程需要被唤醒 3：线程在条件队列里面 4：释放资源的时候需要通知其他共享节点 回到前面的一个图，如果 tryAcquireShared 返回的 &lt;0，那么剩下的操作就是将该 node 的 ws 改成 -1，然后通过 LockSupport.park(this)挂起自己，最后等待被唤醒 疑问在这里可能有人有疑问了，既然countDown方法是判断 head !=null &amp;&amp; head == tail 来决定是否唤醒后续节点，那么假设在同一个时刻，countDown 将 state 改成 0 并且进入了如下的 if 判断，而另一个线程（线程2）正在入队操作，此时是否会出现另一个线程被挂起呢？ 答案是不会的，首先 head 和 tail 节点都是 volatile，所以可以保证可见性，而且在前面提到过入队的时候，会通过 tryAcquireShared 判断当前的 state，所以一旦判断为 0 了，那么就会直接return，所以不会阻塞. 那么在 tryAcquireShared 判断之后，state 改成 0 了，这样其实也没啥问题，还是不会导致阻塞，在这个情况下，head 和 tail 肯定是有值的，如果 head == tail，那么此时 countDown 的线程直接break. 如果head 不等于 tail，那么就说明此时的队列情况如下： head -&gt; node 并且线程二最终执行的方法也是 doReleaseShared，而最终都是会通过 head == h 返回出去，所以也没有线程安全问题","link":"/2021/12/04/CountDownLatch%E8%AF%A6%E8%A7%A3/"},{"title":"CyclicBarrier详解","text":"在上一篇的文章中有提到过 CountDownLatch ，其实 CyclicBarrier 也有异曲同工之妙，不过 CyclicBarrier 是等到所有的线程都到达一个点以后，然后再一起执行 有点像小时候一起去春游，必须等到所有的同学都到了学校，才能一起去坐车，不然就会一直等待。 构造函数CyclicBarrier 的构造函数有两个，分别如下： 123public CyclicBarrier(int parties) { this(parties, null);} 123456public CyclicBarrier(int parties, Runnable barrierAction) { if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction;} 其中的 int 表示当多少个线程到达指定的位置时，然后一起执行，而 Runnable 的含义暂且在这里不说明，文章后面会进行说明 如果要实现线程的同时执行，那么肯定是要进行线程间通信，如果忘记了不要紧，这里我给大家补一下。 Java 中的线程通信常见的通信机制，有如下几种： 锁 等待/通知机制 信号量 管道 join方法 回到 CyclicBarrier，如果由你来设计的话，会选择哪一种呢？ 所有的线程在到达一个点以后，都可以执行，是不是马上就会想到 Object 里面的 notifyAll() 呢？ 新建一个共享对象，然后每一个线程在调用 await 方法的时候，就会自动加入这个对象的等待池，最后一个到达的看下是不是成 0，是的话就通知所有的线程起来干活。 如果你这样想就错了，Object 里面的 notifyAll 是需要配合 synchronized 一起使用的，所以如果你用的是 Object 方法里面的 wait，那么就需要在外面使用 synchronized 了，那么问题就来了，大叔为啥不用呢？ 还是回到这个类的场景，我们的目标是最后一个线程在调用 await 方法到 0 以后，所有的线程就会被唤醒，这个类出现是在 JDK1.5，那个时候 synchronized 还是重量级所，是在 JDK 后续迭代的时候引入了「锁升级」这个概念，从而性能才有了提升。 所以猜测 Doug Lea 大叔就放弃了 synchronized，反而采用的是 Lock/Condition 这一套来实现的 原理awaitCyclicBarrier 里面的主要逻辑就在于 await 方法 当调用 CyclicBarrier 的 await 方法以后，首先会加一个重入锁，然后会判断当前的「屏障」是否被终止了。 被终止的原因有很多，例如线程被中断，最后执行 Runnable 失败等等。 继续往下走，当 count 减少至 0 的时候，表示该条件队列上的所有线程都可以执行 等于0 在这里有一个判断，还记得之前提到的第二个构造函数的第二个参数吗，就是这个 barrierCommand，当通过第二个构造函数传入进来一个 Runnable 方法以后，在 count 减少至 0 时（表示所有的线程已经到了指定位置），就会执行这个 Runnable，但是这里并不是起一个新的线程，而是直接在当前线程 run。 PS：顺带提一句如果父线程需要感应到子线程的话也有方法，不过这个更加简单粗暴了 在执行完 command 以后，就会直接调用 nextGeneration方法了，此时就会唤醒所有的 Condition 节点，然后重新初始化一个新的 Generation，而 Generation 的构造函数默认 broken 是 false，所以就相当于重置了 CyclicBarrier。 nextGeneration 有两个变量，分别是 parties、count，其中 parties 是从构造函数带过来的，count 则代表当前的剩余未到达指定地点的线程，每一次 count 为 0 以后，在重新初始化的时候，count 就会被重新赋值，继续往下走。 如果 Runnable 执行出错了，那么就会进入 finally，因为 ranAction 是 false，所以调用 breakBarrier 设置当前 CyclicBarrier 不可用，同时唤醒所有的 Condition 节点，因为 finally 里面已经设置了 broken 为 true，所以这里就会判处一个异常，不过条件队列中的其他线程都会被执行，只有最后一个线程会因为异常导致不会执行后面的代码 不为0当线程执行到这里不为 0 的时候，首先会判断是否设置了超时时间，最终会调用 trip 的 await 两个方法，只不过 awaitNanos(long nanosTimeout) 在等待 nanosTimeout 以后会自动的返回，而 await 则会一直等下去 其中 awaitNanos(long nanosTimeout) 本质上是使用 LockSupport.parkNanos(this, nanosTimeout) 来实现，只不过 Condition 在判断如果 nanoTimeout 时间小于 1ms 的时候，就会直接进行自旋。 条件队列Condition要理解条件队列，首先就需要理解 AQS 里面的队列，在 AQS 里面有两个队列，一个是基于 prev、next，而另一个就是基于 nextWaiter 其中，如果 CyclicBarrier 中的 count 如果不为 0 ，那么一直是通过 nextWaiter 进行连接，而一旦 count 变成 0，那么所有的 条件节点就会立刻变成另一个 Node 双向队列，然后挨个被唤醒 关于 Condition 为什么需要转换成 node 双向队列的思考首先如果在 Condition 中在实现一套 AQS 逻辑的话就会和现有功能冲突了，既然 AQS 已经有现有的功能，那就直接复用 AQS 的功能即可，不用再重复造轮子了 其实 Condition 的节点也是一个 Node，那么既然都是 Node，为什么 Condition 会单独的自己称之为一个链表呢？ 因为 Condition 和 ReentrantLock 是一起配套使用的，那么大家想过一个问题没有，ReentrantLock 的入队是通过 next 和 prev 来指向头尾指针的，如果 Condition 的 Node 也是用 next 和 prev 的话，会极大的增加 AQS 的每个方法的复杂度。 例如： 现在有十个线程在争抢资源，线程一抢到了资源，那么其他线程自然就会进入 AQS 的队列里面去，而此时线程一又向 AQS 队列里面塞了几个 Condition 节点，而恰巧就在其余九个 Node 之间塞了几个？是不是想骂人了。 唤醒的时候恰好遇到一个 Condition 节点，head 指针指向谁呢？后移的话，那万一条件队列全部满足了，需要执行，是不是又得从前向后挨个再次遍历。 所以这个猜测 Doug Lea 老爷子受不了这个混乱逻辑了，干脆就在 Condition 中用 firstWaiter 和 lastWaiter来表示头尾指针，然后在 AQS 中用 nextWaiter来表示一个单链表，然后条件队列自己玩去～～等需要唤醒的时候，直接转换成 AQS 的队列，复用现有逻辑就成","link":"/2021/12/06/CyclicBarrier%E8%AF%A6%E8%A7%A3/"},{"title":"ES7中大小写不敏感的模糊匹配","text":"在ES7.0中 如果要实现大小写的模糊查询，则首先必须要自定义 analysis，在自定义的 analysis 里面，如果是针对keyword类型的字段， analysis 要定义成 normalizer，而对于text类型的话，则需要为analyzer。如下演示的是normalizer类型的定义。 新建索引 123456789101112131415161718192021222324{ &quot;settings&quot;: { &quot;analysis&quot;: { &quot;normalizer&quot;: { &quot;self_normalizer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [], &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;asciifolding&quot; ] } } } }, &quot;mappings&quot;:{ &quot;properties&quot;:{ &quot;field_1&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;normalizer&quot;: &quot;self_normalizer&quot; } } }} 此时向ES中新增几条数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455{ &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: { &quot;value&quot;: 4, &quot;relation&quot;: &quot;eq&quot; }, &quot;max_score&quot;: 1.0, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;abc&quot; } }, { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;ABC&quot; } }, { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;aBC&quot; } }, { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;Abc&quot; } } ] }} 可以看到此时的对于field_1，在Es中的值大小写都有的，此时进行模糊查询： 1234567891011121314151617181920212223242526{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;wildcard&quot;: { &quot;field_1&quot;: { &quot;value&quot;: &quot;*a*&quot;, &quot;boost&quot;: 1 } } } ], &quot;adjust_pure_negative&quot;: true, &quot;boost&quot;: 1 } } ], &quot;adjust_pure_negative&quot;: true, &quot;boost&quot;: 1 } }} 返回值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455{ &quot;took&quot;: 9, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: { &quot;value&quot;: 4, &quot;relation&quot;: &quot;eq&quot; }, &quot;max_score&quot;: 1.0, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;abc&quot; } }, { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;ABC&quot; } }, { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;aBC&quot; } }, { &quot;_index&quot;: &quot;test_ascii&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;field_1&quot;: &quot;Abc&quot; } } ] }}","link":"/2020/03/20/ES7%E4%B8%AD%E5%A4%A7%E5%B0%8F%E5%86%99%E4%B8%8D%E6%95%8F%E6%84%9F%E7%9A%84%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D/"},{"title":"ElementUI使用rules遇到的一些问题","text":"这些天一直在踩Vue的坑…今天遇到的一个问题是在一个父组件中，将某些数据通过props传递给子组件，同时在子组件里面也有相应的一些rules规则，但是在实际的开发中，却发现子组件的rules并未生效…反而一直提示对应的 message，后来才发现是跟 ElementUI 的prop有关。 首先看一段代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;template&gt;&lt;div&gt; &lt;p&gt;该Demo是为了测试Vue中rule和prop的不同&lt;/p&gt; &lt;el-form :model=&quot;vehicles&quot;&gt; &lt;el-form-item label=&quot;公共汽车车轮个数&quot; prop=&quot;bus.wheel&quot; :rules=&quot;{required: true,message: '请输入公共汽车车轮个数'}&quot;&gt; &lt;el-input v-model=&quot;vehicles.bus.wheel&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;公共汽车司机驾照&quot; prop=&quot;bus.driver.license&quot; :rules=&quot;{validator: licenseCheck ,trigger:'blur'}&quot;&gt; &lt;el-input v-model=&quot;vehicles.bus.driver.license&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;公共汽车车轮个数&quot; prop=&quot;bus.driver.years&quot;&gt; &lt;el-input v-model=&quot;vehicles.bus.driver.years&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;car :car=&quot;vehicles.car&quot;&gt;&lt;/car&gt; &lt;/el-form&gt;&lt;/div&gt;&lt;/template&gt;&lt;script&gt;import Car from './Car';export default { components: { Car }, data () { return { vehicles: { bus: { wheel: null, driver: { license: null, years: null } }, car: { wheel: null, driver: { license: null, years: null } }, train: { wheel: null, driver: { license: null, years: null } } }, person: { child: { year: null } } }; }, methods: { licenseCheck (rule, value, callback) { let m = this.car; console.log(m); debugger; if (value != null) { if (value != 'A') { callback(new Error('必须A照')); } else { callback(); } } }, init () { console.log(this.car); } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 此时在这个组件中，一切都是正常的，但是一个完整的项目里面，是不可能将所有的元素都堆积在一个页面中，那样的话以后的维护就会非常的麻烦。所以此时就需要一个子组件，然后将父组件中一些数据传递至子组件。代码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;template&gt; &lt;div&gt; &lt;p&gt;小汽车子组件&lt;/p&gt; &lt;el-form-item label=&quot;小汽车车轮个数&quot; prop=&quot;wheel&quot; :rules=&quot;{required: true,message: '请输入公共汽车车轮个数'}&quot;&gt; &lt;el-input v-model=&quot;car.wheel&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;小汽车司机驾照&quot; prop=&quot;driver.license&quot; :rules=&quot;{validator: licenseCheck ,trigger:'blur'}&quot;&gt; &lt;el-input v-model=&quot;car.driver.license&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;小汽车车轮个数&quot; prop=&quot;driver.years&quot;&gt; &lt;el-input v-model=&quot;car.driver.years&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { props: { car: { type: Object, default: function () { return {}; } } }, methods: { licenseCheck (rule, value, callback) { let m = this.car; console.log(m); debugger; if (value != null) { if (value &lt;= 'D') { callback(new Error('必须C照以上')); } else { callback(); } } }, init () { console.log(this.car); } }, mounted () { this.init(); }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 当然在这个页面里面，一切都是可以正常输入的…就是rules无法使用。由于自己才是刚刚开始接触vue和ElementUI，所以对vue里面的一些使用技巧还不是很熟悉，这个时候看了下父组件里面的prop和v-model，发现prop都是比v-model少一个前缀…所以以为在子组件里面也是这样..其实后来才发新这个。 然后再去查看ElementUI的官网，发现 prop 表单域 model 字段，在使用 validate、resetFields 方法的情况下，该属性是必填的 string 传入 Form 组件的 model 中的字段 于时倒父组件中看了下，发现el-form确实有:model，然后参照了下ElementUI的介绍…突然想到是不是prop已经自动的将:model的对象带过来了…后来在子组件中进行了测试，修改后如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;template&gt; &lt;div&gt; &lt;p&gt;小汽车子组件&lt;/p&gt; &lt;el-form-item label=&quot;小汽车车轮个数&quot; prop=&quot;car.wheel&quot; :rules=&quot;{required: true,message: '请输入公共汽车车轮个数'}&quot;&gt; &lt;el-input v-model=&quot;car.wheel&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;小汽车司机驾照&quot; prop=&quot;car.driver.license&quot; :rules=&quot;{validator: licenseCheck ,trigger:'blur'}&quot;&gt; &lt;el-input v-model=&quot;car.driver.license&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;小汽车车轮个数&quot; prop=&quot;car.driver.years&quot;&gt; &lt;el-input v-model=&quot;car.driver.years&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { props: { car: { type: Object, default: function () { return {}; } } }, methods: { licenseCheck (rule, value, callback) { let m = this.car; console.log(m); debugger; if (value != null) { if (value === 'A') { callback(new Error('必须A照')); } else { callback(); } } }, init () { console.log(this.car); } }, mounted () { this.init(); }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 于是一切都正常了，后来为了测试是不是非要在el-form上加一个:model才能正常使用rules，所以就写了一个el-form测试。 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;template&gt;&lt;div&gt; &lt;el-form&gt; &lt;el-form-item label=&quot;&quot; prop=&quot;person.child.year&quot; :rules=&quot;{validator: childCheck ,trigger:'blur'}&quot;&gt; &lt;el-input v-model=&quot;person.child.year&quot; placeholder=&quot;请输入小孩的年龄&quot; &gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt;&lt;/div&gt;&lt;/template&gt;&lt;script&gt;import Car from './Car';export default { components: { Car }, data () { return { person: { child: { year: null } } }; }, methods: { childCheck (rule, value, callback) { debugger; if (parseInt(value) &gt; 16) { callback(new Error('请输入16以下')); } else { callback(); } }, init () { console.log(this.car); } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 然后发现在childCheck里面value总是获取不到值…一直是 undefinded ,然后再在el-form里面加上一个:modele…修改如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;template&gt;&lt;div&gt; &lt;el-form :model=&quot;person&quot;&gt; &lt;el-form-item label=&quot;&quot; prop=&quot;child.year&quot; :rules=&quot;{validator: childCheck ,trigger:'blur'}&quot;&gt; &lt;el-input v-model=&quot;person.child.year&quot; placeholder=&quot;请输入小孩的年龄&quot; &gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt;&lt;/div&gt;&lt;/template&gt;&lt;script&gt;import Car from './Car';export default { components: { Car }, data () { return { person: { child: { year: null } } }; }, methods: { childCheck (rule, value, callback) { debugger; if (parseInt(value) &gt; 16) { callback(new Error('请输入16以下')); } else { callback(); } }, init () { console.log(this.car); } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 然后就都好了… 所以以后还是得多看看官方文档…","link":"/2019/05/30/ElementUi%E4%BD%BF%E7%94%A8rules%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"},{"title":"Error是真的不可以被捕获的吗?","text":"在刚接触Java的时候经常听到的一句话便是在 Java 中，Exception 是可以捕获的，Error 是不可以捕获的。但是在随着学习的深入，会发现有些观点需要重新认识下了。Throwable 这个类是自 JDK1.0 开始就存在于 Java 的语言之中。 Throwable首先引用一段 Oracle 官方文档上对 Throwable 的介绍Java8 Thrwoable的介绍： The Throwable class is the superclass of all errors and exceptions in the Java language. Only objects that are instances of this class (or one of its subclasses) are thrown by the Java Virtual Machine or can be thrown by the Java throw statement. Similarly, only this class or one of its subclasses can be the argument type in a catch clause. For the purposes of compile-time checking of exceptions, Throwable and any subclass of Throwable that is not also a subclass of either RuntimeException or Error are regarded as checked exceptions.Instances of two subclasses, Error and Exception, are conventionally used to indicate that exceptional situations have occurred. Typically, these instances are freshly created in the context of the exceptional situation so as to include relevant information (such as stack trace data).太长，省略大部分了…… 简单翻译下就是，Throwable 是 Error 和 Exception 的父类，并且只能是 Error 和 Exception 的实例才可以通过 throw 语句或者 Java虚拟机 抛出异常。Exception 或者 Error 是在出错的情况下新创建的，从而将出错的信息和数据包含进去。另外在这个文档中还提到了一点就是当低层方法向高层方法抛出异常的时候，如果抛出的异常是受检查的异常，则 Error在看 Error 之前首先看一段代码，如下： 123456789public class ExceptionTest { private void test(){ try{ }catch (Error e){ e.printStackTrace(); } }} 可以看见 Error 是可以被捕获的，虽然 Java 的catch语句可以捕获 Error，但是在Error的官方文档上却做了说明：不推荐对Error进行捕获，也就是说 Error 虽然可以被 Java 语言捕获，但是Java官方却是不推荐对Error进行捕获的。具体文档如下： An Error is a subclass of Throwable that indicates serious problems that a reasonable application should not try to catch. Most such errors are abnormal conditions. The ThreadDeath error, though a “normal” condition, is also a subclass of Error because most applications should not try to catch it.A method is not required to declare in its throws clause any subclasses of Error that might be thrown during the execution of the method but not caught, since these errors are abnormal conditions that should never occur. That is, Error and its subclasses are regarded as unchecked exceptions for the purposes of compile-time checking of exceptions. 也就是说 Error 的出现表示程序出现了严重的非正常问题，并且在Java总处于一些原因，Error 被认为是未检查的异常。 异常异常分为受检查的异常和运行时异常，受检查的异常标志着程序在编译期间必须处理，常见的比如在读取一个文件的时候，Java语言必须要求抛出或者Catch FileNotFoundException。而运行时异常 Java 则是对其不做要求。 为什么Java不推荐捕获ErrorJava官方文档的解释说是 Error 的出现代表的是一些严重的非正常的错误。那么在 Java 的官方文档中介绍的 Error 有如下几种： AnnotationFormatError, AssertionError, AWTError, CoderMalfunctionError, FactoryConfigurationError, FactoryConfigurationError, IOError, LinkageError, SchemaFactoryConfigurationError, ServiceConfigurationError, ThreadDeath, TransformerFactoryConfigurationError, VirtualMachineError。 这些Error的出现代表的是程序已经不用进行处理了，比如 OutOfMemoryError，如果出现了这个错误的话，那么程序已经无法运行下去了，此时捕获就没有意义了。 总结所以并不是说Error是不可以捕获的，而是可以捕获的，但是 Java 官方并不推荐捕获Error。","link":"/2018/05/25/Error%E6%98%AF%E7%9C%9F%E7%9A%84%E4%B8%8D%E5%8F%AF%E4%BB%A5%E8%A2%AB%E6%8D%95%E8%8E%B7%E7%9A%84%E5%90%97/"},{"title":"Es6.X升级到Es7.x的变化","text":"ElasticSearch6.升级至ElasticSearch7.x的一些变化 由于最近需要将Es6.x升级至Es7，所以正好记录下在升级过程中遇到的一些问题，以便以后翻阅。 区别Es7.x系列中取消了Type在Es6系列之前，创建一个索引是需要index,type这两个缺一不可的，例如如下请求： 1234567891011121314151617181920 PUT localhost:9200/es_6 { &quot;mappings&quot;:{ &quot;index_type&quot;:{ &quot;properties&quot;:{ &quot;message&quot;:{ &quot;type&quot;:&quot;text&quot; } } } }}response:{ &quot;acknowledged&quot;: true, &quot;shards_acknowledged&quot;: true, &quot;index&quot;: &quot;es_6&quot;} 但是在 ES7 版本中，如果再使用这个 Json 串的话是会跑出一个异常的，如下： 1234567891011121314151617{ &quot;error&quot;: { &quot;root_cause&quot;: [ { &quot;type&quot;: &quot;mapper_parsing_exception&quot;, &quot;reason&quot;: &quot;Root mapping definition has unsupported parameters: [index_type : {properties={message={type=text}}}]&quot; } ], &quot;type&quot;: &quot;mapper_parsing_exception&quot;, &quot;reason&quot;: &quot;Failed to parse mapping [_doc]: Root mapping definition has unsupported parameters: [index_type : {properties={message={type=text}}}]&quot;, &quot;caused_by&quot;: { &quot;type&quot;: &quot;mapper_parsing_exception&quot;, &quot;reason&quot;: &quot;Root mapping definition has unsupported parameters: [index_type : {properties={message={type=text}}}]&quot; } }, &quot;status&quot;: 400} 那么此时在ES7版本中，建立 mapping 是不需要 Type 的，所以其索引修改为下： 1234567891011121314151617PUT localhost:9201/es_7{ &quot;mappings&quot;:{ &quot;properties&quot;:{ &quot;message&quot;:{ &quot;type&quot;:&quot;text&quot; } } }}{ &quot;acknowledged&quot;: true, &quot;shards_acknowledged&quot;: true, &quot;index&quot;: &quot;es_7&quot;} ES7.x中新建数据在 ES6 中由于有一个 Type 类型，因此在新建数据的时候都需要穿入一个Type，那么在 Es7 里面，由于 Type 被取消了，所以在 ES7 里面的新增就需要稍微修改下了。 1234567891011121314151617181920POST localhost:9201/es_7/_create/1{ &quot;message&quot;:&quot;a&quot;}{ &quot;_index&quot;: &quot;es_7&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: { &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 }, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1} 其实还有另一种写法： 1POST localhost:9201/es_7/_doc/2?op_type=create 剩下的一些改动可能就是新的业务上线需要对某些数据进行频繁的改动，而ES的乐观锁机制导致经常失败，这个问题得需要单独处理下","link":"/2019/07/07/Es6-X%E5%8D%87%E7%BA%A7%E5%88%B0Es7-x%E7%9A%84%E5%8F%98%E5%8C%96/"},{"title":"HTTP请求的ContentType以及其使用范围","text":"在发送请求的是时候最需要注意的是 Content-Type ，因为不同的 Type 对应的则是不同类型的数据，今天正好没什么事情，所以来总结下： application/json这种类型在最近几年用的比较多，主要是由于现在前后端分离，数据的请求方式可以由以前的表单提交逐渐偏向于Json的这种格式。所以这种 application/json格式的数据也就越来越多了。 这种数据现在一般使用的较多。例如，在使用Jquery的时候如果没有指定dataType的话，在后端可以设置 Content-Type 为 application/json 也是可行的。用 Java 则是 1response.setHeader(&quot;Content-Type&quot;,&quot;application/json&quot;); application/x-www-form-urlencoded这种就是最常见的一种表单提交方式，也就是常用的form提交了。如果通过表单提交并且想在表单中添加文件，则需要在 form 标签中加入 enctype属性，并且指定 enctype 为 multipart/form-data。 而表单默认的是 application/x-www-form-urlencoded 所以说其实当没有主动写这个属性的时候，浏览器已经帮你加上去了。 text/plain最常用的一个字符串传输类型","link":"/2018/04/26/HTTP%E8%AF%B7%E6%B1%82%E7%9A%84ContentType%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8%E8%8C%83%E5%9B%B4/"},{"title":"HashMap得一点总结","text":"HashMapp为什么在Hash的时候减1在Java的Hashmap中有如下代码： 12345678910111213141516171819final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 上面有一行是 first = tab[(n - 1) &amp; hash]) != null HashMap为什么在传入另一个Map时加一12345678910111213141516171819final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) { int s = m.size(); if (s &gt; 0) { if (table == null) { // pre-size float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); if (t &gt; threshold) threshold = tableSizeFor(t); } else if (s &gt; threshold) resize(); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); } } } 在这里加一的目的时向上取整，假设 s=10 ，那么乘以0.75之后便是7.5，加一之后再取整，便是最恰当的存储个数了。下面的判断则是说当前的map已有的key的数量是否达到了扩容的必要。如果需要扩容的话，则是直接出发扩容函数。 HashMap通过一个键来获取值123456789101112131415161718192021222324public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 其主要是通过 get(object key) 然后再Hash这个key，最后通过hash的key来获取其值。当获取到 hash 的 key 之后再通过tab[(n - 1) &amp; hash来获取保存的位置，当发现该位置为 null 的时候便直接返回 null，若不是 null 则判断第一个位置的键的 hash 值是不是要获取的 key 的 hash 相同，是的话便返回第一个节点，不是的话就欧安段链表的下一个是不是 null，如果不是的话就判断第一个节点的下一个节点是不是红黑树，是的话直接调用红黑树的查询方法，然后返回即可。这里的做法是让 table 第一个节点的 next 指向红黑树的头节点或者指向链表的下一个节点。 HashMap放入键和值123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } 首先还是一样，判断 tab 的长度是否是0，是的话就初始化 table，然后通过hash出来的值与 table 的长度进行与运算，找出最合适存放该key的位置，如果为 null ,则存入。然后判断hash是否相同，另外key是否相同，是的话跳出if，然后判断当前节点是否为null，是的话就执行 afterNodeAccess 将该节点移动到末尾。如果发现 p 是一个树节点的话，那么直接调用树的存入方法即可，不是的话就调用单链表的方法进行插入即可。最后还会判断是否达到了链表转树的阈值，达到了就可以转了。","link":"/2018/06/06/HashMap%E5%BE%97%E4%B8%80%E7%82%B9%E6%80%BB%E7%BB%93/"},{"title":"JDK1.7中的ConcurrentHashMap实现细节(二)","text":"简介在JDK1.7向JDK1.8升级的过程中，ConcurrentHashMap由原来的可重入锁和CAS锁直接被替换为synchronized关键字了，虽然说在功能上都是完全一致的，但是在这里一直都有一个疑惑，既然在1.7的使用过程中没什么问题，那到底是出于什么原因要将其替换呢。 JDK1.7中的ConcurrentHashMap在JDK1.7中，其结构是由一个可重入锁Segment数组和每一个节点下的HashEntry数组来实现的。结构图如下: 由于 segment 是一个锁，所以如果在并发的过程中，多个线程尝试向一个 segment 中的 HashEntry 进行插入的时候，只能有一个线程会获取到锁，其他的线程会被阻塞直至锁被释放，所以这个容器是一个并发安全的。 查看 put 方法的调用链的时候，可以发现最终都是调用的是Segment的put方法。 segment 类在 ConCurrentHashMap 中的变量是以一个数组的形式所存在的，由于segment继承了 ReentrantLock ，所以是它也是一个可重入锁，因此在JDK1.7里面，是通过 segment的 重入锁机制来实现并发的写入。同时也可以发现如果调用的是ConcurrentHashMap的无参构造函数的话，那么初始化Segment数组大小就是16，当然这个数组大小其实是可以被调整的，但是无论怎样进行调整，最终 segment 数组的大小永远都是2的n次方。 Segment在ConcurrentHashMap里面，一个segment就是一个HashEntry的数组，而一个HashEntry就是一个bucket。 但是需要注意的是，ConcurrentHashMap 默认的 segment 数组的大小是16，也就是说最多只可能有16个线程同时进行处理 当调用 put 方法的时候，会通过一个可重入锁的 CAS 操作来尝试获取该 segment 锁，如果获取到了则直接新建一个 Node 节点，如果还未获取到则直接调用scanAndLockForPut方法 123456789101112131415161718192021222324252627282930private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) { HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node while (!tryLock()) { HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) { if (e == null) { if (node == null) // speculatively create node node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else e = e.next; } else if (++retries &gt; MAX_SCAN_RETRIES) { lock(); break; } else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed retries = -1; } } return node;} scanAndLockForPut方法的作用在这个方法里面，会在 while 循环里面尝试 64 次，而且可以看到在这个循环语句里面有一些细节的操作。具体如下： 判断当前头节点 first 是否为 null，是的话则初始化 node 如果 first 不是 null，判断当前的 key 是否和 first 相等 如果 first 即不为null，并且当前的 key 又不和入参的key相同，则直接寻找其 next 节点，直至 next 为null，然后进行第一步 其实上面的一些步骤仅仅是该方法循环的第一步要做的，当上述三个步骤都进行完毕之后，首先会判断循环的次数是否已经大于 MAX_SCAN_RETRIES 如果是的话，则直接调用 lock 方法，如果不是的话则调用第三个判断。 第三个判断中会判断当前循环次数是不是偶数，如果是的话则会判断当前的头节点还是不是之前的first，如果不是的话则需要重新将新的头节点赋值给 first 然后将循环次数改成1，再次重试。 其实这个方法如果仔细看看，你会发现貌似没啥作用，因为返回的是 node，但是 node 一旦第一次被赋值之后，以后便不会做任何的更改，所以正如该方法的注释所说的一样，这个方法仅仅是为 JVM 做一个预热而已。 继续获取锁如果在 64 次以内还是未获取到该锁，则会调用lock方法，由于 ConcurrentHashMap 在初始化 segment 的时候，并未显式调用ReentrantLock的构造方法，而 ReentrantLock 又是默认初始化非公平锁，所以此时在 scanAndLockForPut 里面的 lock 其实调用的是 NonfairSync 里面的 lock 方法，即再次以非公平锁的方式来尝试获取锁 123456final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);} 当最后一次如果 CAS 操作还未获取到锁的时候，segment 就会调用acquire(1) 12345public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} 在这里可以看到的是 if 判断里面还是会再次尝试获取锁，当还未获取到锁的时候，就将该 Node 放入到 FIFO 队列的末尾，然后等待着被唤醒执行 从上面的一个流程不难看出，在 segment 首先会以可重入锁的方式来尝试性的获取锁，当没取到的时候会while循环64次做一个预热，如果在循环的过程中还是未获取到锁，则会进行两次CAS操作(分别在两个不同的方法里面)，如果最终还是无法获取到锁的话，那么此时就会将自己放入到 AQS 中的 FIFO 队列。 回过头来再看 segment 里面的第一行代码： HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); 那么可以看到的是，在JDK1.7里面，put方法如果在大量并发的情况下，如果要获取一个锁会进行非常多的操作，而且它默认的 segment 数组大小还是 16 ，也就是说map的所有键值，出现碰撞的概率不是 1/map.size()，而永远是 1/16。","link":"/2019/09/15/JDK1-7%E4%B8%AD%E7%9A%84ConcurrentHashMap%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/"},{"title":"JDK1.7和1.8中的HashMap区别","text":"Jdk1.7和1.8中，HashMap的一些关键点几乎重写了。 主要变更点：1. hash扰动算法在jdk1.7的时候，HahMap的hash扰动算法如下: 12345678static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);} 而在jdk1.8的时候，其hash算法已经修改为如下了: 1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} HashMap在放入一个元素的时候，首先会获取其HashCode，然后将 key 的 HashCode 进行扰动，避免同一个碰撞概率太大。如下例子。 假设一个key a 的 hashCode 为 1010 1010 1110 1101 1110 1111 1000 0110，如果不进行扰动，那么直接与table的长度 -1 进行与运，若table的长度是16，则计算的过程如下: 12341010 1010 1110 1101 1110 1111 1000 01100000 0000 0000 0000 0000 0000 0000 1111-----------------------------------------0000 0000 0000 0000 0000 0000 0000 0110 计算结果得出： a 的数组下标就是 6 但是这样就会出现一个问题，即每一次比较的都是最低位，如果某一个 key 和a的高位不同，低位却相同。每一次都是取最低位的几个数值进行运算，那么就会产生很严重的hash碰撞，所以就需要进行hash扰动以减少hash碰撞的概率。 以 jdk1.8 的扰动算法为例12345671010 1010 1110 1101 1110 1111 1000 01100000 0000 0000 0000 1010 1010 1110 1101 ^ &gt;&gt;&gt;16-----------------------------------------1010 1010 1110 1101 0101 0101 0110 10000000 0000 0000 0000 0000 0000 0000 1111 &amp;-----------------------------------------0000 0000 0000 0000 0000 0000 0000 1000 8 为什么进行扰动后，碰撞的概率会降低。具体的原因可以阅读这边文章An introduction to optimising a hashing strategy 2. HashMap的数据结构出现了变化在 jdk1.7的时候，HashMap是由一个数组和一个链表构成的。插入规则如下： 计算新插入的 key 的 hashCode，然后通过 hashCode 计算索引，找出该key在Entry中的位置，然后判断该下标是否有元素，如果没有则直接进行插入。 如果有的话就按照如下规则找出是否有相同的 key： hash相同且key相同hash相同且equals方法返回相同 若相同，则直接将当前的 value 替换原来的 value。 如果最后还是未发现相同的 key ，则新建一个Entry ，并将头节点设置为该Entry。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V put(K key, V value) { if (key == null) return putForNullKey(value); int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } void addEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; //找出原来table中的元素 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length); } //注意，此时将该节点是作为现在的table的头节点，原来的e则是新节点的next /** * Creates new entry. */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h; } 以下是在 jdk1.7 的时候第三种方式插入的极简版： 而在 jdk1.8 的时候，则是由一个数组加一个链表、红黑树组成之所以这样改进，是因为在极端情况下，如果所有的元素都 hash 到了一个下标，那么这样的话，HashMap在查找元素的时候就会退化到一个链表，其时间复杂度是O(n)。 为了应对这种情况，HashMap在1.8的时候会判断链表上的元素，如果超过了 8 个，就会将链表转化为红黑树。同时在 1.8 的时候，HashMap将链表的插入方式修改为尾插入。 提示：修改为尾插入是为了避免在并发的情况下出现链表成环（在jdk1.7之前会出现、同时HashMap并不适用并发场景下） 12345678910111213141516171819202122232425262728293031final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { //......省略相关代码 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { // 在未节点进行插入 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st。如果大于8，则会将链表转为红黑树 treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } 上述的变动最大点在于这两行代码： 1234p.next = newNode(hash, key, value, null);if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash);} 第一行是进行尾插入(1.7是头插入) 第二行是大于8会进行链表到红黑树的转化 jdk1.7采用头节点插入导致的链表成环虽然HashMap是一个非线程安全的，但是如果在 jdk1.7 版本中将HashMap用于并发环境下会出现什么情况呢？ 123456789101112131415161718192021222324252627282930313233void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); } // jdk1.7 扩容代码void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } 第一步 此时假设线程二已经将hashMap扩容完毕，但是线程一还在被挂起。 线程一执行，此时 e是为1，next却是2。 线程一第一次循环执行完毕，此时的e是2，然后`e.next是3。 线程一第二次循环执行完毕，此时的e是3，然后e.next是1，注意此时线程二中，已经将 3 的next指向了 1 ，所以此时e是3，然后 next 是1。 此时第三次循环完毕，由于e还不为空，于是进行第四次循环(主要原因是线程二已经将3的next指向为1)。 由于1的 next 是 null，所以循环结束。 jdk1.8的尾节点插入由上面的分析可以不难发现，造成链表成环的主要原因为：多线程下，头节点插入导致原来的链表的尾节点有了next，所以最后会多循环一遍，从而成环。 而在jdk1.8采用的为节点插入在多线程下，顶多是另一个线程把前面一个线程 resize 的过程再重复一遍，却不会再出现链表成环。 多线程下通用的bug虽然 jdk1.8 修复了链表成环这一个问题，但是多线程的情况下导致的数据丢失问题确实一直存在的。 所以不要尝试在多线程的情况下使用HashMap，如果需要用到Map结构的话，可以用CurrentHashMap或者HashTable","link":"/2019/04/08/JDK1-7%E5%92%8C1-8%E4%B8%AD%E7%9A%84HashMap%E5%8C%BA%E5%88%AB/"},{"title":"JDK1.8下ConcurrentHashMap的一些理解(一)","text":"在JDK1.8里面，ConcurrentHashMap在put方法里面已经将分段锁移除了，转而是CAS锁和synchronized ConcurrentHashMap是Java里面同时兼顾性能和线程安全的一个键值对集合，同属于键值对的集合还有HashTable以及HashMap，HashTable是一个线程安全的类，因为它的所有public方法都被synchronized修饰，这样就导致了一个问题，就是效率太低。 虽然HashMap在JDK1.8的并发场景下触发扩容时不会出现成环了，但是会出现数据丢失的情况。所以如果需要在多线程的情况下(多读少写))使用Map集合的话，ConcurrentHashMap是一个不错的选择。 ConcurrentHashMap在JDK1.8的时候将put()方法中的分段锁Segment移除，转而采用一种CAS锁和synchronized来实现插入方法的线程安全。如下代码： 12345678910111213141516171819202122232425262728293031/** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) { //省略相关代码 for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; synchronized (f) { //省略相关代码 } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } 可以看到在JDK1.8里面，ConcurrentHashMap是直接采用数组+链表+红黑树来实现，时间复杂度在O(1)和O(n)之间，如果链表转化为红黑树了，那么就是O(1)到O(nlogn)。在这里值得一提的是，ConcurrentHashMap会判断tabAt(tab, i = (n - 1) &amp; hash)是不是 null，是的话就直接采用CAS进行插入，而如果不为空的话，则是synchronized锁住当前Node的首节点，这是因为当该Node不为空的时候，证明了此时出现了Hash碰撞，就会涉及到链表的尾节点新增或者红黑树的节点新增以及红黑树的平衡，这些操作自然都是非原子性的。 从而导致无法使用CAS，当Node的当前下标为null的时候，由于只是涉及数组的新增，所以用CAS即可。 因为CAS是一种基于版本控制的方式来实现，而碰撞之后的操作太多，所以直接用synchronized比较合适。 ConcurrentHashMap在迭代时和HashMap的区别当一个集合在迭代的时候如果动态的添加或者删除元素，那么就会抛出Concurrentmodificationexception，但是在ConcurrentHashMap里面却不会，例如如下代码: 12345678910111213141516171819202122public static void main(String[] args) { Map&lt;String,String&gt; map = new ConcurrentHashMap&lt;String, String&gt;(); map.put(&quot;a&quot;,&quot;a1&quot;); map.put(&quot;b&quot;,&quot;b1&quot;); map.put(&quot;c&quot;,&quot;c1&quot;); map.put(&quot;d&quot;,&quot;d1&quot;); map.put(&quot;e&quot;,&quot;e1&quot;); Iterator&lt;String&gt; iterator = map.keySet().iterator(); while (iterator.hasNext()){ String it = iterator.next(); if(&quot;b&quot;.equals(it)){ map.remove(&quot;d&quot;); } System.out.println(it); }}控制台打印如下：abce 而当你把ConcurrentHashMap换成HashMap的时候，控制台就会抛出一个异常: 123456Exception in thread &quot;main&quot; abjava.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442) at java.util.HashMap$KeyIterator.next(HashMap.java:1466) at xyz.somersames.ListTest.main(ListTest.java:22) 原因在于ConcurrentHashMap的next方法并不会去检查modCount和expectedModCount，但是会检查下一个节点是不是为空 12if ((p = next) == null) throw new NoSuchElementException(); 当我们进行remove的时候，ConcurrentHashMap会直接通过修改指针的方式来进行移除操作，同样的，也会锁住数组的头节点直至移除结束，所以在同一个时刻，只会有一个线程对当前数组下标的所有节点进行操作。 但是在HashMap里面，next方法会进行一个check，而remove操作会修改modCount，导致modCount和expectedModCount不相等，所以就会导致ConcurrentModificationException 稍微修改下代码: 123456789101112131415161718192021public static void main(String[] args) { Map&lt;String,String&gt; map = new ConcurrentHashMap&lt;String, String&gt;(); map.put(&quot;a&quot;,&quot;a1&quot;); map.put(&quot;b&quot;,&quot;b1&quot;); map.put(&quot;c&quot;,&quot;c1&quot;); map.put(&quot;d&quot;,&quot;d1&quot;); map.put(&quot;e&quot;,&quot;e1&quot;); Iterator&lt;String&gt; iterator = map.keySet().iterator(); while (iterator.hasNext()){ if(&quot;b&quot;.equals(iterator.next())){ map.remove(&quot;d&quot;); } System.out.println(iterator.next()); }}控制台打印如下:bdException in thread &quot;main&quot; java.util.NoSuchElementException at java.util.concurrent.ConcurrentHashMap$KeyIterator.next(ConcurrentHashMap.java:3416) at com.xzh.ssmtest.ListTest.main(ListTest.java:25) 并发下的处理由于每一个Node的首节点都会被synchronized修饰，从而将一个元素的新增转化为一个原子操作，同时Node的value和next都是由volatile关键字进行修饰，从而可以保证可见性。","link":"/2019/05/13/JDK1.8%E4%B8%8BConcurrentHashMap%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3(%E4%B8%80)/"},{"title":"Java8中的流式简单总结","text":"在大量使用Java8中的流式操作之后，觉得用起来还挺舒服的，所以正好趁这个机会总结下。 使用Java8的lambda表达式的时候，需要先把集合转为一种流，也就是调用 stream 方法，但是 stream 却是 Collection 类里面的一个方法，也就是只有 Collection 的子类才可以使用，所以 Map 集合是使用不了的，同理，对于数组，可以通过Arrays.stream() 方法来讲数组转为一个Stream，这样也可以使用Stream里面的方法了， 将集合转为流下面介绍几个很常用的方法来介绍流式操作的便捷性。 123456List&lt;String&gt; stringList = new ArrayList&lt;&gt;();stringList.add(&quot;a1&quot;);stringList.add(&quot;b1&quot;);stringList.add(&quot;a2&quot;);stringList.add(&quot;b2&quot;);stringList.stream(); 这样我们就可以得到了一个流式操作，接下来就可以使用Stream里面定义好的方法了 filter该方法用于过滤我们设置的一些判断条件，如下： 12345678private void streamFilter(List&lt;String&gt; list){ List&lt;String&gt; result = list.stream().filter(item -&gt;{ if(&quot;a&quot;.equals(item)){ return true; } return false; }).collect(Collectors.toList()); } 这个只是一个普通的String数组遍历，可以看到如果我们通过Java8之前的代码写的话，会首先 new一个List，然后通过add方法来进行插入，这样虽然不会出现什么问题，但是会显得不是那么整洁，但是用流式操作的话，感觉会方便不少。 如果我们有多个条件要进行过滤的话，filter也是支持链式调用的。 1234567891011private void streamFilter(List&lt;String&gt; list){ List&lt;String&gt; result = list.stream().filter(item -&gt;{ if(&quot;a1&quot;.equals(item)){ return true; } return false; }) .filter(item -&gt; (item.startsWith(&quot;a&quot;))) .collect(Collectors.toList()); result.stream().forEach(item -&gt; System.out.println(item)); } mapmap常用于一些遍历条件，例如取出某些JavaBean的属性并作为集合。 12345public class Person { private String name; private Integer age; private String sex;} 将一个Person集合的所有姓名取出： 123456789101112131415//取出集合内所有的姓名List&lt;String&gt; nameList = list.stream().map(Person::getName).collect(Collectors.toList());//去重取出集合内所有的姓名List&lt;String&gt; nameDistinctList = list.stream().map(Person::getName).distinct().collect(Collectors.toList());//找出所有成年的人List&lt;String&gt; ageList = list.stream().filter( item -&gt; (Objects.nonNull(item.getAge()) &amp;&amp; item.getAge() &gt; 18) ).map(Person::getName).distinct().collect(Collectors.toList());//将所有已经成年的人按照年龄进行分组Map&lt;Integer, List&lt;Person&gt;&gt; ageSameList = list.stream().filter( item -&gt; (Objects.nonNull(item.getAge()) &amp;&amp; item.getAge() &gt; 18) ).collect(Collectors.groupingBy(Person::getAge)); findFirst在我的使用过程中，感觉这个方法配合枚举类相当的好用： 12345678910111213141516171819202122232425262728public enum ProvinceEnum { BEIJING(&quot;北京市&quot;,&quot;110000&quot;), TIANJIN(&quot;天津市&quot;,&quot;120000&quot;), HEBEI(&quot;河北省&quot;,&quot;130000&quot;), SHANXI(&quot;山西省&quot;,&quot;140000&quot;), ; private String cn; private String code; ProvinceEnum(String cn, String code) { this.cn = cn; this.code = code; } public String getCn() { return cn; } public String getCode() { return code; } public static String getCnByCode(String code){ return Arrays.stream(ProvinceEnum.values()).filter( item -&gt; (StringUtils.isNotEmpty(item) &amp;&amp; item.getCn().equals(code)) ).map(ProvinceEnum::getCn).findFirst().orElse(null); }} 由于其他的方法大致与这上面几个方法相似，所以就不再写demo了","link":"/2019/11/22/Java8%E4%B8%AD%E7%9A%84%E6%B5%81%E5%BC%8F%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/"},{"title":"Java中多线程使用经验","text":"在子线程中获取父线程的ThreadLocal如果想子线程想使用父线程的 ThreadLocal，那么父线程中的 inheritableThreadLocals 有值，这样子线程中的 init 方法就会自动的将父线程的 inheritableThreadLocals 设置为 ThreadLocal 但是因为子线程是在 init 方法中进行赋值的，所以如果子线程是由线程池创建的，那么该方法就又可能会失效，当线程池刚初始化完毕的时候，此时线程池中的还没有线程，当调用 execute 方法，此时就会 new 一个线程，那么这时候子线程是可以读取到父线程中 inheritableThreadLocals 的值 但是线程池中的线程是可以被复用的，所以后续如果线程不再创建的时候，那么子线程便不能再次获取父线程中的 inheritableThreadLocals，也就无法再将 ThreadLocal 进行父子线程的传递 如何感知到子线程的异常这种业务场景常见于一定时任务，尤其是一些需要多线程并发处理的job，理想情况是所有的job都执行成功了，但是如果有异常情况，那么需要及时的通知开发人员查看日志。从代码的可维护性上来说，每一个单独的子job最好只处理本job该处理的事情，而一些告警通知或者重试机制，则留给上层来处理。 实现的方式有两种： 子job抛出异常，然后父线程通过 future.get() 来获取子线程的异常信息 \b子线程抛出一个特定的异常，父线程感知到以后通知开发，并且决定业务的接下来走向 第一种的方式实现上比较繁琐，而且代码非常不美观，因此不太建议，当然如果多线程执行的job最后需要在主线程进行聚合操作，那么第一种方式还是可取的 下面来说下第二种，设置 UncaughtExceptionHandler，这种既可以为单独一个线程设置指定异常的抛出 也可以为一个线程组来设置，当线程没有设置的时候，会判断当前线程组是否有线程异常处理器 所以第二种方式其实也适用于线程池，线程池中有一个参数是 ThreadFactory，而这个类里面有一个参数是 ThreadGroup，因此在创建线程池的时候，如果自定义一个 ThreadFactory + ThreadGroup，那么是完全可以实现线程池中的线程异常统一处理 http://www.codebaoku.com/it-java/it-java-225099.html","link":"/2021/10/21/Java%E4%B8%AD%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C/"},{"title":"Java中的Optional","text":"Java中的Optional目前的项目中，JDK 版本已经全部是 1.8 以上了，项目函数式编程的代码多了起来，但是由于每个人的语言风格不同，如果滥用 JDK8 的各种函数式 API，会造成一定的坏味道 Optional网上最多的说法是为了避免 NPE（NullPointException），但是我个人觉得这和说法不太成立，例如下面的一个方法： 如果调用方不注意进行非空判断，那么一样会导致 NPE，所以在判断某一个变量是否为空的这一点上，其实 Optional 和 != null 没有本质上的区别。 Optional 的强大之处在与后续的 API 下面举一个例子： 如果你需要找出一个用户的子账户里面的家庭地址，如果不使用 Optional，可能是这种写法： 123456789if (user != null) { Account subAccount = user.getSubAccount(); if (subAccount != null) { Address address = subAccount.getAddress(); if (address != null) { return address.getDetail(); } }} 这种子弹型写法非常的不雅，而且后续也非常不利于维护，如果换成 Optional 的写法就简单多了。 12345String result = String address = Optional.ofNullable(u) .map(User::getSubAccount) .map(Account::getAddress) .map(Address::getDetail) .orElse(&quot;&quot;); Optional 会自动的判断当前 Stream 中的元素是不是为空，如果为空就终止，并且将 orElse 的值进行返回，在一定的程序上可以预防 NPE 的出现。 如果正常的代码中，有人忘记判断非空，例如 address 未判空就去取 detail 的值，就会导致了 NPE 业务实践上，到底方法的返回需不需要加 Optional，这个确实就见仁见智了，加 Optional，如果调用方直接 get 进行使用，IDEA 会显示黄色的 warning，在条件允许的情况下，再配合一个自动化的代码检测，那么可以很大程度上减少生产事故的发生。 但是这样导致的一个弊端就是代码的可读性降低，任何一个取值的地址，都必须调用 isPresent然后 get()，还不如 != null 来的实在。 所以个人建议是底层的方法不返回 Optional，最基础和最核心的方法，就返回对象或者报错就行了，业务的组合逻辑可以随意使用。 坑再说下团队在使用 Optioal 的一些注意事项。 orElse 这个方法并不是说只有第一个为 null，第二个才会执行，而是无论第一个有不有值，第二个都会执行，所以打印的结果就是 123getAgetB1 如果想实现第一个为null，就执行后面的方法，否则不执行的这个逻辑，就需要这个方法了。 orElseGet 此时如果 getA 返回的有结果，则 getB不会执行。 ifPresent这个方法是我用的最多的一个方法，主要是用来代替下面的代码： 123if(XXX ! = null){ YYY} 这种就可以优化为： 12Optional.ofNullable(XXX).ifPresent(YYY) 非常的方便。 最后Optional 其实是比较基础的一个类，JDK8 中最重要的还是 Stream，如果喜欢函数式编程，可以看下 vavr","link":"/2022/07/08/Java%E4%B8%AD%E7%9A%84Optional/"},{"title":"Java中的SPI机制以及应用场景","text":"SPI 全称是 Service Provider Interface，是 JDK1.5 新增的一个功能，允许不同的服务提供者去实现某个规定的接口，而且将具体的实现完全提供给使用方，允许使用方按需加载服务提供方的一些功能。 前言提到 SPI，就不得不提下 API，以 dubbo 为例，服务提供方对外提供一系列 API，而使用方是不用关心服务提供方是如何实现具体的业务逻辑，只需要通过 RPC 调用远程服务即可。 这样的好处就是 client 端不用关心服务端的具体逻辑，方便服务的水平扩展以及解耦。 SPI上面提到了 API 的相关知识，而 SPI 则是由服务方将具体实现提供给调用方，如何使用完全取决于调用方的具体业务逻辑，即调用方是可以拿到服务方的具体实现逻辑，然后决定是否使用，这有点像 Spring 的控制反转 实现该功能由如下两种方式： 直接在代码中硬编码 通过SPI来实现 第三方 jar我们日常用 maven 将第三方 jar 导入到我们的项目中，大致思想和这个类似，都是将具体实现引入到 client，由 client 来决定使用的方式。但是通过 maven 导入的有一个缺点，就是代码中存在硬编码，即需要 import 第三方包的类全路径，以后如果要替换具体实现，那么所有 import 了旧包的地方就需要全部修改一遍。 例如现在有一个接口如下： 123public interface Buy { Boolean buy(Integer var1);} 而提供方则在自己的项目中实现了具体的逻辑： 123456public class AliPayBuy implements Buy { public Boolean buy(Integer num) { System.out.println(&quot;buy&quot; + num + &quot;with AliPayBuy&quot;); return true; }} 下面就来看看 maven 和 SPI 是如何具体实现的。 首先项目的整体结构如下，然后分别 deploy 到自己的私服中 普通使用如果不想通过 SPI 来调用，那么直接在项目中 new 一个也是可以的。 123456public class SimpleTest { public static void main(String[] args) { Buy buy = new AliPayBuy(); System.out.println(buy.buy(1)); }} 这种硬编码会导致后续完全没有扩展性，以后如果需要将支付改为其他方式，那么所有涉及到 AliPayBuy 的地方全部都得替换。 SPI而 JDK 的作者为了解决这个问题，引入了 SPI 机制，具体来说就是定义了一个文件夹「META-INF/services」，调用方规定一个接口，提供方则在自己的项目中实现具体的逻辑，然后在自己的项目中将具体实现放置在「META-INF/services」即可。 12345678public class SPITest { public static void main(String[] args) { ServiceLoader&lt;Buy&gt; shouts = ServiceLoader.load(Buy.class); for (Buy s : shouts) { System.out.println(s.buy(1)); } }} 而采用 SPI 机制，可以避免在代码中直接引入第三方 jar，ServiceLoader.load 加载的正是之前 deploy 进私服的 alipay jar 包。 META-INF/servicesJDK 中规定，只有在这个文件夹中的 SPI 才会被加载，因为 JDK 已经将该路径硬编码到代码中了，而文件的命令也很有规范。 文件名称必须是接口的全路径名称（大小写也必须一致）而文件里面的内容就是实现该接口的类的全路径名称，例如 xyz.somersames.Buy 这个里面的内容就是 1xyz.somersames.AliPayBuy 双亲委派机制通过 SPI 机制加载的类是会破坏双亲委派机制的，因为按照双亲委派的机制，当 classLoader 加载某一个类的时候是一层一层往上递增的，然后再逐级往下，但是 SPI 机制是通过 thread.contextClassLoader 直接加载了具体的实现类。虽然说原生的 classLoader 是按照双亲委派机制在加载类，但是 thread.contextClassLoader 这里由于极大的灵活性可能会导致被用户的自定义 classLoader 覆盖，而如果用户自定义的 classLoader 不按照规范来，那么就直接破坏了双亲委派机制了 其二，因为 JDK 的 SPI 接口一般是位于 rt.jar 中，按照双亲委派机制，应该由 BootstrapClassLoader 加载，但是其实现类却是位于 classPath，由 AppClassLoader 加载，所以这也算是破坏的一种","link":"/2021/08/04/Java%E4%B8%AD%E7%9A%84SPI%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"},{"title":"Java的volatile和MESI协议","text":"使用过Java多线程的都知道，volatile可以确保多线程永远都可以读取到最新的变量。但是却无法保证一个操作的原子性关于这部分可以先从硬件部分说起： 硬件单核CPU在现代计算机中，CPU的速度是远远快于内存的，如果不加以任何处理，此时CPU就会一直在等待内存的IO，从而导致一些资源的浪费。所以就有了高速缓冲区(cache)，但是在Cache里面肯定是不会将内存的所有数据都拷贝一份，因为cacahe只有几十kb的大小。 这就造成了一个现象，也就是说每一个核心都会有一个共享的内存区域和一个自己Cache区域。 多核CPU随着现代技术的发展，单核CPU已经被淘汰了，取而代之的是多核CPU，而且现代的CPU已经都是多级缓存了，大部分是三级缓存。在多核心的CPU中，每一个核心都会有自己的Cache，如果某一个核心修改了自己缓存区的数据，那么就会造成自己的缓存区和内存中数据不一致 所以这时候就会有一个MESI协议(缓存一致性) MESI协议MESI协议简单来讲，就是为了确保每一个核心的缓存数据都是一致的。当有一个核心对某一个变量做了修改之后，就会通知到其他的核心，然后使其失效。 Javavolatile在了解volatile之前，首先需要了解Java的内存模型JMM，在Java内存模型中，堆是所有线程共享的 图中即是JMM模型对硬件的简单映射，在Java里面，对象都是在堆中，而在线程的私有栈空间，每一个线程只能对变量进行修改或者赋值操作。 volatile首先多线程的本质其实是多个线程轮流执行，在时间片轮转调度算法里面，每一个线程都会获取到一个执行时间，执行时间结束，就会轮到其他的线程执行。 可见性考虑一下场景，如果内存中有一个值x=1，线程一和线程二都会对x进行一个x+=1的操作。那么无论哪一个线程率先执行完毕，都会将最新的结果集写入到内存中，这样其他线程在读取的时候都会读取到最新的数据。这样就保证了任意一个线程在读取这个数据的时候都会一个最新的值。 如果不加volatile限制，则可能每一个核心操作完成之后只是将值写入到自己Cache区，而不刷新内存。什么时候刷新完全是随机的 非原子性但是如果出现一下情况： 线程一读取x=1 线程一正在执行+1的操作 线程一的执行时间结束，此时线程二开始执行 线程二在执行的时候读取内存数据也是x=1， 于是线程二将x+1的结果写入内存(由于volatile的限制，CPU在执行完毕的时候必须强制写入内存) 线程一将x+1的结果写入内存 所以volatile虽然具有可见性，但是却无法保证一个操作的原子性，如下上测试代码：新建一个增长类: 123456public class AInstance { public volatile static int a =0; public static void increase(){ a++; }} 然后新建一个测试线程： 12345678public class AThread extends Thread { @Override public void run() { for(int i =0 ; i&lt;100 ;i++){ AInstance.increase(); } }} 最后启动类： 123456789public class ATest { public static void main(String[] args) { for(int i =0 ;i&lt;1000; i++){ AThread aThread = new AThread(); aThread.start(); } System.out.println(AInstance.a); }} 如果使用Volatile修饰的变量可以保证原子性的话，那么a一定会是100000,但是测试结果一直是随机数字 131600或者其他的 MESI协议和volatile虽然MESI协议可以保证缓存一致性，但是如果有一个线程在正要进行+1的时候被挂起了，而另一个线程则正好执行完了x+=1的操作，此时回到第一个线程继续执行，这样就会导致一个错误的数据。如下： volatile所以虽然有MESI保证缓存的一致性，但是在赋值操作之前已经读取了，所以此时并不会再次读取内存 这就是volatile只能保证内存的可见性，但是无法保证原子性的问题 单例模式正是由于volatile的这个特性，所以在多线程中的单例模式都会在获取实例的方法上加上一个synchronized关键字，以确保只会生成一个对象。 总结在多线程的场景下，使用volatile需要注意的是原子性操作的问题，否则就会造成程序的数据错误","link":"/2019/01/05/Java%E7%9A%84volatile%E5%92%8CMESI%E5%8D%8F%E8%AE%AE/"},{"title":"Java通过反射用指定构造器初始化","text":"Java通过反射用指定构造器初始化首先， 一般来讲在Java中初始化一个类是通过new来操作的， 但是有一种情况却不适合这种new操作，那就是通过配置文件来进行实例化操作。 例如，在Spring中，需要加载配置文件中的类，这是比较常见的配置。 那么在Spring启动类中如何将这个类加载进容器中呢，显然进行new操作是不太现实的。 这时候就需要Java的反射操作了，Java的反射操作一般来讲有两种：分别是Class.forName()和classLoader.loadCLass() 最后都是通过newInstance() 来进行初始化，但是在这里却发现假设反射的类中含有带参数的构造器，那么此时这个newInstance()就会抛NoSuchMethodException ，这是因为newInstance()因为不加参数所以调用的是默认构造器，而反射类中已经包含了带参数的构造器，所以无不带参数构造器，遂抛出异常。 但是此时newInstance()是加不了参数的，所以若需要通过制定构造器来进行反射的话需要一个类叫Constructor， 新建一个实体类： 123456789101112131415161718192021package reflec.cglib_test.ConstructTest;public class Entity1 { static { System.out.println(&quot;static初始化&quot;); } public Entity1() { } int id; String name; public Entity1(int id, String name) { this.id = id; this.name = name; } public void say(){ System.out.println(this.id); System.out.println(this.name); }} 反射测试： 123456789101112131415161718package reflec.cglib_test.ConstructTest;import java.lang.reflect.Constructor;import java.lang.reflect.InvocationTargetException;public class Test1 { public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException, ClassNotFoundException {// Entity1 en2=Entity1.class.getConstructor(int.class,String.class).newInstance(1,&quot;2&quot;);// Class.forName(&quot;reflec.cglib_test.ConstructTest.Entity1&quot;).newInstance();// Test1.class.getClassLoader().loadClass(&quot;reflec.cglib_test.ConstructTest.Entity1&quot;);// en2.say(); } private static void do1(Object object){ System.out.println(object.getClass().getName().toString()); }} 在上面的Test1中，下面两行在newInstance()中添加参数的话是会提示出错的。那么如何调用哪个带参数的构造器呢？ 这就是Constructor 类的功能了，他可以通过Class.getConstructor()来选择参数，在这里需要注意的是int及其他的java基本数据类型都是原生的类，非封装类。之后再newInstance()中输入参数既可以反射调用了。","link":"/2018/03/12/Java%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E7%94%A8%E6%8C%87%E5%AE%9A%E6%9E%84%E9%80%A0%E5%99%A8%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"title":"Java通过getResourceAsStream()读取不到文件的原因","text":"首先出现这个原因的时候，需要弄清楚工程目录和编译目录。 工程目录以IDEA为例，在IDEA里面，我们写代码的地方就是一个工程目录，常见的例如src下面的各种java文件，这种目录就可以称之为一个工程目录，例如如下所示： 工程目录主要存放的是一些配置文件或者一些java文件之类的，而经jvm编译之后的目录便是编译目录了 编译目录编译目录主要用于存放java编译后的class文件，也就是我们运行的文件。众所周知，java是一种跨平台语言，所以jvm实际运行的是java变异之后的class文件。 当知道了这个之后便会理解为什么会通过getResourceAsStream()读不到文件了。 getResourceAsStream()翻开Java的官方文档，可以看到getResourceAsStream()是ClassLoader的一个方法， 12static InputStream getSystemResourceAsStream(String name)Open for reading, a resource of the specified name from the search path used to load classes. 那么一般获取Java配置文件的代码如下： 1InputStream inputStream = DemoTest.class.getClassLoader().getResourceAsStream(); 这个时候程序运行起来了，那么她就不会去工程目录下寻找配置文件了，例如在如下工程里面运行如下代码： 123456789101112public class DemoTest { public static void main(String[] args) { InputStream inputStream = DemoTest.class.getClassLoader().getResourceAsStream(&quot;mybatis-config.xml&quot;); if (inputStream == null){ System.out.println(&quot;获取异常&quot;); }else{ System.out.println(&quot;获取到了文件&quot;); } }}程序打印如下：获取异常 这个时候就会出现getResourceAsStream获取不到文件了，那么假如将mybatis-config.xml复制到target的目录下面去呢? 再次运行该代码，控制台打印：获取到了文件。 所以遇到了这个情况的话一般就是工程目录和编译目录缺少文件了。 如何找到ClassLoader的获取文件目录呢只需在Resource类下面debuggetResourceAsStream，然后打开loader即可，找出domain属性就可以看到了。 1234567public static InputStream getResourceAsStream(ClassLoader loader, String resource) throws IOException { InputStream in = classLoaderWrapper.getResourceAsStream(resource, loader); if (in == null) { throw new IOException(&quot;Could not find resource &quot; + resource); } return in; } 可以看到那个就是一个获取的编译目录。","link":"/2018/09/10/Java%E9%80%9A%E8%BF%87getResourceAsStream()%E8%AF%BB%E5%8F%96%E4%B8%8D%E5%88%B0%E6%96%87%E4%BB%B6%E7%9A%84%E5%8E%9F%E5%9B%A0/"},{"title":"Java集合学习之HashSet","text":"简介在一般的使用中，HashSet经常用于数据的去重，例如我们有一个List，这个List里面有一些重复的数据，于是我们便可以这样操作 1234567List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(&quot;a&quot;);list.add(&quot;b&quot;);list.add(&quot;c&quot;);list.add(&quot;a&quot;);Set&lt;String&gt; set =new HashSet&lt;&gt;();set.addAll(list); 此时，在Set里面，只会有一个a元素。 底层其实HashSet的底层是一个HashMap，HashSet的去重使用了HashMap的Key。如图所示： 12345678910111213141516/** * Adds the specified element to this set if it is not already present. * More formally, adds the specified element &lt;tt&gt;e&lt;/tt&gt; to this set if * this set contains no element &lt;tt&gt;e2&lt;/tt&gt; such that * &lt;tt&gt;(e==null&amp;nbsp;?&amp;nbsp;e2==null&amp;nbsp;:&amp;nbsp;e.equals(e2))&lt;/tt&gt;. * If this set already contains the element, the call leaves the set * unchanged and returns &lt;tt&gt;false&lt;/tt&gt;. * * @param e element to be added to this set * @return &lt;tt&gt;true&lt;/tt&gt; if this set did not already contain the specified * element */public boolean add(E e) { return map.put(e, PRESENT)==null;} HashSet的add方法是向一个map里面放入元素，而HashMap则是不允许键重复，所以就可以确保在HashMap上的键都是不重复的。 HashMap是如何确保每一个对象都只有一个的呢?首先当调用HashSet的add方法的时候，其实是调用HashMap的put方法， 123456789101112131415/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } 这个方法无非是先将这个key进行hash，然后再调用putVal方法进行保存， 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } 这个是HashMap的底层方法，当首次传入值的时候， if ((tab = table) == null || (n = tab.length) == 0){ n = (tab = resize()).length;} 如果table未空就进行初始化，如果不为空则执行下面的代码 1234567891011121314151617181920Node&lt;K,V&gt; e; K k;if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; }} 在HashMap里面，有一个数组table存放着所有的key，而 HashMap 定位下标的方式就是通过(n - 1) &amp; hash。 当HashMap发现该下标的值是null，就会直接将入参的key和value疯转成一个Node保存进去，如果发现不是null，则HashMap认为发生了HashMap碰撞，于时进行如下判断: 如果新传入的一个key在HashMap中已经存在，则HashMap会直接将旧的key的value替换掉。否则就会进行新增HashMap在判断一个key是否相等会采取以下措施： 如果两个Key的hash不同，则HashMap直接会判断key不等 Key和hash完全相同 第一次传入key=a，Hash值是1，value是100;第二次传入key=a，Hash值是1，value是101; 此时hashMap会认为新传入的key已经存在，所以会将旧的value替换为新的value 产生Hash碰撞，也就是两个key的hash都是一样的，那么就会通过key是否相同或者equals方法判断对象是否相等了2. Key不同，hash相同 HashMap会判断传入的key、以及key的hash值，如果相等则认为该键以相等，例如: 第一次传入key:a，Hash值是1;第二次传入key:b，Hash值是1; 这个时候由于key不同，hashMap还会通过equals继续判断。 由于第二次传入的key是b，但是他们的Key并不相等，此时HashMap就会调用他们的equals方法，如果通过equals方法判断的是相同对象，则也会认为是同一个key。 此时如果判断新增的key确实不存在就会在当前的table位置通过链表地址方法辛增一个key了。 那么到这里就可以看到，其实HashSet就是完全的利用了HashMap的键的特性来进行去重。 Iterator方法12345678910/** * Returns an iterator over the elements in this set. The elements * are returned in no particular order. * * @return an Iterator over the elements in this set * @see ConcurrentModificationException */ public Iterator&lt;E&gt; iterator() { return map.keySet().iterator(); } 其实都是利用了hashMap的一些方法来实现 线程不安全由于HshMap是非线程安全的，自然HashSet也不是一个线程安全的。测试代码如下: 1234567891011121314151617181920212223242526272829public class HashSetTest implements Runnable{ Set&lt;Integer&gt; set =new HashSet&lt;Integer&gt;(); public void run() { for(int i=0 ;i&lt;10000;i++){ set.add(i); } }}public class HashSetTestMain { public static void main(String[] args) throws InterruptedException { HashSetTest hashSetTest =new HashSetTest(); Thread t1 = new Thread(hashSetTest); Thread t2 = new Thread(hashSetTest); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(hashSetTest.set.size()); Set&lt;Integer&gt; set =new HashSet&lt;Integer&gt;(); for(Integer i :hashSetTest.set){ if(set.contains(i)){ System.out.println(i); }else{ set.add(i); } } }} 可以看到打印出来的结果会多于10000，这是因为在上面也说到过的，HashMap在判断一个key是否相同以及后续新增节点的时候并非是一个原子性的，所以就有可能会导致t1线程刚好判断10不在hashMap中，准备新增一个节点为10。结果此时t1被挂起，t2执行，但是t2也判断了10不在hashMap中，也准备新增，那么此时就会出现新增了两个一摸一样的Key。这样就会导致Set集合中出现了重复的数据。","link":"/2019/04/04/Java%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E4%B9%8BHashSet/"},{"title":"Json的底层实现一览","text":"在开始了解Json的原理之前，首先看一段代码，在这里以阿里的FastJson为例。 12345678public class JsonRun { public static void main(String[] args) { JSONObject jsonObject =new JSONObject(); jsonObject.put(&quot;id&quot;,&quot;a&quot;); jsonObject.put(&quot;name&quot;,&quot;b&quot;); System.out.println(jsonObject.toJSONString()); }} 当看到上述代码的时候，可能一般的程序员都会想到的是输出为如下Json串 {“id”:”a”,”name”:”b”}但是运行这段程序，你会发现控制台打印出来的是如下代码：{“name”:”b”,”id”:”a”} 那么为什么会出现这种情况呢，翻开FastJson的源码便知道了，首先定位到 JsonObject 这个类的构造函数，如下： 1234567public JSONObject(int initialCapacity, boolean ordered){ if (ordered) { map = new LinkedHashMap&lt;String, Object&gt;(initialCapacity); } else { map = new HashMap&lt;String, Object&gt;(initialCapacity); } } 这里的 ordered 为一个构造参数，表示的是是否按照顺序添加，此处先不管，然后可以发现在阿里的FastJson中，其实默认的Json实现是一个Map，那么对于LinkedHashMap来讲，它是一个map和双向链表的整合体，所以在LinkedList中，每一个Node都会有一个前指针和一个后指针 HashMapLinkedHashMap 是一个HashMap的变种，大家都知道，一个HashMap是由一个桶和一个桶后面的节点组成的，而桶其实是一个数组，每一个桶的索引所对应的值都是由Hash()函数计算得出的。那么这样就会导致桶的元素是一个乱序的存储的，例如在本段代码中的id和name，它们所在的桶索引可能是: 这样就导致了一个问题，就是Json的键的顺序是无法保证的，那么既然HashMap是无法保证的，为什么LinkedHashMap却可以保证顺序。 LinkedHashMap翻开LinkedHashMap的源码可以发现，在其节点类里面，LinkedHashMap在 HashMap的Entry基础上又添加了一个before和after指针， 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); } } 那么这两个指针就是双向链表的指针。有了这两个指针之后，每一个新插入的节点都会知道他的前驱结点和后置节点，那么对于LinkedHashMap的插入顺序就会有保证了。所以其对应的数据结构如图： 在这个结构里面，桶索引是id的第一个节点是一个头节点，在新插入name的时候，LinkedHashMap会将head节点的after指针指向name，所以虽然这是一个HashMap，但是它的顺序还是可以保证的。 LinkedHashMap的迭代区别于HashMap以索引的方式进行迭代，LinkedHashMap是以链表的指针进行迭代的，如以下代码所示： 12345678910111213141516171819202122abstract class LinkedHashIterator { LinkedHashMap.Entry&lt;K,V&gt; next; LinkedHashMap.Entry&lt;K,V&gt; current; int expectedModCount; LinkedHashIterator() { next = head; expectedModCount = modCount; current = null; }final LinkedHashMap.Entry&lt;K,V&gt; nextNode() { LinkedHashMap.Entry&lt;K,V&gt; e = next; //next就是head节点 if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); current = e; next = e.after; //此处每一次的迭代都是链表的after return e; } 可以看到在每一次迭代的时候LinkedHashMap都是以链表的next节点作为下一个迭代，那么HashMap呢？ HashMap的迭代1234567891011121314151617181920212223242526272829abstract class HashIterator { Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slotHashIterator() { expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) { // advance to first entry do {} while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); } }final Node&lt;K,V&gt; nextNode() { Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) { do {} while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); } return e; } 注意这一段代码 if (t != null &amp;&amp; size &gt; 0) { // advance to first entry do {} while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); } 这一段代码的作用是找出table[]中第一个不为null的桶，所以其实HashMap的迭代就是依据桶中的顺序来的，但是LinkedHashMap则是按找链表的顺序来的。 总结其实每一个java的设计都是很精妙的…","link":"/2018/09/06/Json%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E4%B8%80%E8%A7%88/"},{"title":"leetcode的一道字符串题目：最长不重复的子字符串长度","text":"找出最长的不重复的子字符串首先一拿到这个题目的时候想到利用set集合来存储子字符串，当发现含有重复的字符的时候就将这个set清零。这个做法有一个明显的缺点就是当子字符串中的某一个字符和后面的重复的话，那么后面不行同的也会被清楚：例如dfvfab，在这个里面vfab应该是最短的子字符串，但是这时候这个方法会输出错误的值。 思路：自己想不到解法之后就参考了Solution2，这个方法想了下也挺好的，遂记录下：该方法是设置两个游标，一个在前，一个在后(定义为i,j)，当前面的一个发现在set集合中含有重复的字符，那此时停止i,然后j自增，当发现在set集合中已经将重复的那个字符剔除之后，此时在i自增 123456789101112131415161718192021222324252627282930313233public int lengthOfLongestSubstring(String s) { //错误的思路，以为存入set就可以 // if (s == null || s.length() == 0){ // return 0; // } // Set&lt;Character&gt; set =new HashSet&lt;&gt;(); // int max=0; // for (int i =0 ;i&lt; s.length() ;i++){ // char c = s.charAt(i); // if (set.contains(c)){ // set.clear(); // } // set.add(c); // if (set.size() &gt; max){ // max = set.size(); // } // } // return max; if(s == null || s.length() ==0){ return 0; } Set&lt;Character&gt; set =new HashSet&lt;&gt;(); int i=0,j=0,result=0,n=s.length(); while(i &lt; n &amp;&amp; j &lt;n){ if(!set.contains(s.charAt(j))){ set.add(s.charAt(j++)); result=Math.max(result,j-i); }else{ set.remove(s.charAt(i++)); } } return result; } 与求最长的子字符串（朴素求解法）相比较：朴素求解法的一般步骤是已经给出了子字符串，然后判断是否是另一个字符串的子字符串。那么此时既然已知子字符串的话，其步骤就是通过子字符串依次比较 与判断链表成环相比较：判断链表成环也有一个快慢指针的方法，但是在那个快慢指针中，快慢指针都是同时在变化的","link":"/2018/03/14/LeetCode%E7%9A%84%E4%B8%80%E9%81%93%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%A2%98%E7%9B%AE%EF%BC%9A%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%95%BF%E5%BA%A6/"},{"title":"leetcode上两道判断n次方的题目","text":"这两道题目都是判断一个数字是不是2(第一题)，3(第二题)的n次方，在做第一题的时候思路基本上和标准解法想法相同，但是在做第二题的时候，看到了许多比较有创意的解法，所以记录下 判断一个数是不是2的n次方解法一这个解法也就是我第一次就想到的一个解法，就是做 &amp; 运算，因为一个数字若是2的n次方，那么很明显就是这个数字的2进制肯定只会有一个1，例如：32=100000 ,64 =1000000。所以只需要判断 n 与 n-1 做一个&amp; 运算就可以知道了。 123456public boolean isPowerOfTwo(int n) { if( n &lt;1){ return false; } return (n &amp; ( n-1)) ==0; } 解法二在Java里面。Int的最大值是2^31 - 1到 -2^31次，所以很明显，只需要让 n 与 2^30 次做一个 &amp; 运算即可。 123456public boolean isPowerOfTwo(int n) { if( n&lt;1){ return false; } return (1073741824 % n) == 0; } 判断一个数是不是3的n次方标准解法123456789public boolean isPowerOfThree(int n) { if(n ==0){ return false; } while(n % 3 ==0){ n = n/3; } return n ==1; } 解法二在Java里面int的最大值是2^30，那么用3的最大值就可以是3^19,所以解法二为： 123456public boolean isPowerOfThree(int n) { if( n&lt;1){ return false; } return (1162261467 % n) == 0; } 解法三由数学公式: n= 3^1,可以得到 所以会有以下代码： 123public boolean isPowerOfThree(int n) { return (Math.log10(n) / Math.log10(3)) % 1 == 0; }","link":"/2018/09/26/Leetcode%E4%B8%8A%E4%B8%A4%E9%81%93%E5%88%A4%E6%96%ADn%E6%AC%A1%E6%96%B9%E7%9A%84%E9%A2%98%E7%9B%AE/"},{"title":"leetcode上两道比较好的BFS和DFS题目","text":"首先是Leetcode上两道比较好的一个题目，分别如下： https://leetcode.com/problems/letter-case-permutation/description/ Letter Case Permutation https://leetcode.com/problems/max-area-of-island/description/ Max Area of Island 关于字符串的那一题便是将一个字符串里面的任意一个字符进行大小写的变换，此题有两种解法，一种是 BFS 按照字符窜中的字符遍历，将其变成大小写，然后存入栈中，最后便每一次向后迭代，然后再存入即可。另一种则是 DFS ，通过一种不断递归的方式来进行大小写的变换的，和爬楼梯的那个算法极其类似 字符串字符串的BFS伪代码： 12345678910Stack &lt;- stack;for i -&gt; S.length(){ if(i is char){ stack.pop() -&gt; y; y[i] &lt;- Upper stack.push(y[i]); y[i] &lt;- Lower stack.push(y[i]); }} 在这个代码的写法中，采取的是广度有限遍历，即在一个平面上展开，而不是深入到这个字符的 字符串的DFS伪代码: 1234567891011121314151617List &lt;- list ;char[] c &lt;- S.toCharArray();fun change(list,index){if( index = S.length()){ list.add(S); return;}if(c[index] is not char){ change(list,index+1);}char[] temp &lt;- S.toCharArray();temp[index] &lt;- UpperCase;change(list,index+1);temp[index] &lt;- Lower;change(list,index+1);} 关于这两个算法的差异一个是从广度借助额外的存储空间来进行大小写的变换，而另一个则是通过递归将这个字符串从尾到头的进行大小写的变化。 最大岛屿面积的DFS题意就是从给定的二维数组中找出数字为 1 的，并且要求它们之间不能有间隙，所以这一题是比较适合 DFS 的解法，其类似于上楼梯的那一道题目，上楼梯就是一个递归，把每一次的步数罗列出来。 面积题目的DFS伪代码: 12345678boolean[][] &lt;- xfun getMaxArea(int[][] a, x,int xIndex, int yIndex ){ if( xIndex &lt; 0 ,yINdex&lt;0 , xINdex&lt;a.length, yINdex&lt;a[0].length){ return 0; } x &lt;- true; return 1 + getMaxArea(a,x,xIndex+1,yIndex)+getMaxArea(a,x,xIndex,yIndex+1) + getMaxArea(a,x,xIndex-1,yIndex) getMaxArea(a,x,xIndex,yIndex-1)}","link":"/2018/06/14/Leetcode%E4%B8%8A%E4%B8%A4%E9%81%93%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84BFS%E5%92%8CDFS%E9%A2%98%E7%9B%AE/"},{"title":"Leetcode数组中连续分组","text":"在Leetcode上有一道题目，如下： In a deck of cards, each card has an integer written on it. Return true if and only if you can choose X &gt;= 2 such that it is possible to split the entire deck into 1 or more groups of cards, where: Each group has exactly X cards. All the cards in each group have the same integer. 这一题就是一个求最大公约数的题目，当任意两组的公约数为1的时候，那么此时就说明，他们的分组数量不相等就可以直接返回false了。 1234567891011121314151617181920public boolean hasGroupsSizeX(int[] deck) { int[] count = new int[10000]; for(int val : deck){ count[val]++; } int sum = -1; for(int val: deck){ if(sum == -1){ sum = count[val]; } else { sum = getGcd(sum,count[val]); } if(sum == 1){ return false; } } return true; }","link":"/2019/09/08/Leetcode%E6%95%B0%E7%BB%84%E4%B8%AD%E8%BF%9E%E7%BB%AD%E5%88%86%E7%BB%84/"},{"title":"Linux上安装rabbitmq遇到的一些问题","text":"在安装elixir的时候erlang，安装了错误的包错误记录如下： 12{&quot;init terminating in do_boot&quot;,{'cannot get bootfile','no_dot_erlang.boot'}}init terminating in do_boot ({cannot get bootfile,no_dot_erlang.boot}) 此时的错误是在安装erlang的时候安装了错误的erlang，正确的需要安装的是esl-erlang，详情如下： The “esl-erlang” package is a file containg the complete installation: it includes the Erlang/OTP platform and all of its applications. The “erlang” package is a frontend to a number of smaller packages. Currently we support both “erlang” and “esl-erlang”. Note that the split packages have multiple advantages:seamless replacement of the available packages,other packages have dependencies on “erlang”, not “esl-erlang”,if your disk-space is low, you can get rid of some unused parts; “erlang-base” needs only ~13MB of space. 也就是说相较于erlang，esl-erlang的安装包是包含了所有的组件的。 rabbitmq和erlang的cookies不一致，导致启动不了启动的日志如下： 1234567891011121314attempted to contact: [rabbit@izm5e0h94dt7do1kplgd15z]rabbit@izm5e0h94dt7do1kplgd15z: * connected to epmd (port 4369) on izm5e0h94dt7do1kplgd15z * epmd reports node 'rabbit' running on port 25672 * TCP connection succeeded but Erlang distribution failed * Authentication failed (rejected by the remote node), please check the Erlang cookie current node details: - node name: 'rabbitmq-cli-56@izm5e0h94dt7do1kplgd15z' - home dir: /root - cookie hash: ajtINcxQAbART7QakzjkSg== 在搜索中发现了两个链接：StackOverFlow、Rabbitmq官方文档 其中在官网中看到了介绍： Linux, MacOS, *BSDOn UNIX systems, the cookie will be typically located in /var/lib/rabbitmq/.erlang.cookie (used by the server) and $HOME/.erlang.cookie (used by CLI tools). Note that since the value of $HOME varies from user to user, it’s necessary to place a copy of the cookie file for each user that will be using the CLI tools. This applies to both non-privileged users and root. 也就是说在Linux上，默认的cookies是在 /var/lib/rabbitmq/.erlang.cookie， 但是该rabbitmq的home目录是root，所以需要将默认路径的cookies替换到home目录下。替换之后即可。 关于Rabbitmq的web界面启动不了提示如下： 1234Plugin configuration unchanged.Applying plugin configuration to rabbit@izm5e0h94dt7do1kplgd15z... failed. 发现只要重启下rabbitmq就好了。。。 另外就是erlang的版本最好在20，在21貌似会出问题，当然针对rabbit3.6.15版本","link":"/2018/08/29/Linux%E4%B8%8A%E5%AE%89%E8%A3%85rabbitmq%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"},{"title":"leetcode求二叉树的节点最小绝对值","text":"二叉树的中序遍历题目如下: 1234567891011121314151617Given a binary search tree with non-negative values, find the minimum absolute difference between values of any two nodes.Example:Input: 1 \\ 3 / 2Output:1Explanation:The minimum absolute difference is 1, which is the difference between 2 and 1 (or between 2 and 3). 意思就是需要求出一个二叉树中绝对值最小的值。 刚开始的第一个想法就是递归，然后比较当前节点和其左右两个节点的值，但是发现有一个缺陷就是如果一个根节点是3，其左节点是10，10的右节点是4，那么最小的值便是1，然后通过递归只能是 10-3，10-4 。所以当时就放弃了 中序递归后来又想到了中序递归便是这种遍历顺序，也就是通过中序遍历的方式便可以正确的得出结果了。但是一直想不出这种情况如何进行中序操作，但是在讨论区发现了一个解法如下： 123456789101112131415int minDiff = Integer.MAX_VALUE;TreeNode prev;public int getMinimum(TreeNode root) { inorder(root); return minDiff; } public void inorder(TreeNode root) { if (root == null) return; inorder(root.left); if (prev != null) minDiff = Math.min(minDiff, root.val - prev.val); prev = root; inorder(root.right); } 后来发现这个 prev 才是这个中序遍历的关键，当一个节点的左节点遍历完之后，保存该节点为 Prev 然后与下一个节点进行比较。","link":"/2018/07/25/Leetcode%E6%B1%82%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E8%8A%82%E7%82%B9%E6%9C%80%E5%B0%8F%E7%BB%9D%E5%AF%B9%E5%80%BC/"},{"title":"LockSupport的一些琐事","text":"LockSupport 是一个用于线程阻塞或者唤醒的类，位于 rt.jar，主要是通过 Unsafe 类来进行操作 该类的方法都是静态方法，最常使用的方法是 void park(Object blocker) 和 void unpark(Thread thread) 是AQS操作的基础类，阻塞线程的时候不需要加锁，比较方便 下面主要介绍这两种方法： void park(Object blocker)这个方法和 void park() 类似，最主要的区别在于 void park(Object blocker) 设置了一个 blocker，这个参数是一个 Object，一般是当前对象放进去，尤其是需要通过 jstack 查看线程阻塞的原因，此时会打印block void unpark(Thread thread)该方法会将一个处于阻塞状态的线程唤醒，可以将「许可」理解为一个信号量如果当前的没有信号量，则 unpark 会将设置一个，如果有一个信号量，则 unpark 也不会进行任何操作，因为LockSupport 所允许的信号量仅允许一个 特点该类有一个特点就是，阻塞线程不需要在同步代码块里面 如果是 Object.wait()，需要在 synchronized 同步块里面的 唤醒一个由 park 方法阻塞的线程，被唤醒的线程是不知道由于何种原因导致的，例如唤醒它的方法有如下几种： 另一个线程调用了 unpark 方法 另一个线程调用了中断函数 阻塞的时间到了，自动唤醒被阻塞的线程可以由于上述任何一个原因被唤醒，但是唤醒之后如果要知道唤醒原因，除了第二种可以通过 isInterrupted 知道，其余两种均无法获知 使用注意事项为了避免虚假唤醒，最好是在 while 里面调用 park 方法，同时 park 和 unpark 需要配对使用，但是需要注意不要在调用 park 方法之前调用 unpark 方法，避免导致 park 不会阻塞","link":"/2021/05/16/LockSupport%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%90%E4%BA%8B/"},{"title":"MySQL_icp","text":"如果没有明确的说明，本文的存储引擎均是 InnoDB，版本：8.0 假设一个表如下： 12345678910111213CREATE TABLE `dy_video_list` ( `id` bigint unsigned NOT NULL AUTO_INCREMENT, `aweme_id` varchar(100) NOT NULL DEFAULT '', `collect_count` int NOT NULL DEFAULT '0', `comment_count` int NOT NULL DEFAULT '0', `digg_count` int NOT NULL DEFAULT '0', `share_count` int NOT NULL DEFAULT '0', `tag` varchar(100) NOT NULL DEFAULT '', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `aweme_id_time` (`aweme_id`,`create_time`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 目前数据有 2700 + 索引下推（ICP）这是MySQL 在 5.6 版本所推出的一个功能，目的是为了减少回表次数，在 InnoDB 中，数据都是存储在 BufferPool 中，因此 InnoDB 中的索引下推不能减少 IO 次数，并且 InnoDB 中的索引下推仅仅适用于二级索引，无法用于主键索引。 在 MySQL 5.6 之前，如果 where 条件中，aweme_id 采用非等值查询，那么就会造成整个索引的失效。 例如我们先将 ICP 关掉 12mysql&gt; set optimizer_switch='index_condition_pushdown=off';Query OK, 0 rows affected (0.00 sec) 然后执行一个查询： 然后此时我们再将 ICP 功能打开： 工作原理按照 MySQL 官方自己的解释是： Using Index Condition Pushdown, the scan proceeds like this instead: Get the next row’s index tuple (but not the full table row). Test the part of the WHERE condition that applies to this table and can be checked using only index columns. If the condition is not satisfied, proceed to the index tuple for the next row. If the condition is satisfied, use the index tuple to locate and read the full table row. Test the remaining part of the WHERE condition that applies to this table. Accept or reject the row based on the test result.EXPLAIN output shows Using index condition in the Extra column when Index Condition Pushdown is used. It does not show Using index because that does not apply when full table rows must be read. 当通过「联合索引」进行查询的时候，如果未开启索引下推功能，存储引擎返回的数据都是需要服务端进行过滤，例如这个 SQL： 12select * from dy_video_list where aweme_id like '61%' and create_time = 1655540046; 在未开启索引下推的功能时，会看到后面的 Extra 是 Using where，此时 InnoDB 会将所有 61 开头的数据通过回表查询，但同时也将 create_time 不是 1655540046 的数据也一起回表，并返回给服务端，服务端则根据 where 条件自己过滤。] 而开启索引下推以后，服务端将 where 过滤的功能下推至存储引擎，此时由 InnoDB 自己完成 where 过滤 意外情况是不是觉得以后是一个索引都会走索引下推呢？那么如果把 SQL 改一下呢？ 此时是不是会发现竟然直接走了「全表扫描」？？说好的索引下推呢？ 一探究竟既然 MySQL 选择了全表扫描，那么我们不妨看下 MySQL 是如何来计算成本的，其中 MySQL 提供了 OPTIMIZER_TRACE 这个关键字来供大家分析，默认是 OFF 状态。 使用的时候，首先需要打开这个开关： 12mysql&gt; SET OPTIMIZER_TRACE=&quot;enabled=on&quot;;Query OK, 0 rows affected (0.00 sec) 然后执行我们的SQL： 12mysql&gt; select * from dy_video_list where aweme_id like '6%' and create_time = 1655307436; 最后在执行： 12select * from information_schema.OPTIMIZER_TRACE\\G 此时就可以看到 MySQL 的控制台打印一大堆信息了。 其中主要关心的是这三个节点的信息，其中第一个也就是 MySQL 认为全表扫描的成本，cost 为 266.8。 现在暂且不纠结该成本是怎么算出来的，先看下走 aweme_id_name 这个索引它的成本是多少，继续往下翻就可以找到。 可以看到成本是 297.06，于是 MySQL 认为直接走全表扫描的代码是小于走这个二级索引的，因此才会放弃。 那么为什么在这个情况下，MySQL 会选择放弃呢？，上图的截图中看到 6% 查询到的数据行数是 848 行，而这个表的总行数是多少呢？ 大家可能发现这个数据和上面通过 OPTIMIZER_TRACE 分析的不一致，这个是由于 InnoDB 的特性，会有一定的差异，现在可以看到 848 大约是占比总行数的 30.6%。 那我给他改几条数据，让他小于 30% 看看 可以看到已经是小于 30% 了，此时再次分析看看 此时可以发现该查询已经可以通过索引下推来优化了 30%阈值具体这个 30% 的阈值是怎么来的，目前是参考 oreilly 的一篇文章，具体的链接如下： 最后如果有人问你索引下推一定会生效吗？可以直接跟他说，还得看索引的分散情况，如果联合索引的建立的不太好，那么索引下推也不会生效。","link":"/2022/06/19/MySQL-icp/"},{"title":"Mybatis中sql排序以及#和$的区别(二)","text":"mybatis的orderby在使用mybatis的时候，一般来讲是使用#{}这种方式来设置sql的参数，因为mybatis在解析sql的时候的时候对于使用#{}的参数首先会解析成?,然后再加入参数。具体可以看mybatis的解析日志： 12DEBUG [main] - ==&gt; Preparing: select * from clazzentity where clazz_name = ? DEBUG [main] - ==&gt; Parameters: 一年级(String) 但是在mybatis中如果需要使用groupby和orderby的话就需要注意不可以使用#了，因为使用#的话会导致解析出来的参数自动的带了一个引号,而使用$的话就会直接把参数带进去，所以在进行groupby的时候是需要使用$来进行参数的替换的。但是在使用${}这个的时候需要注意下。 mybatis的多参数和单参数单参数mybatis的单参数一般来讲是会自动被识别的，#的话直接可以加参数名称，$的话则是使用_paramter来接收 多参数parammybatis中若需要使用多参数的话要么使用@param注解，要么使用集合或者对象来作为参数,其代码如下： 1List&lt;Map&lt;String,Object&gt;&gt; queryStudnet2(@Param(&quot;CLAZZID&quot;) String clazzId ,@Param(&quot;name&quot;) String name); 而其配置文件则如下图所示： 123&lt;select id=&quot;queryStudnet2&quot; resultType=&quot;java.util.HashMap&quot; &gt; select count(CLAZZ_ID) as 'id' , CLAZZ_ID from student GROUP by ${CLAZZID} order by ${name} &lt;/select&gt; 在${}里面的参数全部是@Param,所以这是一种多参数传递方式，同时这种方式进行传参的话#和$都可以解析， ListList的传参应该只是适用于foreach迭代,也就是需要动态的执行多条语句的时候才会使用List Java对象而Java对象传参的话在入参的地方需要加入Java对象的实体类，但是在查询需要返回结果的时候可以通过resultMap接收，不是一个map也可以是一个Java对象也可以接收，但是需要注意的是通过Java对象接收的话要java字段和sql的字段映射。一种就是已经说了的resultMap，另一种就是数据库返回字段设置别名然后让其返回的是Java对象的字段 Mapmap则适用于多参数传递，类似于java对象通过map传递参数的话是最常见的一种方式在传入#{}的动态参数，但是在使用map的方式的时候需要注意，通过map传值的话，用${}是接受不到的，会直接抛出异常 12### Error querying database. Cause: org.apache.ibatis.builder.BuilderException: Error evaluating expression '_paramter.clazz'. Cause: org.apache.ibatis.ognl.OgnlException: source is null for getProperty(null, &quot;clazz&quot;)### Cause: org.apache.ibatis.builder.BuilderException: Error evaluating expression '_paramter.clazz'. Cause: org.apache.ibatis.ognl.OgnlException: source is null for getProperty(null, &quot;clazz&quot;) 所以map传值的话最好是传入需要#{}接受的参数 索引传参仅适用于# 123 &lt;select id=&quot;queryStudnet3&quot; resultType=&quot;java.util.HashMap&quot; &gt; select count(CLAZZ_ID) as 'id' , CLAZZ_ID from student GROUP by #{arg0} order by #{arg1}&lt;/select&gt; Java代码如下： 1234//Dao层代码List&lt;Map&lt;String,Object&gt;&gt; queryStudnet3(String clazzId ,String name);//实际方法 List&lt;Map&lt;String,Object&gt;&gt; list =clazz.queryStudnet3(&quot;CLAZZ_ID&quot;,&quot;id&quot;); 通过这个方式传参的话需要注意mybatis的提示使用arg0还是param 异常汇总1Cause: org.apache.ibatis.executor.ExecutorException: A query was run and no Result Maps were found for the Mapped Statement 'study.dao.clazzStudent.queryStudnet2'. It's likely that neither a Result Type nor a Result Map was specified 这个表示若返回的是一个集合的话需要制定下返回的类型。其代码如下所示：","link":"/2018/04/12/Mybatisz%E4%B8%ADsql%E6%8E%92%E5%BA%8F%E4%BB%A5%E5%8F%8A-%E5%92%8C-%E7%9A%84%E5%8C%BA%E5%88%AB-%E4%BA%8C/"},{"title":"MySql中的幻读(一)","text":"什么是幻读幻读表示的是在一个事物里面 同一个select语句，前后两次查询出来的结果是不相同的，需要注意的一点是，在InnoDB里面，幻读跟事物的隔离级别有关，更加准确的说是跟一个事物的快照和当前读有关 下面是在Mysql8.0.11版本下进行幻读的复现： 引擎：InnoDB 事物隔离级别：Read Commited MVCC和快照读以及当前读MVCC在介绍MVCC之前先来介绍下MVCC为什么会出现，首先数据库作为一个数据存储工具，那么肯定是存在并发的情况，在Mysql的InnoDB里面最常见的就是x锁，这是一种写锁，在并发的情况下只有一个事务会获取该锁，其他事务则会一直等待直至获取到该锁。 那么在读取的时候如何保证并发的事物都能正确的读取到自己正确的数据呢？ 在MVCC的概念里面，如果事务的隔离级别是Read Commited的话，那么每一次的快照都都会读取该行的最近一次commited数据，而如果是Repeatable Read的话，则是会读取当前事务ID开始之前的一次commited数据。 所以MVCC仅仅是作为一个保证数据库并发读情况下的一个数据正确的手段而已，在不同的数据库里面，有不同的实现，例如在OceanBase里面，是通过操作链来解决并发读的问题 快照读快照读是利用MVCC和 undo log 来实现的，其主要作用就是当我们对某行数据修改之后，并不会将原值修改，而是在上一个版本上面再新建一个版本(修改InnoDB的隐藏两列)。所以在不同的隔离级别下，可以根据自己的事物ID来获取自己所需要的数据。当第一条不满足的时候，会沿着undo log一直寻找，在Read Commited隔离级别下就是直接找出该行数据最后一次提交的版本 当前读当前读是对数据的加锁读取，读取的都是最新的数据，例如如下SQL： 123456select ... for updateselect ... in share modeinsertupdatedelete... 同时也需要注意的一点是，当前读会对涉及到的行都进行加锁 为什么会有当前读和快找读按照Mysql官方的解释，当前读是为了防止其他事物修改你即将进行的操作 If you query data and then insert or update related data within the same transaction, the regular SELECT statement does not give enough protection. Other transactions can update or delete the same rows you just queried. InnoDB supports two types of locking reads that offer extra safety 幻读发生的条件首先解释下Mysql官方对于幻读的解释： The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row. 这里Mysql官方虽然只是给出了对于幻行的定义，但是仍然可以简单解释下，也就是说两次的select前后得到的结果集是不同的，那么新多出来的一行就可以称之为幻行 在这里特别需要注意的是，在PR隔离界别下，只有当前读才会出现幻读 Read Commited在Read Commited隔离级别下，每一次的select 都是一次快照读，所以在该隔离级别下，幻读可以在快照读和当前读发生。 事务一 12345678910111213141516171819202122232425262728mysql&gt; set session transaction isolation level read committed;Query OK, 0 rows affected (0.00 sec)mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from szh.t1 -&gt; ;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 2 | 10 |+---------+-----------+2 rows in set (0.00 sec)mysql&gt; select * from `szh`.`t1`;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 2 | 10 |+---------+-----------+2 rows in set (0.00 sec) 事务二 1INSERT INTO `szh`.`t1`(`area_id`, `order_num`) VALUES (3, 22); 此时事务一再进行select 12345678910mysql&gt; select * from `szh`.`t1`;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 2 | 10 || 3 | 22 |+---------+-----------+3 rows in set (0.00 sec) 于是幻读发生了 Repeatable Read那么如果在Repeatable Read隔离级别下，上面的SQL在执行一遍会出现什么情况呢？ 事务一 12345678910111213141516171819mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM `t1` -&gt; ;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 2 | 10 || 3 | 22 || 4 | 33 || 5 | 55 |+---------+-----------+5 rows in set (0.00 sec) 事务二 1INSERT INTO `szh`.`t1`(`area_id`, `order_num`) VALUES (6, 66); 事务一再次select 123456789101112mysql&gt; SELECT * FROM `t1`;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 2 | 10 || 3 | 22 || 4 | 33 || 5 | 55 |+---------+-----------+5 rows in set (0.00 sec) 可以看到的是在快照读下面，是没有幻读出现的，那么修改select为当前读呢？在事务一里面再次执行以下SQL 123456789101112mysql&gt; SELECT * FROM `t1` for update;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 2 | 10 || 3 | 22 || 4 | 33 || 5 | 55 || 6 | 66 |+---------+-----------+6 rows in set (0.00 sec) 可以看到确实出现了两次的select不同的情况。所以需要强调一点的是，在Repeatable Read隔离级别下，只有当前读才会出现幻读，因为在该级别下，快照读是从begin开始的第一个普通select建立的Read View，以后的普通select都是基于第一次的select，自然而然不会出现幻读了","link":"/2019/08/18/Mysql%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%AF%BB-%E4%B8%80/"},{"title":"MySql中int类型的简单总结","text":"问题首先问两个问题： int(1)和int(10)有什么区别。 int(3)可以存储 10000 这个数字吗？ int(11)可以用来存储手机号么？ 本次的源代码以及测试的 Mysql 版本均为 8.0.17 解释首先新建一个表，SQL如下： 12345create table test_int( id int(3) not null primary key , no int(5) not null, phone int(11)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 当你执行完这个 SQL 以后，Mysql会发出一个如下提示： [2019-12-16 23:08:24] [HY000][1681] Integer display width is deprecated and will be removed in a future release.首先不管这个提示，待会后文会解释的。 然后新增测试数据： 12INSERT INTO `test`.`test_int` (`id`, `no`, `phone`) VALUES (10000, 1008611, 123124);INSERT INTO `test`.`test_int` (`id`, `no`, `phone`) VALUES (10, 134, 124); 然后再进行 select 查看。 12345678 mysql&gt; select * from test_int;+-------+---------+--------+| id | no | phone |+-------+---------+--------+| 10 | 134 | 124 || 10000 | 1008611 | 123124 |+-------+---------+--------+2 rows in set (0.00 sec) 可以看到其实并没有影响到任何数据的插入，随后查询 Mysql 的官方文档，里面有这样的一段话 If you specify ZEROFILL for a numeric column, MySQL automatically adds the UNSIGNED attribute to the column. 随后再次新建一个表： 1234567create table test_int_zerofill( id int(3) unsigned zerofill not null primary key , no int(5) unsigned zerofill not null , phone int(11) unsigned zerofill) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;The ZEROFILL attribute is deprecated and will be removed in a future release. Use the LPAD function to zero-pad numbers, or store the formatted numbers in a CHAR column. 虽然提示 ZEROFILL 快要被废弃了，但是为了演示区别，所以暂时还是不管了。 12345678mysql&gt; select * from test_int_zerofill;+-------+--------+-------------+| id | no | phone |+-------+--------+-------------+| 010 | 00010 | 00000000010 || 12345 | 123456 | 00001234567 |+-------+--------+-------------+2 rows in set (0.00 sec) 所以对于问题1，所以 int(1) 和 int(10) 其实是没有区别的，唯一有区别的地方就在于如果使用了 unsigned zerofill 修饰的话，那么不足长度的会在左边进行补 0 ，而如果没有用 unsigned zerofill 进行修饰的话，可以说基本上是没有区别的。 对于问题2，因为 int 括号里面的值与位数无关， 所以是可以的。到此，对于执行第一个SQL所进行的提示，是因为 Mysql 也觉得其实没什么意义，而是更加推荐用 LDAP() 这个函数来判断。 对于第二个问题，可以尝试一个手机号： 123INSERT INTO `test`.`test_int_zerofill` (`id`, `no`, `phone`) VALUES (10, 10, 13100000000)[22001][1264] Data truncation: Out of range value for column 'phone' at row 1 出现这个问题的原因在于 int 类型在 mysql 里面是四个字节，而一个字节是 8 位，所以在 mysql 里面，一个int 类型最多可以储存 2^31(一个符号位置)，也就是约 21 亿左右，无符号的也最多42亿，但是手机号最低也是11位，也就是 130 多亿。所以肯定是无法存储的。下面是 Mysql 官方给出的 num 类型的存储范围 源码由于想看下 Mysql 到底是如何处理 int 类型的数值的，于是下载并且编译了 Mysql 的源码，一直跟着 debug，最后找到了 Mysql 判断 int 是否超长的一个代码，如下： 123456789101112131415161718192021222324252627282930313233343536type_conversion_status Field_long::store(double nr) { ASSERT_COLUMN_MARKED_FOR_WRITE; type_conversion_status error = TYPE_OK; int32 res; nr = rint(nr); if (unsigned_flag) { if (nr &lt; 0) { res = 0; error = TYPE_WARN_OUT_OF_RANGE; } else if (nr &gt; (double)UINT_MAX32) { res = UINT_MAX32; set_warning(Sql_condition::SL_WARNING, ER_WARN_DATA_OUT_OF_RANGE, 1); error = TYPE_WARN_OUT_OF_RANGE; } else res = (int32)(ulong)nr; } else { if (nr &lt; (double)INT_MIN32) { res = (int32)INT_MIN32; error = TYPE_WARN_OUT_OF_RANGE; } else if (nr &gt; (double)INT_MAX32) { res = (int32)INT_MAX32; error = TYPE_WARN_OUT_OF_RANGE; } else res = (int32)(longlong)nr; } if (error) set_warning(Sql_condition::SL_WARNING, ER_WARN_DATA_OUT_OF_RANGE, 1);#ifdef WORDS_BIGENDIAN if (table-&gt;s-&gt;db_low_byte_first) { int4store(ptr, res); } else#endif longstore(ptr, res); return error;} 在这段代码里面，Mysql首先会判断是否带有符号位，如果是无符号位的话，则是直接判断是否大于 UINT_MAX32，而如果是有符号的话，则是判断是否小于 INT_MIN32 或者大于 INT_MAX32，否则直接为最小或者最大值，然后设置error。PS：这一段代码是还未进行 InnoDB 引擎层，可以看到 Mysql 是在 Server 层进行SQL语句的校验。","link":"/2019/12/17/Mysql%E4%B8%ADint%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/"},{"title":"MySql之binlog的使用(一)","text":"背景假设有一个如下的业务场景如下：需要记录一个商品或者股票的实时价格，每一个小时记录一次，而商品或者股票的数量十分多，这时业务发展到一定的程度之后就需要考虑数据库的设计。首先商品每个小时的价格肯定是需要入库的。其次每小时的购买人群以及各种埋点数据随之一起也要入库。以便于日后的数据分析。 解决方案随着数据量的增大，一般的解决方案是设置索引，然后再考虑是进行垂直还是水平分库分表。 但是一旦使用水平分库分表就会无形之间增加开发的复杂程度，而且分库分表之后考虑的各种因素也会随之而来增加数倍。例如各种表的唯一ID以及如何进行维度的划分。 对于数据量不大的一个另解决方法是：解析mysql的binlog日志，然后将其存入另一个库，优先推荐mongo，该库只作为一个读取库进行查询，不进行任何的写入。这样处理之后，在中等程度的规模数据是完全可以满足需求的，将其读写进行分离。而且丝毫不影响之前的业务和设计。 binlog其实对于增删改查之外，mysql的binlog日志也是一个非常有用的工具，它可以记录下数据库的每一次操作，例如查询，新增，删除，更新等。然后将其作为日志记录在binlog之中。 例子首先确保你的数据库已经开启了binlog日志， 1234567891011121314151617181920212223242526mysql&gt; show variables like 'log_%';+----------------------------------------+------------------------------------------------------------------+| Variable_name | Value |+----------------------------------------+------------------------------------------------------------------+| log_bin | ON || log_bin_basename | /opt/mysql/mysql-8.0.11-linux-glibc2.12-x86_64/data/master || log_bin_index | /opt/mysql/mysql-8.0.11-linux-glibc2.12-x86_64/data/master.index || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF || log_error | ./error.log || log_error_services | log_filter_internal; log_sink_internal || log_error_verbosity | 2 || log_output | FILE || log_queries_not_using_indexes | OFF || log_slave_updates | ON || log_slow_admin_statements | OFF || log_slow_slave_statements | OFF || log_statements_unsafe_for_binlog | ON || log_syslog | ON || log_syslog_facility | daemon || log_syslog_include_pid | ON || log_syslog_tag | || log_throttle_queries_not_using_indexes | 0 || log_timestamps | UTC |+----------------------------------------+------------------------------------------------------------------+20 rows in set (0.01 sec) 可以看到| log_bin | ON ，这就表示数据库已经开启了binlog日志，如果你查询到还没有开启的话，可以去搜索下如何开启。 另外需要注意的是，如果你需要在binlog中看到日志的话，你同时也需要在Mysql中设置binlog_rows_query_log_events为Row，如果不确定的话，可以将set binlog_rows_query_log_events=1执行一次。 为了测试，已经建立好了一个表，其结构与如下： 1234567891011mysql&gt; desc T_fund;+-------+---------------+------+-----+---------+-----------------------------+| Field | Type | Null | Key | Default | Extra |+-------+---------------+------+-----+---------+-----------------------------+| id | int(10) | NO | PRI | NULL | || name | varchar(255) | YES | | NULL | || price | decimal(10,2) | YES | | NULL | || date | timestamp | NO | | NULL | on update CURRENT_TIMESTAMP |+-------+---------------+------+-----+---------+-----------------------------+4 rows in set (0.01 sec) 那么现在尝试向该表插入一个语句， 123mysql&gt; insert into T_fund(id,name,price,date) values(3,'测试基金3',1234.1,'2018-11-11 22:12:00');Query OK, 1 row affected (0.09 sec) 此时查看binlog， 12345678910mysql&gt; show master logs;+---------------+-----------+| Log_name | File_size |+---------------+-----------+| master.000001 | 178 || master.000002 | 62644 || master.000003 | 112942 |+---------------+-----------+3 rows in set (0.00 sec) 可以看到目前是有三个binlog文件，由于binlog是依次从1开始递增，所以刚刚的插入语句是在第三个日志中，查看master.000003即可。 12345678910111213141516171819202122232425262728293031mysql&gt; show binlog events in 'master.000003' from 114213\\G;*************************** 1. row *************************** Log_name: master.000003 Pos: 114213 Event_type: Rows_query Server_id: 1End_log_pos: 114330 Info: # insert into T_fund(id,name,price,date) values(3,'测试基金3',1234.1,'2018-11-11 22:12:00')*************************** 2. row *************************** Log_name: master.000003 Pos: 114330 Event_type: Table_map Server_id: 1End_log_pos: 114397 Info: table_id: 179 (T_binlog.T_fund)*************************** 3. row *************************** Log_name: master.000003 Pos: 114397 Event_type: Write_rows Server_id: 1End_log_pos: 114461 Info: table_id: 179 flags: STMT_END_F*************************** 4. row *************************** Log_name: master.000003 Pos: 114461 Event_type: Xid Server_id: 1End_log_pos: 114492 Info: COMMIT /* xid=4004 */4 rows in set (0.00 sec) 由于我知道该条日志的position，所以加了一个参数from 114213，如果不知道的话，可以直接输入show binlog events in 'master.000003然后看结尾即可。 有了mysql的binlog之后，下一步就需要考虑如何将binlog解析到其他的数据库之中，目前开源的轮子，比较好的有: Maxwell(github) canal(alibaba) 在这里推荐Maxwell，因为Maxwell目前已经对MySql8有一个比较好的支持了，而且我们已经将部分业务应用到了Maxwell了，目前还比较稳定。 下一篇主要会将Maxwell与binlog以及rabbitmq进行一个整合。","link":"/2018/11/11/Mysql%E4%B9%8Bbinlog%E7%9A%84%E4%BD%BF%E7%94%A8-%E4%B8%80/"},{"title":"MySql数据库出现死锁的情况(一)","text":"在临近上线之前，我们系统做了一次压力测试，发现有一个接口在高并发情况下会出现一个死锁的情况。。首先申明…不是我写的，我只是帮忙排查下。 随着对Mysql锁的深入了解，于是就准备写几篇文章来记录下Mysql各种事物和索引的情况下出现死锁的情况。 今天就介绍下在并发插入的情况下，哪几种情况会出现死锁： INNODB下的各种锁在介绍锁的时候只会介绍跟本节相关的锁，而且只会讲述大概是什么，至于锁的更加详细的讲解可能会到以后再详细介绍。 行锁行锁分为写锁和读取锁， 读锁（S锁）也可以称之为 共享锁 ， 它表示的是任何一个事物都可以读取该行数据(可以被多个事物获取到)。 写锁（X锁）也可以称之为排它锁，它表示的是该行数据不允许任何人进行修改，同时也不允许任何事物获取该行事物的S锁，但是普通的 select 语句是可以的。 背景信息一注意：以下测试都是基于该事物隔离级别和数据库版本数据库版本：8.0.11 事务隔离级别：REPEATABLE-READ 1234567mysql&gt; SHOW VARIABLES LIKE 'transaction_isolation';+-----------------------+-----------------+| Variable_name | Value |+-----------------------+-----------------+| transaction_isolation | REPEATABLE-READ |+-----------------------+-----------------+1 row in set (0.00 sec) SQL准备:12345678910create database if not exists test_lock DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;CREATE TABLE test_lock.`test` ( `user_id` varchar(32) NOT NULL, `user_name` varchar(32) NOT NULL, `user_password` varchar(16) NOT NULL, `is_deleted` int(1) NOT NULL, `phone` varchar(20) NOT NULL, PRIMARY KEY (`user_id`), KEY `t1_god` (`user_name`,`user_password`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; 准备数据： 12345678insert into test_lock.`test` values('a','zhangsan','123456',0,'15112345678');insert into test_lock.`test` values('b','lisi','lisi123456',0,'15112345678');insert into test_lock.`test` values('c','wangwu','wangwu123456',0,'15112345678');insert into test_lock.`test` values('d','caocao','caocao123456',0,'15112345678');insert into test_lock.`test` values('e','liubei','liubei123456',0,'15112345678');insert into test_lock.`test` values('f','zhangfei','zhangfei123456',0,'15112345678');insert into test_lock.`test` values('g','guanyu','guanyu123456',0,'15112345678');insert into test_lock.`test` values('h','xiaoqiao','xiaoqiao123456',0,'15112345678'); 情况一： 三个事物都执行同一个Insert语句，第一个事物然后回滚 此时第三个事物会提示死锁，第二个事物正常插入。 复现步骤：开启三个事物，每一个事物分别执行如下SQL，然后再将第一个事物进行回滚或者提交，然后你就会发现第三个事物发生了死锁。 1234## DeadLockstart transaction ;begin;insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678'); Mysql提示如下： [2019-08-04 14:33:48] Connectedsql&gt; start transaction[2019-08-04 14:33:49] completed in 4 mssql&gt; begin[2019-08-04 14:33:49] completed in 6 mssql&gt; insert into test_lock.test values(‘i’,’daqiao’,’daqiao123456’,0,’15112345678’)[2019-08-04 14:34:00] [40001][1213] Deadlock found when trying to get lock; try restarting transaction[2019-08-04 14:34:00] [40001][1213] Deadlock found when trying to get lock; try restarting transaction 那么打印出Mysql出现死锁的日志：重点日志如下 1234567891011121314151617181920212223242526272829303132LATEST DETECTED DEADLOCK------------------------2019-08-04 06:33:59 0x7f1818599700*** (1) TRANSACTION:TRANSACTION 231690, ACTIVE 34 sec insertingmysql tables in use 1, locked 1LOCK WAIT 4 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 9, OS thread handle 139741464762112, query id 95 172.17.0.1 root update/* ApplicationName=IntelliJ IDEA 2019.1.3 */ insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678')*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 7 page no 4 n bits 80 index PRIMARY of table `test_lock`.`test` trx id 231690 lock_mode X insert intention waitingRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;*** (2) TRANSACTION:TRANSACTION 231691, ACTIVE 10 sec insertingmysql tables in use 1, locked 14 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 10, OS thread handle 139741464467200, query id 139 172.17.0.1 root update/* ApplicationName=IntelliJ IDEA 2019.1.3 */ insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678')*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 7 page no 4 n bits 80 index PRIMARY of table `test_lock`.`test` trx id 231691 lock mode SRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 7 page no 4 n bits 80 index PRIMARY of table `test_lock`.`test` trx id 231691 lock_mode X insert intention waitingRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;*** WE ROLL BACK TRANSACTION (2)------------ 从以上日志我们可以看到，事物一正在请求该行记录的 X锁 ，事物二持有该行的S锁，但是也在等待获取该行的X锁。 关于Mysql的 insert 逻辑，可以大致理解为如果一个事物正在对一个记录进行 insert，此时 InnoDB 并不会主动的将其加一个锁，而是在主键索引上加一个 trx_id， 当第二个事物检测到该行记录正在被一个活跃的事物持有的时候，此时第二个事物会帮第一个事物的隐式锁住转为显式锁。如下例子： 1234## DeadLockstart transaction ;begin;insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678'); 此时查看Mysql中锁的情况： 可以看到此时Mysql仅仅是在表上加入了一个插入意向锁（IX锁），持有该锁表示该事物在接下来有可能会对自己设计到的行加入排它锁（X锁） Mysql关于意向锁的介绍 The intention locking protocol is as follows: Before a transaction can acquire a shared lock on a row in a table, it must first acquire an IS lock or stronger on the table. Before a transaction can acquire an exclusive lock on a row in a table, it must first acquire an IX lock on the table. 大意就是如果你想获得一个行的 X锁，那么你就必须先获取表的 IX锁 那么此时再进行第二个事物的插入： 1234## DeadLockstart transaction ;begin;insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678'); 再次查看数据库中的锁： 此时会发现第一个事物已经获取到了行级别的X锁，第二个事物获取到了 IX 锁以及 S锁 。 那么此时第三个事物开启之后数据库中的锁又会是什么样子的呢？ 可以发现另外两个事物都在等待获取该行的S锁。然后此时第一个事物进行回滚，此时第三个事物就会提示死锁。 原因前提结论 S锁是可以升级到X锁的 一个S锁需要升级到X锁必须保证只有当前事物持有S锁 死锁原因当第一个事物回滚之后，第二个事物和第三个事物都会获得到该行的S锁，但是此时第二个事物将要执行 insert 语句，也就是第二个事物正在等待获取该行的X锁，但是此时第三个事物也正在准备 insert，它也在准备获取 X锁，目前这两个事物都持有该行的 S锁 ，而如果需要获取 X锁，则是需要对方释放S锁。如下图： 简单复现步骤：为了验证上面的情况，我准备如下几个SQL： 12345678910111213141516171819## 事物一start transaction ;begin;select * from test_lock.test where user_id='h' lock in share mode;## 事物二start transaction ;begin;select * from test_lock.test where user_id='h' lock in share mode;## 事物一再执行update test_lock.`test` set user_name='abc' where user_id = 'h';## 事物二再执行update test_lock.`test` set user_name='abc' where user_id = 'h';## 此时事物二就会提示死锁 这是因为事物一和事物二都在等待对方释放S锁，但是都不肯释放，于是死锁发生了。 情况二 三个事物都执行同一个Insert语句，第一个事物然后提交 此时第二个事物和第三个事物都会提示主键冲突，并无死锁出现。 总结这种情况下的死锁是由于 S锁 升级到 X锁导致的一种死锁，在平常的业务代码中应该尽量避免并发插入一个主键。","link":"/2019/08/04/Mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%87%BA%E7%8E%B0%E6%AD%BB%E9%94%81%E7%9A%84%E6%83%85%E5%86%B5-%E4%B8%80/"},{"title":"Python2.7使用Pandas连接数据库","text":"今天遇到一个需求，需要将Excel中的一些数据导入到mysql中，由于之前接触到了Python的Pandas，所以这个时候便想到了Python，但是连接数据库的时候出现了问题，所以便写一个文章记录下。 解决办法： 下载Mysql_Python的一个exe文件 注意tosql的这个方法使用的类。pd.io.sql.to_sql 注意添加index=False防止出现出入的时候多了一个index sqlalchemy方式连接导入库由于使用的版本是Python2.7.14，所以在安装MySQLdb的时候一直出现问题，大意就是说需要升级pip，但是pip已经升级了。所以去网上查询解决办法是需要安装一个文件Mysql-Python。。。.exe但是这里需要注意版本，官网下载的是win32的，所以导致一直识别不到Python2.7的路径，导致MySQLdb这个一直安装不上。 另外需要导入的库就是Pandas了 to_sql()在查看官网API(pandas1.6.0版本)的时候，发现是pd.to_sql()，但是实际上这里是pd.io.sql.to_sql(),参数还是不变。 1234567def ins(cls): xls = pd.ExcelFile(U'C:\\\\Users\\\\SZH\\\\Desktop\\\\aaa\\\\product表.xlsx') previous_score_csv_file = xls.parse(U'Sheet1'); conn = create_engine('mysql+mysqldb://root:root//@localhost:3306/vendor?charset=utf8') pd.io.sql.to_sql(previous_score_csv_file,&quot;product&quot;,conn,if_exists='append',index=False) # c.commit() print previous_score_csv_file 最后便可以了。 MySQLdb方式连接暂时找不到解决办法","link":"/2018/04/11/PPython2-7%E4%BD%BF%E7%94%A8Pandas%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"Python使用Plotly来画图","text":"Plotly是一个比较好的画图工具，主要用于数据的展示，以及分析，最好配合pandas一起使用发挥出最大的作用 引入基础库123456import plotlyimport pymysqlimport sysimport plotly.plotlyimport plotly.graph_objs as goimport pandas as pd 画图：在这里画图的话可以使用参照官网给出的一个饼形图例子： 123456789labels = ['Oxygen','Hydrogen','Carbon_Dioxide','Nitrogen']values = [4500,2500,1053,500]colors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']trace = go.Pie(labels=labels, values=values, hoverinfo='label+percent', textinfo='value', textfont=dict(size=20), marker=dict(colors=colors, line=dict(color='#000000', width=2))) go.pie()函数的参数含义分别是名字，值，鼠标覆盖后需要显示的信息：标签和占比，最后一个是饼形图每一块显示内容，是限制数量还是什么，最后如果需要在本地会吐的话需要调用离线画图函数，但是貌似它的图形会比在线的少不少。 1plotly.offline.plot([trace]) 若需要绘制多个的话类似于折线图可以修改为plotly.offline.plot([trace1,trace2])或者直接data=[trace1,trace2]然后plotly.offline.plot(data)","link":"/2018/04/13/Python%E4%BD%BF%E7%94%A8Plotly%E6%9D%A5%E7%94%BB%E5%9B%BE/"},{"title":"Rabbitmq深度学习一","text":"rabbitmq 深度学习一简介rabbitmq是目前使用最多的一个消息中间件，配合微服务的使用可以使业务模块化，便于之后的维护。同时使用rabbitmq可以将许多业务异步化，提高系统的性能。 队列的创建原则但是在使用rabbitmq 的时候需要注意点就是到底是生产者来建立队列和交换机还是由消费者来建立交换机和队列。一般的情况是由消费者来建立队列，但是假如消费者挂掉了，导致生产者发出的消息被交换机路由到了一个不存在的队列，那么此时 rabbitmq会忽略该条消息。所以对于重要的消息。即不允许该消息丢失，那么此时最好是由生产者和消费者一起船创建一个队列。 在rabbit里面，假设生产者和消费者同时创建一个队列，如果队列的各项参数都相同的话，rabbitmq是不会有问题的，假设生产者和消费者同时创建一个队列，但是后创建的和前面创建的参数不同，那么此时rabbitmq就会报错。 特殊的队列：死信队列死信队列适用于当某一个队列的消息被拒绝的时候，或队列的消息大于最大TTL时以及队列大于最大值。此时该消息会被路由到死信交换器。其一般的使用方式是建立一个死信交换机和一个死信队列，然后监听死信队列即可。 代码示例：生产者端建立队列123456789101112131415161718192021222324252627282930313233@Configuration@ConfigurationProperties(ignoreUnknownFields = false, prefix = &quot;somersames.rabbitmq&quot;)@Datapublic class RabbitmqConfig { private String laGouFailQueue = &quot;laGouFailQueue&quot;; private String laGouQueue = &quot;laGouQueue&quot;; private String laGouFailExchange = &quot;laGouFailExchange&quot;; private String laGouFailExchangeRoutingKey = &quot;laGouFailExchangeRoutingKey&quot;; public static final String DEAD_LETTER_EXCHANGE = &quot;x-dead-letter-exchange&quot;; public static final String DEAD_LETTER_ROUTING_KEY = &quot;x-dead-letter-routing-key&quot;; @Bean public Queue laGouFailQueue() { return new Queue(laGouFailQueue); } @Bean public DirectExchange laGouFailExchange() { return new DirectExchange(laGouFailExchange); } @Bean public Binding bindingLagouFailExchange(Queue laGouFailQueue, DirectExchange laGouFailExchange) { return BindingBuilder.bind(laGouFailQueue).to(laGouFailExchange).with(laGouFailExchangeRoutingKey); } @Bean public Queue laGouQueue() { Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(DEAD_LETTER_EXCHANGE, laGouFailExchange);//设置死信交换机 map.put(DEAD_LETTER_ROUTING_KEY, laGouFailExchangeRoutingKey);//设置死信routingKey return new Queue(laGouQueue, true, false, false, map); }} 消费者端建立队列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Configurationpublic class RabbitConfig { private String laGouQueue = &quot;laGouQueue&quot;; private String laGouExchange = &quot;laGouExchange&quot;; private String laGouFailQueue = &quot;laGouFailQueue&quot;; private String laGouFailExchange = &quot;laGouFailExchange&quot;; private String laGouFailExchangeRoutingKey = &quot;laGouExchangeRoutingKey&quot;; public static final String DEAD_LETTER_EXCHANGE = &quot;x-dead-letter-exchange&quot;; public static final String DEAD_LETTER_ROUTING_KEY = &quot;x-dead-letter-routing-key&quot;; @Bean public Queue laGouQueue() { Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(DEAD_LETTER_EXCHANGE, laGouFailExchange);//设置死信交换机 map.put(DEAD_LETTER_ROUTING_KEY, laGouFailExchangeRoutingKey);//设置死信routingKey return new Queue(laGouQueue, true, false, false, map); } @Bean public Queue laGouFailQueue() { return new Queue(laGouFailQueue); } @Bean public DirectExchange laGouFailExchange() { return new DirectExchange(laGouFailExchange); } @Bean public SimpleMessageListenerContainer laGouListenerContainer( ConnectionFactory connectionFactory, LagouRabbitMqListener lagouRabbitMqListener, Queue laGouQueue) { SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(); container.setMessageListener(lagouRabbitMqListener); container.setQueues(laGouQueue); container.setConnectionFactory(connectionFactory); container.setConcurrentConsumers(10); container.setMaxConcurrentConsumers(20); container.setAcknowledgeMode(AcknowledgeMode.MANUAL); return container; }}@Servicepublic class LagouRabbitMqListener implements ChannelAwareMessageListener { @Override public void onMessage(Message message, Channel channel) throws Exception { byte[] bytes = message.getBody(); System.out.println(new String(bytes)); channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); }} rabbitmq的路由方式一般来讲rabbitmq的路由方式分为三种，一种是fanout，即该交换机会将此条消息推送到机绑定在该交换所有队列之中。 topic:该路由器可以支持通配符来进行消息的匹配， 1234@Bean public Binding bindingPublicFundFailExchange(Queue queue, DirectExchange directExchange) { return BindingBuilder.bind(queue).to(directExchange).with(&quot;*.key&quot;); } #可以匹配多个关键字*只可以匹配一个关键字 direct:该路由器会精确匹配队列的key，从而路由器会将指定的消息发送到相应的队列 1234@Bean public Binding bindingPublicFundFailExchange(Queue queue, DirectExchange directExchange) { return BindingBuilder.bind(queue).to(directExchange).with(&quot;error.key&quot;); } topic是fanout和diret居中的一种模式，当topic为#的时候就类似于fanout，当topic的设置不带有* 和 # 的时候，就是一个directe 了。 vhost多租户模式在一个企业中，肯定不会是一个业务系统会使用rabbitmq，那么假设每一个业务系统都搭建一个自己的rabbitmq服务，此时就会造成极大的浪费。最好的解决办法是搭建一个集团一起使用的rabbbitmq集群，然后通过rabbitmq的vhost来进行一个隔离。 vhost模式类似于docker环境，一个vhost就是一个docker镜像，每一个vhost里面的交换机和队列都是互相隔离的。在rabbitmq中，默认的vhost就是一个\\，","link":"/2018/11/07/Rabbitmq%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%80/"},{"title":"Rabbitmq深度学习二","text":"消息的接收和拒绝当消费段接收到一个消息之后，会进行消费的处理。假如在业务中发现该消息是一个错误的消息，那么很显然业务方会直接拒绝此条消息。 拒绝消息拒绝消息一般在消费端调用reject或者neck，这两个Api的作用分别是：一个可以批量消息，一个则只是每一次处理一条 接收消息一般直接调用basic.ack即可 发送方的消息确认消息的确认既然rabbitmq作为一款成熟的消息中间件，那么自然也有完善的消息确认机制。消息确认机制分为生产者端和消费者端，生产者端的消息确认主要是用于确认消息是否成功的到达交换机以及其绑定的队列。 使用生产者端的消息确认需要实现RabbitTemplate.ConfirmCallback和abbitTemplate.ConfirmCallback，并且重写confirm方法，例如一下代码: 1234567891011@Servicepublic class MqConfirmCallback implements RabbitTemplate.ConfirmCallback { @Override public void confirm(CorrelationData correlationData, boolean b, String s) { if(b) { System.out.println(&quot;消息确认成功&quot;); }else{ System.out.println(&quot;消息确认失败&quot;); } }} 1234567@Servicepublic class MqReturnCallback implements RabbitTemplate.ConfirmCallback { @Override public void confirm(CorrelationData correlationData, boolean b, String s) { System.out.println(&quot;确认回调被出发&quot;); }} 当重写完这两个方法之后还不行，另外还需要的是在RabbitTemplate设置其setConfirmCallback和setReturnCallback,然后即可使用。 1234567891011@Autowired MqConfirmCallback mqConfirmCallback; @Autowired MqReturnCallback mqReturnCallback; @PostConstruct public void init(){ rabbitTemplate.setConfirmCallback(mqConfirmCallback); rabbitTemplate.setReturnCallback(mqReturnCallback); } 此时启动生产者但是却不启动消费者，然后发送一条消息，在控制台可以看到打印出来了消息确认成功,此时表明消息已经成功的到达了交换机，那么此时如果我将绑定在交换机上的所有队列删除呢? 删除交换机以及队列来测试此时消息因为到不了交换机，控制台会打印确认回调被出发和消息确认成功，至于此处明显是由于消息路由不到交换机，而导致消息被Return。那么为啥那么还会出发一次消息确认成功呢？。查看Rabbitmq的文档发现： For unroutable messages, the broker will issue a confirm once the exchange verifies a message won’t route to any queue (returns an empty list of queues). If the message is also published as mandatory, the basic.return is sent to the client before basic.ack. The same is true for negative acknowledgements (basic.nack) 意思就是当消息不可路由的时候，则broker会在basic.ack之前发送一个basic.return。那么也就不奇怪了为什么这里会收到两条消息 消费者拒绝测试既然mq的发送方会有一个确认，那么如果消费者拒绝了此条消息，发送方还会收到提示吗？可以看到在控制台只会打印消息确认成功，也就是说发送方只会确认此条消息到达了交换机，而且交换机可以路由到指定对的队列。 不可重复在一个message进行多次操作假设在一个消息上进行了拒绝之后，然后再进行确认，此时mq会抛出一个异常： 1Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; reason: {#method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - unknown delivery tag 1, class-id=60, method-id=80) 不要将错误的消息重新归队若一个消息已经明确知道会导致消费方异常，则不要将此条消息拒绝然后重新写到队列，否则会导致一个循环，从而阻塞后面的消息","link":"/2018/11/10/Rabbitmq%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BA%8C/"},{"title":"Redis中String的实现细节","text":"String 是 Redis 中的基本数据结构之一，也是日常开发中使用最多的场景，例如秒杀扣库存，token缓存，详情缓存等，使用的频率还是比较高的，但是 Redis 中的 String 实现还是比较复杂的。 String 最底层的数据结构还是 char[]，但是 Redis 在对数组进行封装的时候，做了一些细节上的优化 Redis 对象在 Redis 中，每一个 Key 都可以称之为一个对象，Redis 包含了这个 Key 的类型，value 的内存地址，LRU 淘汰时间，引用计数等 1234567struct RedisObject { int4 type; int 4 encoding; int24 lru; int32 refcount; void *ptr;} type 表示这个对象的类型：String、list、set、zset、hash、stream、module encoding 表示该 Key 的 value 以什么类型存储，在 Redis 中有如下几个：1234567891011#define OBJ_ENCODING_RAW 0 /* Raw representation */#define OBJ_ENCODING_INT 1 /* Encoded as integer */#define OBJ_ENCODING_HT 2 /* Encoded as hash table */#define OBJ_ENCODING_ZIPMAP 3 /* Encoded as zipmap */#define OBJ_ENCODING_LINKEDLIST 4 /* No longer used: old list encoding. */#define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */#define OBJ_ENCODING_INTSET 6 /* Encoded as intset */#define OBJ_ENCODING_SKIPLIST 7 /* Encoded as skiplist */#define OBJ_ENCODING_EMBSTR 8 /* Embedded sds string encoding */#define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */#define OBJ_ENCODING_STREAM 10 /* Encoded as a radix tree of listpacks */ lru 代表的是该 Key 被最后一次访问的时间 refcount 表示的被引用的次数，当refcount 为 0 的时候，对象就会被回收 *prt 代表的是内存指针，指向真正存储该 value 的内存区域 SDSSDS 是 Redis 中字符串的具体实现，其结构如下： 123456struct SDS { int8 len; int8 alloc; int8 flags; byte[] buf;} len 表示的当前所使用的长度 alloc 表示的是分配的内存大小（去除了 sdshrd 以及末尾的结束字符） flags 则是一个 8bits 的变量，前三位表示类型，后 5 位暂时还没使用 buf 是字符串具体存放的地址 flags 的几种含义Redis 中的 String 有几种 sdshdr，分别是： 12345#define SDS_TYPE_5 0 最大长度：1&lt;&lt;5 = 32#define SDS_TYPE_8 1 最大长度：1&lt;&lt;8 = 256#define SDS_TYPE_16 2 最大长度：1&lt;&lt;16 = 65535#define SDS_TYPE_32 3 最大长度：1&lt;&lt;32 = 2^32#define SDS_TYPE_64 4 最大长度：1&lt;&lt;64 = 2^64 在这里面，SDS_TYPE_5 基本不会使用，因为只能存放 32 个字符串，有一种特情况是，如果设置的字符串长度为0，会直接使用 SDS_TYPE_8，而不是 SDS_TYPE_5 字符串的存储方式假设现在分别有两个字符串需要存储，分别是 a 和 aaa…aaa（100）个，常见的做法是直接为这两个字符和都开辟一块内存。 但是 Redis 使用的是内存进行存储的，可以存储多少数据完全取决于内存容量的大小，在内存不变的情况下，能存储多少数据取决的每一个 value 的大小，所以 Redis 的作者对 String 的存储进行了优化。 前面提到过 Redis 的对象有一共有五个字段，其分别如下 在 64位操作系统的机器上，Redis 对象的大小是 64 bits，那么剩下的 48bits 是否就可以利用起来存储 String 呢？ 于是 embstr 这种存储方式就出现了 embstr上面提到 Redis对象剩下的可存储容量是 48bits，那么可以真正用来存储字符串的有多少呢？ 阈值计算首先 RedisObject 自己就需要占用 16 个字节，所以剩下可以给 SDS 用的也就是 64 - 16 = 48 字节 前面提到了 SDS 的结构，首先 sdshdr 就需要占用 3 个字节，而且 sds 字符串结尾需要以 NULL 结尾，所以由占用了一个字节，那么最终的阈值就是 44 也就是当设置的字符串长度小于等于 44 的时候，Redis 将会直接将 SDS 拼接到 ptr 之后，避免新开辟一块内存区域 Redis 测试在这里设置了一个Key a，它的长度刚好是 44，于是对 a 进行debug 1234127.0.0.1:6379&gt; set a '12345678901234567890123456789012345678901234'OK127.0.0.1:6379&gt; debug object aValue at:0x7f82e06150c0 refcount:1 encoding:embstr serializedlength:21 lru:14548208 lru_seconds_idle:3 可以看到 a 的编码方式刚好是 emdstr，那么再增加一个字符呢？ 1234127.0.0.1:6379&gt; set b '123456789012345678901234567890123456789012345'OK127.0.0.1:6379&gt; debug object bValue at:0x7f82e0666640 refcount:1 encoding:raw serializedlength:21 lru:14548247 lru_seconds_idle:2 可以看到 b 的编码方式已经变成 RAW 了。 RAW 编码当字符串长度大于 44 以后，就会将编码方式改为 RAW，Redis 会新开辟一块内存区域来存储 SDS，并且将 ptr 指针指向该内存区域 1234567891011121314151617robj *createObject(int type, void *ptr) { robj *o = zmalloc(sizeof(*o)); o-&gt;type = type; o-&gt;encoding = OBJ_ENCODING_RAW; // *ptr 是 SDS 的内存指针，然后将 Redis 的 ptr 指向 SDS o-&gt;ptr = ptr; o-&gt;refcount = 1; /* Set the LRU to the current lruclock (minutes resolution), or * alternatively the LFU counter. */ if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LFU) { o-&gt;lru = (LFUGetTimeInMinutes()&lt;&lt;8) | LFU_INIT_VAL; } else { o-&gt;lru = LRU_CLOCK(); } return o;} 在使用 Redis 的时候，常用的还有 incr 和 incrBy，那么 Redis 又会以怎样的方式存储呢？ int当通过 set 命令设置的 value 可以被转换为 long 类型的时候，Redis 就会尝试将其作为 int 的方式进行存储，int 方式存储本质上也是将其和 ReidsObject 放在一起 1234127.0.0.1:6379&gt; set a 123OK127.0.0.1:6379&gt; object encoding a&quot;int&quot; Redis 是如何进行判断的？在 src/object.c 文件中，tryObjectEncoding 方法详细的做了详细的判断，在这里有一个小细节就是 Redis 采用了一个对象池来复用 1 到 10000 之间的数字，复用的条件限制如下： Redis 没有设置最大内存限制 Redis 的设置了淘汰算法，但是淘汰算法不是 LRU 或者 LFU 12345678910111213141516171819202122232425262728293031323334353637383940414243/* Try to encode a string object in order to save space */robj *tryObjectEncoding(robj *o) { long value; sds s = o-&gt;ptr; size_t len; // 断言必须是 String serverAssertWithInfo(NULL,o,o-&gt;type == OBJ_STRING); // 其目前的 encoding 方式必须是 RAW 或者是 embStr if (!sdsEncodedObject(o)) return o; // 共享变量不参与encoding if (o-&gt;refcount &gt; 1) return o; len = sdslen(s); // 如果长度大于 20 并且 可以转化为long类型 if (len &lt;= 20 &amp;&amp; string2l(s,len,&amp;value)) { /* 如果没有内存限制或者 没有 将 Key 的淘汰策略设置为 LRU or LFU 并且 value &gt;=0 切 小于 10000，则使用共享策略 */ if ((server.maxmemory == 0 || !(server.maxmemory_policy &amp; MAXMEMORY_FLAG_NO_SHARED_INTEGERS)) &amp;&amp; value &gt;= 0 &amp;&amp; value &lt; OBJ_SHARED_INTEGERS) { decrRefCount(o); incrRefCount(shared.integers[value]); return shared.integers[value]; } else { // 如果不满足上面的 if 条件，且原编码是 raw， 则还是设置为 int if (o-&gt;encoding == OBJ_ENCODING_RAW) { sdsfree(o-&gt;ptr); o-&gt;encoding = OBJ_ENCODING_INT; o-&gt;ptr = (void*) value; return o; } else if (o-&gt;encoding == OBJ_ENCODING_EMBSTR) { // 如果是 embStr，则 decrRefCount(o); return createStringObjectFromLongLongForValue(value); } } // 省略}","link":"/2021/07/01/Redis%E4%B8%ADString%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/"},{"title":"Redis中ziplist的实现(一)","text":"ziplist 是一个双向链表，但是不同于熟知的利用头指针和尾指针所形成的双向链表，ziplist 而是采用了一个特殊的实现 区别普通的双向链表因为在每一个节点上都含有一个头节点和尾节点，所以在遍历的时候可以依次沿着每一个节点进行递归，而且在新增或者删除的时候，直接移动指针即可，并且每一个节点在内存中并不要求是连续的，只需要指针指向对应节点的内存地址即可。 但是 redis 使用的是内存，在内存满了的情况下就会造成 redis 的不可用，需要依据淘汰策略清理内存。 试想下如果 ziplist 像普通的双向链表一样，每一次新增一个 entry 都去申请一块新的内存，实现起来确实比较简单。 但是由于每一次的 value 大小都是不一样的，每一次都去申请一块新的内存，很容易造成很多的内存碎片。现象之一就是，如果后续需要新增一个 value，却发现内存中还有很多内存碎片，但是大小都比当前 value 小，造成了 redis 不得不提前进行淘汰 所以 redis 作者在 Hash、zset、set 这三种结构中加入了一个额外的逻辑，如果数据量小则会使用 ziplist 作为其数据的存储结构 ziplistziplist \b会在初始化的时候直接分配一整块内存，默认初始化的大小为 ZIPLIST_HEADER_SIZE+ZIPLIST_END_SIZE，而这两个值的大小分别是 (4 * 2 + 2) + 1 = 11字节 \b\b\b结构在 ziplist.c 文件的顶部注释中，其结构如下： &lt;zlbytes&gt; &lt;zltail&gt; &lt;zllen&gt; &lt;entry&gt; &lt;entry&gt; ... &lt;entry&gt; &lt;zlend&gt; zlbytes\b用来计算整个 ziplist 大小，主要用于 resize，大小为 4bytes zltail这个表示 ziplist 从第一个字节到\b最后一个 entry 的偏移量，主要用于在尾部新增元素使用 zllen表示整个 ziplist 中 entry 的数量，最大值为 2^16-1， zlend作为一个特殊的尾节点，大小为1个 byte，这个节点存在的意义就是方便进行尾插 其中 zltail 可以理解普通链表的尾节点， entryentry 的结构如下：&lt;prevlen&gt; &lt;encoding&gt; &lt;entry-data&gt;其中 prevlen 表示上一个节点的长度，方便进行链表从尾部向头部进行遍历，在遍历的过程中，只需要移动 prevlen 就可以得到前一个 entry 地址而 encoding 则代表该 content 的类型以及长度，按照 redis 源码的注释，整理如下： String如果存储内容是 String，那么 encoding 的值会随着 string 的长度改变而改变 例如：如果要存储的字符串是长度为 30 的全英文字符串（或者字节长度为30的中文）那么这个 entry 所的结构就如下图所示 其中 prevlen 代表上一个 entry 的长度，当从尾部进行遍历，可以直接偏移 prevlen 个长度然后直接得出上一个 entry 的内存地址。由于存储的字符串长度小于 63，则采用「00pppppp」的方式存储字符串长度，而 30 个长度对应的 16 进制正好是 1e，所以这个字节就存储的是 1e，而后续的 data 存储的就是真正的原始内容了 tips:一个字节=2个16进制数，=8个2进制数 int如果存储的内容是 int 类型，那么 ziplist 的 encoding 则会通过另一种方式进行存储其中 encoding 有 5 中类型，分别对应的是 int 的几个大小 这里还有一个小细节：因为 ff(11111111) 这种 encoding 表示的是尾节点，但是在 f0(11110000) 至 fe(11111110) 之间还有 4 个 bit 没有被使用，分别是 0001 至 1101，也就是 1 到 13，于是 redis 的作者就想，能不能把这部分的空间利用起来呢？ 所以如果 int 的值处于 1-13 之间，那么此时直接和 encoding 存储在一起，避免额外开辟一个字节的空间来存储数据，减少内存的使用","link":"/2021/07/12/Redis%E4%B8%ADziplist%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B8%80/"},{"title":"Redis中ziplist的实现二","text":"添加 entryziplist 的添加过程大致如下： 如果是一个新建的 ziplist，其中新增元素的时候，直接将 prevlen 设置为 0 即可 如果是在中间位置新增，那么首先需要获取前一个 entry 的长度，因为后面的元素已经计算过了，因此直接拿来用即可 如果是在尾部进行新增元素，那么需要计算其前一个元素的长度，因为 zlend 不会保存前一个最后一个尾节点的长度，因此需要在新增的时候计算 连锁更新由于 ziplist 的设计，当前一个元素长度小于 254 的时候， prevlen 会以一个字节进行存储，但是当前一个元素大于 254 的时候，那么 prevlen 就会变成 5 个字节 1234567#define ZIP_DECODE_PREVLENSIZE(ptr, prevlensize) do { if ((ptr)[0] &lt; ZIP_BIG_PREVLEN) { (prevlensize) = 1; } else { (prevlensize) = 5; } } while(0); 其中前一个字节固定为 FE，后面四个字节存储的是前一个 entry 的长度，这个也很好理解， 为什么是 254因为 ziplist 也需要像其他的双向链表一样，需要有一个 head 和 tail 来表示头尾，head 节点因为前面没有 entry，所以 prevlen 是 0，而尾节点 redis 作者就将其设置为 255，当遍历的时候，如果遇到 prevlen 的值是 255 的就表示该节点是一个尾节点。 所以在 redis 的源码中也经常看到 ptr[0] == 255 这种判断来判断是否是尾节点 连锁更新触发的场景因为 redis 的 prevlen 存在两种存储方式，如果前一个节点的长度发生变化，那么不可避免的需要更新后续的 entry例如原有的 ziplist 的 entry 如下： 现在需要在 entry1 和 entry2 之间新增一个 entry，如果新增的 entry 长度大于 254，那么 entry2 的 prevlen 势必会需要扩容 此时由于新增的 entry 长度已经大于 254，所以 entry2 的 prevlen 需要由 1个字节扩展到 5个字节，而且还会进行递归进行下去，一直到后一个节点的 prevlen 不需要更新为止 使用场景在 redis 中，list、zset、hash 在数据量小的时候都会使用 ziplist 作为其存储数据的结构，而且三种结构都支持自定义的配置： 1234567/* Zip structure config, see redis.conf for more information */ size_t hash_max_ziplist_entries; size_t hash_max_ziplist_value; size_t zset_max_ziplist_entries; size_t zset_max_ziplist_value; /* List parameters */ int list_max_ziplist_size; 其中 list 的配置少了一个 _value，是因为 list 这种数据后续无论怎样存储都会是一个双向链表，不用考虑每一次 value 的大小，但是最后如果元素 &gt; list_max_ziplist_size，则会变成 quicklist","link":"/2021/07/18/Redis%E4%B8%ADziplist%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BA%8C/"},{"title":"Redis实现的分布式锁是完美的吗？","text":"单实例setnx 是 Redis 官方提供的一个分布式原子性锁，它的实现利用了 Redis 在执行命令的时候是一个原子性操作，所以可以实现同一时间只有任务才能获取到锁。 当在同一个 redis 实例中进行加锁的操作的时候，如果加锁成功则会返回1，如果加锁失败的话，则是直接返回 0 12127.0.0.1:6379&gt; setnx 'redislock' 1(integer) 1 如果此时另一个任务想进行加锁的话，则会返回0： 12127.0.0.1:6379&gt; setnx 'redislock' 1(integer) 0 虽然这样设置就可以实现一个分布式锁，但是如果一个客户端进行了加锁操作，后续自己的系统异常导致进程挂掉了，此时就会导致没有任务来进行解锁，从而导致任意一个客户端都无法再次加锁。 超时时间redis官方提供了一个命令来支持这个加锁和设置超时时间的原子性命令： SET key value [EX seconds] [PX milliseconds] [NX|XX] 其中 NX 和 XX 的区别在于，NX 是只有当 key 不存在的时候才设置，而 XX 则是当 key 存在的时候才进行设置。 12127.0.0.1:6379&gt; set k1 v1 EX 10 NXOK 这个命令表示当 k1 不存在的时候，设置该 key ，并且将其超时时间设置为 10s。 这个时间是一般是由业务人员根据业务的特性来指定的，当加入了超时时间以后，如果该 key 在超时时间内没有被主动的调用 del 命令，等超时时间过了以后，该 key 会自动被删除。此时另外一个系统就可以对这个 key 加锁了。 这样设置以后虽然可以解决上面的锁无法释放的问题，但是却又有一个新的问题，就是锁被他人释放了，例如： 这种情况对于业务来说，是绝对不可以接受的，因为对于A任务来讲，它虽然释放了锁，但是它释放的其实是B的锁，此时如果有一个C任务再来加锁，就可以加锁成功了，于是分布式锁就变成了B、C可能在同一时间都在执行一个任务。 随机数为了保证在只有自己加的锁才能被自己释放，此时每一个任务就需要自己的一个唯一标志，这个标志一定是要全局唯一的。当释放锁的时候，需要判断当前持有锁的ID是否是释放锁的任务ID，但是由于这是一个非原子性的操作，所以此时就需要通过 lua 脚本来执行。 加锁在这里以 lua 脚本为例，lua脚本可以同时传入多个参数，在一个脚本里面执行，这样就可以判断加锁的value是不是当前传入的value。 12127.0.0.1:6379&gt; set k1 A EX 10 NXOK 此时的操作是 A 对 k1 进行加锁，假设 A 是一个全局唯一的，此时 A 对 k1 进行了加锁，并且锁的超时时间是 10s 。 释放锁由于在这里是以 value 来作为唯一标志的，当释放锁的时候需要把当前的 任务ID 作为 value 传入，然后在删除key的时候，通过以下 lua 脚本来释放锁。 lua脚本：12345if (redis.call('exists',KEYS[1]) == 1) and (ARGV[1] == redis.call('get',KEYS[1])) then return redis.call('del',KEYS[1]) else return 0 end; 首先加载 lua 脚本 12127.0.0.1:6379&gt; script load &quot;if (redis.call('exists',KEYS[1]) == 1) and (ARGV[1] == redis.call('get',KEYS[1])) then return redis.call('del',KEYS[1]) else return 0 end;&quot;&quot;51fd717f3d833a79f1a102483df7932d4b71cd69&quot; 此时可以看到返回了一个hash值，这个值就是代表这个函数，当然也可以不用 hash 函数，每次用 eval 函数执行这个文本即可： 12127.0.0.1:6379&gt; eval &quot;if (redis.call('exists',KEYS[1]) == 1) and (ARGV[1] == redis.call('get',KEYS[1])) then return redis.call('del',KEYS[1]) else return 0 end;&quot; 1 k1 B(integer) 0 此时可以看到以 B 尝试释放这个锁，但是返回的是0，并未释放成功，再次查看该锁： 12127.0.0.1:6379&gt; get k1&quot;A&quot; 可以看到 k1 还是被 A 锁持有，尝试以 A 来释放锁： 12127.0.0.1:6379&gt; EVALSHA '51fd717f3d833a79f1a102483df7932d4b71cd69' 1 k1 A(integer) 1 可以看到已经被释放了，所以这个可以解决锁被其它任务释放的问题，但是还是无法解决超时导致的锁释放的问题。 锁超时要解决这个问题，需要对超时时间进行续约，即除非 A服务 自己挂掉了让锁自己超时释放掉，否则就必须让A自己释放掉它。 redissionRedission 是一款基于 Java 的 redis 操作 API 库，它采用的是一个 Watch dog 模式来解决这个问题的，具体做法是后台开启一个线程，这个线程每隔一定的时间去检查该锁还有多久超时，然后给这个锁进行续租。 集群上述 Redis 的分布式锁在单实例的情况下是可以完美运行的，但是一旦涉及到 reids 集群，就会出现重复加锁的情况。假设在一个一主三从的redis架构中， 如果任务 A 对主节点进行加锁成功了，此时主节点突然挂掉了，但是在挂掉之前，其锁没有同步到从节点，此时从节点其中一个晋升为主节点，于是两个任务此时都加锁成功了。 可以看到此时Redis 出现了脑裂情况，在这个情况下，A 和 B 都加锁成功了，但是这也就违背了分布式锁最初的初衷了，于是 RedLock 就被人提出来了。 RedLock该算法是用 Redis CRC16 算法，计算出所有的 master 节点，然后记一个初始化时间，随后对所有的 master 节点进行加锁，计算出加锁的耗时时间，如果加锁的耗时时间小于超时时间，则接下来就可以执行任务了，否则锁过期了，就必须再次尝试再次加锁。 记初始化时间 对所有的master加锁 计算加锁的耗时 判断是否小于过期时间 执行任务 缺陷虽说 RedLock 可以解决某一个 Redis 节点挂掉，导致任务重复执行，但是还是无法避免如下问题： 锁超时 例如如果在第4步的时候，应用出现了阻塞，就会导致锁其实过期了，但是任务 A 在锁过期以后还是在执行。 极度依赖服务器的时间 如果对A、B、C三个服务器进行加锁，任务 A 已经对A、B、C加锁了，但是此时B、C的服务器时间有问题，导致锁被提前释放了，而此时任务B对B、C加锁了，由于一半以上的master的节点已经加锁成功了，所以此时 任务B 其实也加锁成功了。 ​ 总结对于 Redis 的分布式锁，需要了解其可能出现的问题，然后再来做一些决策，任何一个技术都不是完美的，需要根据业务类型来选择最适合的。 参考资料http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html","link":"/2020/08/03/Redis%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%98%AF%E5%AE%8C%E7%BE%8E%E7%9A%84%E5%90%97%EF%BC%9F/"},{"title":"SSO单点登录的理论与实践(一)","text":"SSO简介随着企业业务的发展，企业中可能会出现多个业务的系统。如果是对内使用的话，那稍微还好点，如果是ToC的业务，客户如果每进入一个系统都需要登录的话，对用户来说是一个麻烦事情，很可能会造成用户的流失。 如果在每一个系统里面都存储一份用户的账号和密码数据，这种做法显然是不靠谱的，而且也不安全的。会造成客户的数据大量冗余，而且还会导致后期维护十分的麻烦。 所以，现在一般都会采用SSO单点登录 在介绍SSO登录之前，需要先了解一下浏览器的同源策略 浏览器同源策略此处直接将mozilla官网给出的介绍搬过来https://developer.mozilla.org/zh-CN/docs/Web/Security/Same-origin_policy URL 结果 原因 http://store.company.com/dir2/other.html 成功 http://store.company.com/dir/inner/another.html 成功 https://store.company.com/secure.html 失败 不同协议 ( https和http ) http://store.company.com:81/dir/etc.html 失败 不同端口 ( 81和80) http://news.company.com/dir/other.html 失败 不同域名 ( news和store ) 所以根据浏览器的同源策略，SSO单点登录又分为跨域和非跨域 SSO登录 类别 示例 实现要求 同一个域名下单点登录 a.com/user,a.com/order 用户在访问完用户系统之后，如果进入订单系统，则无需登录拿到用户信息 非同一个域名下SSO登录 a.com/user,b.com/user 当用户访问a系统然后再访问b系统，自动获取用户数据 前后端分离跨域 这个只是上面非同源跨域的一个前后端分离版本 实现细节如上，不过是前端自己一个服务器 而实现SSO登录的一个关键点是用户的唯一标识，即如何确在一个系统中生成的用户凭据在其他几个系统中都可以被识别。 SSO登录同源的实现同源之间的实现比较简单，因为是同源，所以可以直接通过cookie或者jwt实现，具体的流程图如下： 仅仅想实现同源下的SSO登录，实现的方式可以有多种，但是最终的一点就是SSO生成的用户凭据，其他的系统必须要可以解密出来。这样一个同源下的SSO单点登录便可以实现 SSO登录非同源的实现由于是在非同源下实现SSO单点登录，所以一个用户在a.com下登录了账户，那么当用户访问b.com的时候，cookie肯定是带不过去的。这时候不妨参考下业内的公司的设计 淘宝和天猫的实现打开浏览器，在taobao.com下登录我们的账号，成功获取到用户信息之后，这时候我们再新建一个标签页，然后打开tmall.com，这时候会发现，天猫已经自动的帮我们登录了账号。考虑以下问题： 淘宝和天猫的域名并非同源 我们只在一处进行了登录 我们并未在tmall域名下输入任何我们的信息，然后我们再次访问天猫，天猫就自动识别出来我们了 很明显，我们在请求tmall之前，淘宝已经将我们的信息同步到了tmall 所以根据上面的例子，我们需要考虑的是，如何让不同系统之间互相识别用户 常见的非同源SSO登录体系中一般有一个统一的授权中心，再加上一个共用的用户凭据存储中心，如下： 在上图的流程中，不仅可以通过jsonp来进行传输，还可以通过iframe进行通信(淘宝和天猫的做法)， 这个简单版的SSO登录，主要是通过cookies来存储token，然后进行每个跨域系统的交互，当然这个方式是有瑕疵的，因为这种方式在跨域系统多的时候，需要维护多个跨域系统，代码会写的比较多，后期如果新增或者删除一个系统的话，需要需改js文件，就会显得很繁琐。 实现：由于同源下的SSO登录实现起来比较简单，所以此次直接实现前后端分离的SSO单点登录 可以看下非同源下的实现： 所用技术:SprngCloud,redis,vue 首先分别访问http://localhost:8092/#/orderinfo和http://localhost:8091/#/orderinfo,这个时候由于都没有登录，所以直接被重定向到登录界面，但是在http://localhost:8092/#/登录之后，再次访问http://localhost:8091/#/orderinfo，可以看到8092端口的服务直接识别出来了是该用户 这里有两个注意点：首先，我在8092端口的服务器上并未登录过，而且8091服务和8092服务是一个非同源服务，所以很明显8091服务是无法将cookie给到8092服务器。 项目结构：此次的项目结构是一个Auth认证中心，两个后端系统，两个前端系统。而且一个前端分别对应一个后端系统 实现方式首先两个子系统的认证分别是基于cookie，然后将tokan存入cookie，首次登录的时候，cookie肯定是不存在的，然后重定向至SSO登录系统,SSO认证之后会下发一个token，然后登录的系统会将此token以get的方式作为参数传给另一个个系统，另一个系统再将此token存入cookie进行回写，最终实现两个系统都一起登录 ###回调实现 1234567@GetMapping(ApiConstant.APP+&quot;/{token}&quot;) public ResponseUtils getInfo(@PathVariable String token , HttpServletResponse response){ ResponseUtils&lt;String&gt; resp = new ResponseUtils&lt;String&gt;(); ResponseEnum.SUCCESS.setResponse(resp); response.addCookie(new Cookie(&quot;token&quot;,token)); return resp; } 获取SSO登录返回的token然后调用各个跨域系统 1234567891011121314151617setApp1Token (userdata) { console.log(userdata) this.$http({ method: 'get', url: 'http://localhost:8083/api/app1/query/' + userdata }).then(function (res) { console.log(res) }) this.$http({ method: 'get', url: 'http://localhost:8085/api/app2/query/' + userdata }).then(function (res) { console.log(res) }) return 'OK' } }, 项目地址 https://github.com/Somersames/Springboot-vue-sso该项目为了验证，仅仅是用userId进行md5加密作为token，所以比较简陋，但是以后如果有时间会继续写几篇文章同时也将这个项目补起来","link":"/2019/01/13/SSO%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5-%E4%B8%80/"},{"title":"Spring Cloud的Zuul相关总结","text":"Zuul是SpringCloud生态体系中的网关一环，首先简单配置如下：开启注册中心并且配置yml文件，如下： 12345678910111213spring: application: name: somersames-eruekaserver: port: 8081eureka: client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://localhost:8081/eureka/ 开启用户的微服务： 123456789spring: application: name: somersames-userserver: port: 8082eureka: client: service-url: defaultZone: http://localhost:8081/eureka 编写一个测试的Controller： 12345678@RestController@RequestMapping(&quot;testuser&quot;)public class UserTestController { @GetMapping(&quot;/testzuul&quot;) public String testZUul(){ return &quot;测试Zuul&quot;; }} 配置注册中心： 123456789101112131415spring: application: name: somersames-zuuleureka: client: service-url: defaultZone: http://localhost:8081/eurekazuul: routes: user-route: url : http://localhost:8082 //用户微服务的地址 path : /user/** //映射的路径server: port: 8083 设置Zuul启动类的注解： 1234567@SpringBootApplication@EnableZuulProxypublic class ZuulConfigApplication { public static void main(String[] args) { SpringApplication.run(ZuulConfigApplication.class); }} 测试：使用微服务的SericiceId访问： 使用Zuul的path访问：","link":"/2018/05/23/Spring-Cloud%E7%9A%84Zuul%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/"},{"title":"SpringBoot2.x是怎样只初始化LettuceConnectionFactory的呢","text":"SpringBoot1.5使用Redis和2.x的区别在 SpringBoot1.5 的版本的时候，如果要创建一个 RedisTemplate 的话，那么可以直接使用如下代码: 1234567@Beanpublic RedisTemplate&lt;String,Object&gt; redisTemplate(JedisConnectionFactory jedisConnectionFactory){ RedisTemplate&lt;String,Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(jedisConnectionFactory); // 添加序列化代码 return redisTemplate；} 然后在业务类中直接通过 @Autowired 注解来调用 redisTemplate，但是如果将 SpringBoot1.5 升级到 2.0 之后，你会发现这样写的话，SpringBoot 启动的时候会报错。报错内容如下： 12345The following candidates were found but could not be injected:- Bean method 'redisConnectionFactory' in 'JedisConnectionConfiguration' not loaded because @ConditionalOnBean (types: org.springframework.data.redis.connection.RedisConnectionFactory; SearchStrategy: all) found beans of type 'org.springframework.data.redis.connection.RedisConnectionFactory' redisConnectionFactory Consider revisiting the entries above or defining a bean of type 'org.springframework.data.redis.connection.jedis.JedisConnectionFactory' in your configuration. 这个提示说明了 redisConnectionFactory 没有被加载到，这个时候先去官方文档上看下 changelog，然后在看下是不是有什么变化。SpringBoot1.x升级到2.X的简介 123456RedisLettuce is now used instead of Jedis as the Redis driver when you use spring-boot-starter-data-redis. If you are using higher level Spring Data constructs you should find that the change is transparent.We still support Jedis. Switch dependencies if you prefer Jedis by excluding io.lettuce:lettuce-core and adding redis.clients:jedis instead.Connection pooling is optional and, if you are using it, you now need to add commons-pool2 yourself as Lettuce, contrary to Jedis, does not bring it transitively. 也就是说 SpringBoot2.x 已经将 LettuceConnectionFactory 作为官方的 Redis 连接工具了，那么尝试着将 LettuceConnectionFactory 作为那个 redisTemplate 的入参，最后调整如下： 1234567@Beanpublic RedisTemplate&lt;String,Object&gt; redisTemplate(LettuceConnectionFactory lettuceConnectionFactory){ RedisTemplate&lt;String,Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(lettuceConnectionFactory); // 序列化代码 return redisTemplate;} 于是启动就不再报错了。而且也可以正常的使用。但是如果你还是想使用 Jedis 的话，直接 exclued io.lettuce:lettuce-core 即可。 为什么在SpringBoot2.x中两个RedisConnectionFactory只会加载一个SpringBoot2.x中加载的加载机制redisConnectionFactory的实现首先 Redis 的一些配置都是依赖于 RedisAutoConfiguration，这个类是在 spring-boot-autoconfigure 里面，首先看下这个类的大体结构： 12345678910111213141516171819202122232425@Configuration(proxyBeanMethods = false)@ConditionalOnClass(RedisOperations.class)@EnableConfigurationProperties(RedisProperties.class)@Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class })public class RedisAutoConfiguration { @Bean @ConditionalOnMissingBean(name = &quot;redisTemplate&quot;) public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; }} 首先这个类上面有四个注解，那么这四个注解的作用如下： @Configuration(proxyBeanMethods = false)，声明这是一个配置类，同时也说明了不要让 SpringBoot 为这个类生成代理类 @ConditionalOnClass(RedisOperations.class)，当 classPath 中出现了 RedisOperations 这个类之后，该类才会加载成一个Bean @EnableConfigurationProperties(RedisProperties.class)，将配置类 RedisProperties 加载进来，由于 @Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class }) 在上述的几个注解里面，最重要的是 @Import 注解，这个注解可以允许我们在 TODO LettuceConnectionConfiguration 和 JedisConnectionConfiguration当查看这两个类里面的方法的时候，会发现在创建 redisConnectionFactory 这个Bean的时候，都会有一个 @ConditionalOnMissingBean(RedisConnectionFactory.class) 注解。 12345678910111213141516@Bean@ConditionalOnMissingBean(RedisConnectionFactory.class)LettuceConnectionFactory redisConnectionFactory( ObjectProvider&lt;LettuceClientConfigurationBuilderCustomizer&gt; builderCustomizers, ClientResources clientResources) throws UnknownHostException { LettuceClientConfiguration clientConfig = getLettuceClientConfiguration(builderCustomizers, clientResources, getProperties().getLettuce().getPool()); return createLettuceConnectionFactory(clientConfig);}@Bean@ConditionalOnMissingBean(RedisConnectionFactory.class)JedisConnectionFactory redisConnectionFactory( ObjectProvider&lt;JedisClientConfigurationBuilderCustomizer&gt; builderCustomizers) throws UnknownHostException { return createJedisConnectionFactory(builderCustomizers);} 很明显，这两个注解都是当 RedisConnectionFactory.class 这个 Bean 不存在的时候才会创建，而且在 RedisAutoConfiguration 中，首先 import 的是 LettuceConnectionConfiguration，所以最后才会导致在 SpringBoot2.x 的时候，默认加载的 redisConnectionFactory 是 LettuceConnectionFactory。","link":"/2020/01/05/SpringBoot2-x%E6%98%AF%E6%80%8E%E6%A0%B7%E5%8F%AA%E5%88%9D%E5%A7%8B%E5%8C%96LettuceConnectionFactory%E7%9A%84%E5%91%A2/"},{"title":"SpringBoot中异步线程的处理","text":"在工作或者学习的时候，我们都会接触到异步编程，大多数情况下都是通过新建一个线程池，然后调用submit方法或者execute方法来执行。如下： 1234567891011121314151617public void simpleThreadPool(){ ExecutorService executor = new ThreadPoolExecutor(4,5,0, TimeUnit.SECONDS,new LinkedBlockingDeque&lt;Runnable&gt;()); executor.execute(new Runnable() { @Override public void run() { System.out.println(&quot;run&quot;); } }); Callable&lt;String&gt; callable = new Callable&lt;String&gt;() { @Override public String call() throws Exception { return &quot;callable&quot;; } }; Future future = executor.submit(callable); System.out.println(future.get()); } 在Springboot中其实也可以这样做，但是不利于后期的维护，加入后期需要把 runable 的方法修改为同步类型的，那么此时就需要大量的改动代码，如果说很多地方都用到的了的话，就会很容易漏掉了一处导致bug的产生。 Spring的解决方法不需要返回值的异步其实在Spring中就有类似的解决方法，只不过需要我们自己配置。首先新建一个配置类： 1234567891011121314@Configurationpublic class ThreadConfig { @Bean(&quot;asyncPool&quot;) public ThreadPoolTaskExecutor asyncPool(){ ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setMaxPoolSize(10); executor.setQueueCapacity(100); executor.setDaemon(true); executor.setKeepAliveSeconds(30); return executor; }} 在这里最好的做法是将其配置到配置文件中，这样以后调整就不需要改动代码，不过此处为了演示，也就直接固定了。 新建一个 Service 测试。 1234567891011121314@Servicepublic class AsyncService { @Async(&quot;asyncPool&quot;) public void sayA(){ System.out.println(&quot;A&quot;); } @Async(&quot;asyncPool&quot;) public void sayB() throws InterruptedException { Thread.currentThread().sleep(1000); System.out.println(&quot;B&quot;); }} 调用 123456789101112@Servicepublic class TestService { @Autowired AsyncService asyncService; public void noReturnAsync() throws InterruptedException { asyncService.sayB(); asyncService.sayA(); }} 新建一个测试类 123456789101112131415@RunWith(SpringRunner.class)@SpringBootTest(classes = AsyncApplication.class)@WebAppConfigurationpublic class TestServiceTest { @Autowired TestService testService; @Test public void noReturnAsync() throws InterruptedException { testService.noReturnAsync(); Thread.currentThread().sleep(2000); } } 此时在控制台会发现是先打印的 A，然后再打印的 B，所以这里可以肯定确认的是肯定是异步执行的。 但是一般情况下，有一个业务方法并不是通用的，假如有一个 C 方法，这个方法是 TestServcie 类里面单独一个人使用的，这个情况下如果需要在 TestServicde 里面使用的话，那么就需要通过 Aop 来获取当前的代理对象。如下： 1234567891011121314151617@Servicepublic class TestService { @Autowired AsyncService asyncService; public void noReturnAsync() throws InterruptedException { asyncService.sayB(); asyncService.sayA(); C(); } @Async(&quot;asyncPool&quot;) public void C(){ System.out.println(&quot;C&quot;); }} 如果这样写的话，C方法就会被当成一个同步方法，于是就需要通过 AopContext.currentProxy() 来切换代理对象 1234567891011121314151617@Servicepublic class TestService { @Autowired AsyncService asyncService; public void noReturnAsync() throws InterruptedException { asyncService.sayB(); asyncService.sayA(); ((TestService) AopContext.currentProxy()).C(); } @Async(&quot;asyncPool&quot;) public void C(){ System.out.println(&quot;C&quot;); }} 如果是使用 ((TestService) AopContext.currentProxy()).C() 的话，则必须要新增如下Bean 12345678@Componentpublic class AsyncBeanFactoryPostProcessor implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { BeanDefinition beanDefinition = beanFactory.getBeanDefinition(org.springframework.scheduling.config.TaskManagementConfigUtils.ASYNC_ANNOTATION_PROCESSOR_BEAN_NAME); beanDefinition.getPropertyValues().add(&quot;exposeProxy&quot;, true); }} 需要返回值的异步上面介绍的都是不需要返回值的异步方法，那么其实很多场景下都是需要返回值的，此时可以通过如下方法来实现: 12345@Async(&quot;asyncPool&quot;) public Future&lt;String&gt; futureA() throws InterruptedException { Thread.currentThread().sleep(1100); return new AsyncResult&lt;String&gt;(&quot;A&quot;); } 调用方式还是和之前一直，就是最后需要用一个 Future 来接。 1234567public void noReturnAsync() throws InterruptedException, ExecutionException { Future future = asyncService.futureA(); asyncService.sayB(); asyncService.sayA(); System.out.println(future.get()); ((TestService) AopContext.currentProxy()).C(); }","link":"/2020/04/02/SpringBoot%E4%B8%AD%E5%BC%82%E6%AD%A5%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%A4%84%E7%90%86/"},{"title":"SpringBoot中在一个事物中更新多表的注意事项","text":"现象：具体表现为数据被update之后，在同一个事物里面再次查询，查询的是一个更新之后的值。 复现步骤更新一个商品的信息，其步骤如下： 更新商品表的一些数据 在进展表中新加入一条申请 在操作日志表中新增各种变更的操作(记录变更之前的值，变更之后的值) 由于之前是每一个数据的操作都是独立的一个方法，所以其代码结构如下所示: 123456@Transactionalpublic void ceateProductEditRequest(){ public void editProduct(); public void createNewProductProgress(); public void insertLogs();} 但是这样的操作顺序会出现一个问题，因为在同一个事物中。首先执行了更新商品的操作，然后在进展表中新增一个记录。一直到这里 在此之前一直都是没问题的，然而在第三步的时候，由于数据库中已经将商品表的数据进行了更新，所以此时查询出来的是一个更新之后的值。但是日志表中是需要记录申请的前后值得变化。所以此时就需要调整方法的顺序。 方案同一个事物里面，在更新之后进行的查询语句，查询出的是更新之后的语句，如果要实现上述的业务场景，即需要将insertLogs方法提至editProduct之前。 123456@Transactionalpublic void ceateProductEditRequest(){ public void insertLogs(); public void editProduct(); public void createNewProductProgress();} 由于在同一个事物里面，且该业务场景实际中并不多。所以这样简单处理了下。 但是这样的写法在并发高的情况下，需要考虑数据库的锁设计，防止出现了死锁，例如数据库开启了GAP锁或者Next-Key Lock","link":"/2019/01/02/SpringBoot%E4%B8%AD%E5%9C%A8%E4%B8%80%E4%B8%AA%E4%BA%8B%E7%89%A9%E4%B8%AD%E6%9B%B4%E6%96%B0%E5%A4%9A%E8%A1%A8%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"},{"title":"SpringCloud配置中心的使用","text":"在实际的开发过程中，很可能会涉及到很多的开发环境，常见的例如 dev , product 等，在使用SpringCloud的时候可以通过配置中心微服务，结合 Git 管理工具实现配置的集中式管理。 配置中心config配置中心的yml文件： 123456789101112131415spring: application: name: sc-config cloud: config: server: git: uri: ${GitAddress:https://gitee.com/somersames/spring-cloud-demo-config} search-paths: ${GitPath:dev}eureka: client: service-url: defaultZone: http://${EurekaHost:localhost}:${EurekaPort:8081}/eureka/server: port: 8888 这里的 ${A:B} 配置表示的是如果 A 获取不到就取 B 的值。search-paths 则是代表从哪个文件夹中获取配置文件，可以使用 , 来进行分割。然后在启动类中添加如下几个注解，一个配置中心的微服务便开启了。 123456789@SpringBootApplication@EnableConfigServer@EnableAutoConfiguration@EnableEurekaClientpublic class ConfogApplication { public static void main(String[] args) { SpringApplication.run(ConfogApplication.class); }} 获取配置中心的配置当其他几个微服务需要通过配置中心获取配置文件的时候，需要添加一些配置文件才可以。 123456789101112spring: profiles: active: master application: name: sc-auth cloud: config: discovery: service-id: sc-config #注册中心的ServiceId enabled: true label: master #表示的是分支 profile: config #表示的是一个标签 那么这个微服务在拉取配置文件的时候就会拉取 sc-auth-config.yml 的配置","link":"/2018/06/11/SpringCloud%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"Springboot中使用Mysql多数据源","text":"随着业务的发展，很可能需要在一个项目里面同时使用多个数据源。 大致看了网上的多数据源Demo，发现无非有两种： 一种是自己封装多个JdbcTemplate，然后调用对应的数据库就使用对应的JdbcTemplate 一种是通过注解的方式来实现，在需要切换数据源的方法上添加一个自己封装的注解便可以完成切换。 考虑了一下以后的扩展性和通用性，便决定采用基于注解的多数据源方式 分析看了下官网的介绍，大致了解了在Spring中使用多数据源的一个关键类是AbstractRoutingDataSource。 先看下这个类的结构 在这个类里面，只需要关注一下几个变量或者方法即可。 private Map&lt;Object, Object&gt; targetDataSources; private Object defaultTargetDataSource; protected Object determineCurrentLookupKey() targetDataSources这个变量是一个存储数据源的Map，其实一般在使用的时候，更加像是Map&lt;String, DataSource&gt;这样的，其中key表示的是这个数据源的名称，而value则是表示这个DataSource的信息(例如url，username等) defaultTargetDataSource该变量表示的是一个默认的数据源，非空，必须设置 determineCurrentLookupKey该方法返回的一个targetDataSources里面的键，用于选择某一个数据源。其中多数据源的切换就是控制该方法的返回值来实现。 该方法返回的是targetDataSources里面的键，从而HikariPool可以直接切换数据源 思路首先是通过读取配置文件，将其转为DataSource，然后再将dataSource存入targetDataSources 代码配置文件 1234567891011121314151617spring: application: name: multi-resource datasource: mysql1: driver-class-name: com.mysql.cj.jdbc.Driver jdbc-url: jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC username: root password: 123456 mysql2: driver-class-name: com.mysql.cj.jdbc.Driver jdbc-url: jdbc:mysql://localhost:3306/cloud?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC username: root password: 123456 aop: auto: true proxy-target-class: true 获取配置文件信息 1234567891011121314151617181920@Slf4j@Configurationpublic class MysqlMultiProperties { private static final Logger LOGGER = LoggerFactory.getLogger(MysqlMultiProperties.class); @Bean(&quot;mysql1&quot;) @ConfigurationProperties(&quot;spring.datasource.mysql1&quot;) public DataSource mysql1Source(){ log.info(&quot;正在初始化Mysql_DB1&quot;); return DataSourceBuilder.create().build(); } @Bean(&quot;mysql2&quot;) @ConfigurationProperties(&quot;spring.datasource.mysql2&quot;) public DataSource mysql2Source(){ log.info(&quot;正在初始化Mysql_DB2&quot;); return DataSourceBuilder.create().build(); }} 实现AbstractRoutingDataSource 1234567891011121314151617181920@Slf4jpublic class DataSourceRouter extends AbstractRoutingDataSource { private static final ThreadLocal&lt;String&gt; contextHolder = new ThreadLocal&lt;&gt;(); @Override protected Object determineCurrentLookupKey() { return getDataSource(); } public static void setDataSource(String dataSource) { contextHolder.set(dataSource); } public static String getDataSource() { return contextHolder.get(); }} 实例化Bean 1234567891011121314151617181920212223242526@Configurationpublic class MysqlConfig { @Autowired @Qualifier(&quot;mysql1&quot;) DataSource mysql1; @Autowired @Qualifier(&quot;mysql2&quot;) DataSource mysql2; @Bean @Primary public DataSourceRouter generateRouter(){ DataSourceRouter router =new DataSourceRouter(); Map&lt;Object,Object&gt; targetMap = new HashMap&lt;Object, Object&gt;(); targetMap.put(&quot;mysql1&quot;,mysql1); targetMap.put(&quot;mysql2&quot;,mysql2); router.setTargetDataSources(targetMap); router.setDefaultTargetDataSource(mysql1); router.afterPropertiesSet(); return router; }} AOP切面在使用切面的时候遇到了一些坑，这个有空再说 新建一个注解 1234567@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface UseDataSource { String name();} 新建一个切面类 1234567891011@Slf4j@Component@Aspect@Order(-1)public class MysqlSourceAspect { @Before(&quot;@annotation(useDataSource)&quot;) public void changeMysqlSource(UseDataSource useDataSource){ DataSourceRouter.setDataSource(useDataSource.name()); }} 至此大部分功能都已经实现了，在Springboot的启动类上添加或修改如下注解 12345678@SpringBootApplication(exclude={DataSourceAutoConfiguration.class})@MapperScan(basePackages = &quot;com.somersames.dao&quot;)@Import({MysqlConfig.class})public class ServiceApplication { public static void main(String[] args) { SpringApplication.run(ServiceApplication.class); }} 使用只需要在切换数据源的地方添加@UseDataSource注解即可。 项目地址：https://github.com/Somersames/Multi-Resource 总结这次在编写AOP部分的时候需要了一点小坑，有空会整理出来","link":"/2019/01/06/Springboot%E4%B8%AD%E4%BD%BF%E7%94%A8Mysql%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90/"},{"title":"Spring中AOP的探索与实践(一)之Redis多数据源切换","text":"一般在项目的使用过程中，有时候为了减轻数据库的压力，从而将一部分数据缓存至Redis，但是随着业务量的增多。我们所需要的Redis服务器也会越来越多，就算不需要多个Redis数据源，那么在一个redis里面，切换不同的DB也是很麻烦的一件事情。 非AOP的一般的多数据源操作在Redis的多数据源使用中，一般的方法是从配置文件中读取多个RedisProperties，读取到配置文件之后，将RedisProperties配置到RedisTemplate，然后每次使用的时候就通过不同的Template来调用Redis服务器。示例如下: 代码示例Redis的配置类12345678910111213141516171819202122232425262728293031323334353637383940@Configuration@Order(1)@Slf4jpublic class RedisConfig { @Bean(name = &quot;redis1&quot;) @Primary JedisConnectionFactory jedisConnectionFactory1(){ JedisConnectionFactory jedisConnectionFactory =new JedisConnectionFactory(); jedisConnectionFactory.setHostName(&quot;127.0.0.1&quot;); jedisConnectionFactory.setPort(6379); jedisConnectionFactory.setPassword(&quot;123456&quot;); jedisConnectionFactory.setDatabase(0); return jedisConnectionFactory; } @Bean(name = &quot;redis2&quot;) @ConfigurationProperties(&quot;spring.redis.db1&quot;) JedisConnectionFactory jedisConnectionFactory2(){ JedisConnectionFactory jedisConnectionFactory =new JedisConnectionFactory(); jedisConnectionFactory.setHostName(&quot;127.0.0.1&quot;); jedisConnectionFactory.setPort(6379); jedisConnectionFactory.setPassword(&quot;123456&quot;); jedisConnectionFactory.setDatabase(2); return jedisConnectionFactory; } @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate() { RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(jedisConnectionFactory1()); return template; } @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate2() { RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(jedisConnectionFactory2()); return template; }} 在上面的配置类里面，分别生成了两个JedisConnectionFactory和两个RedisTemplate，那么在使用的时候直接通过注解@Autowired装配两个RedisTemplate即可。 使用示例12345678910111213141516@Servicepublic class RedisCommonService { @Autowired RedisTemplate&lt;String,Object&gt; redisTemplate ; @Autowired RedisTemplate&lt;String,Object&gt; redisTemplate2 ; public void redis1Save(String key,String value){ redisTemplate.opsForValue().set(key,value); } public void redis1Save2(String key,String value) { redisTemplate2.opsForValue().set(key, value); }} 调用12345678910111213141516171819RestController@RequestMapping(&quot;/&quot;)public class RedisCommonController { @Autowired RedisCommonService redisCommonService; @RequestMapping(value = &quot;/redis1&quot;,method = RequestMethod.GET) public void redis1(){ redisCommonService.redis1Save(&quot;1&quot;,&quot;2&quot;); } @RequestMapping(value = &quot;/redis2&quot;,method = RequestMethod.GET) public void redis2(){ redisCommonService.redis1Save2(&quot;2&quot;,&quot;3&quot;); }} 然后在Redis的服务器上可以看到 可以看到两个数据分别写入到了不同的Db中 通过AOP的调用通过AOP方法调用的基础是需要获取RedisTemplate里面的JedisConnectionFactory切面代码如下： 1234567891011121314151617181920212223242526@Around(&quot;execution(* com.somersames.service.redis.RedisService.*(..))&quot;) public void as(ProceedingJoinPoint joinPoint) throws Throwable { Field methodInvocationField = joinPoint.getClass().getDeclaredField(&quot;methodInvocation&quot;); System.out.println(AopUtils.isAopProxy(joinPoint.getTarget())); methodInvocationField.setAccessible(true); ReflectiveMethodInvocation o = (ReflectiveMethodInvocation) methodInvocationField.get(joinPoint); Field h = o.getProxy().getClass().getDeclaredField(&quot;CGLIB$CALLBACK_0&quot;); h.setAccessible(true); Object dynamicAdvisedInterceptor = h.get(o.getProxy()); Field advised = dynamicAdvisedInterceptor.getClass().getDeclaredField(&quot;advised&quot;); advised.setAccessible(true); Object target = ((AdvisedSupport)advised.get(dynamicAdvisedInterceptor)).getTargetSource().getTarget(); Field re = target.getClass().getDeclaredField(&quot;redisTemplate&quot;); re.setAccessible(true); Object re2= re.get(target); Field d = re2.getClass().getSuperclass().getDeclaredField(&quot;connectionFactory&quot;); d.setAccessible(true); Object[] objs = joinPoint.getArgs(); if(objs != null &amp;&amp; objs.length !=0){ re.set(target,applicationContext.getBean((String) objs[0])); } joinPoint.proceed(); } RedisServer123456789101112@Servicepublic class RedisService { @Autowired RedisTemplate&lt;String,Object&gt; redisTemplate; public void aopRedis(String reditTemplate){ redisTemplate.opsForValue().set(&quot;a&quot;,&quot;a&quot;); }} 上述的代码也很简单，就是获取RedisServer的aopRedis方法的第一个参数，然后通过AOP将其替换为指定的Redis连接。测试如下: 1234567891011121314@RestController@RequestMapping(&quot;/&quot;)public class AopController { @Autowired RedisService redisService; @RequestMapping(value = &quot;/redis&quot;,method = RequestMethod.GET) public void testCurd1(){ redisService.aopRedis(&quot;redisTemplate2&quot;); }} 在AopController里面，我想通过redisTemplate2来执行aopRedis方法。但是在RedisService里面，我们又是配置的是Redis连接数据源1，那么如何 RedisTemplate&lt;String,Object&gt; redisTemplate; 这个时候，我们可以通过切面，直接替换RedisTemplate的连接，从而获取指定的Redis连接，测试如下：启动服务。访问http://localhost:8080/redis。 在不开启切面的情况下，可以看到直接访问的是0号库，而开启切面之后，在调用RedisService的时候，由于切面将RedisTemplate的connectionFactory替换为2号库，所以访问结果如下: 本篇文章只是简单的介绍了下AOP的使用，下面几篇可能会基于这篇文章做一些AOP补充和增加一些其他功能。例如：添加Redis的AOP的自动切换，同时添加多个Redis数据源的自动注入，不再手动写Bean。然后会可能基于Mongo的多数据源来讲解AOP的不同代理获取方式，和一般通用的获取方式","link":"/2019/03/12/Spring%E4%B8%ADAOP%E7%9A%84%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%AE%9E%E8%B7%B5(%E4%B8%80)%E4%B9%8BRedis%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%87%E6%8D%A2/"},{"title":"Spring的本质-Servlet初探","text":"现在Java的web开发体系中，Spring以其轻量级，低耦合而占据了老大的地位，但是Spring的本质是什么，为什么在Spring里面不需要像以前写Servlet项目一样，需要配置web.xml。这些都需要我们去刨根问底的。 Servlet是什么按照Servlet规范所解释的那样，Servlet是一个Web组件，就是类似于生物里面的病毒和宿主一样，病毒还是那个病毒，但是离开了宿主之后就不能单独生存了。而宿主就是一个Servlet容器。(tomcat就是一个Servlet容器) Servlet 是基于 Java 技术的 web 组件，容器托管的，用于生成动态内容。像其他基于 Java 的组件技术一样， Servlet 也是基于平台无关的 Java 类格式，被编译为平台无关的字节码，可以被基于 Java 技术的 web server 动态加载并运行。容器，有时候也叫做 servlet 引擎，是 web server 为支持 servlet 功能扩展的部分。客户端 通过 Servlet 容器实现的请求/应答模型与 Servlet 交互 在Tomcat的源码包里面，Servlet其实是一个接口，如下所示: 123456789public interface Servlet { public void init(ServletConfig config) throws ServletException; public ServletConfig getServletConfig(); public String getServletInfo(); public void destroy();} init方法代表的是一个Servlet实例化完毕之后执行的方法，该目的是为了在使用Servlet之前初始化一些基础数据，例如数据库读取或者某些必须的初始化 Spring与Servlet的联系在Spring的配置里面，有一个最重要的步骤就是配置Spring的DispatcherServlet，然后再配置一个ContextListener，那么Spring和Servlet有什么关系呢?首先看一段代码： 123456789101112131415161718192021@Override public void onStartup(ServletContext servletContext) throws ServletException { String servletName = getServletName(); Assert.hasLength(servletName, &quot;getServletName() must not return empty or null&quot;); ApplicationContext applicationContext = createApplicationContext(); Assert.notNull(applicationContext, &quot;createApplicationContext() must not return null.&quot;); refreshApplicationContext(applicationContext); registerCloseListener(servletContext, applicationContext); HttpHandler httpHandler = WebHttpHandlerBuilder.applicationContext(applicationContext).build(); ServletHttpHandlerAdapter servlet = new ServletHttpHandlerAdapter(httpHandler); ServletRegistration.Dynamic registration = servletContext.addServlet(servletName, servlet); Assert.notNull(registration, &quot;Failed to register servlet '&quot; + servletName + &quot;'.&quot;); registration.setLoadOnStartup(1); registration.addMapping(getServletMapping()); registration.setAsyncSupported(true); } 。。。未完待续","link":"/2018/09/18/Spring%E7%9A%84%E6%9C%AC%E8%B4%A8-Servlet%E5%88%9D%E6%8E%A2/"},{"title":"Spring中AOP的探索与实践(二)之Mongo多数据源切换","text":"在之前的一片文章中介绍了使用AOP的方式来实现Redis的多数据源切换。而今天这一篇则是主要讲述Mongo的多数据源切换。 使用AOP来实现Mongo的数据源切换与Redis的AOP切换相同，不同之处是需要替换MongoRepository里面的MongoOperations,从而实现多数据源的切换 代码示例配置类，读取Mongo的配置 123456789101112131415161718192021@Configurationpublic class MongoMultiProperties { private static final Logger LOGGER = LoggerFactory.getLogger(MongoMultiProperties.class); @Bean(name=&quot;mongodb1&quot;) @Primary @ConfigurationProperties(prefix = &quot;spring.data.mongodb.db1&quot;) public MongoProperties db1Properties(){ LOGGER.info(&quot;正在初始化db1&quot;); return new MongoProperties(); } @Bean(name = &quot;mongodb2&quot;) @ConfigurationProperties(prefix = &quot;spring.data.mongodb.db2&quot;) public MongoProperties db2Properties(){ LOGGER.info(&quot;正在初始化db2&quot;); return new MongoProperties(); }} 配置MongoRepository 以下是为了演示，所以配置了两个MongoRepository，实际上使用了AOP的方式实现的多数据源，只需要配置一个默认的MongoRepository即可。 12345678910111213141516171819@Configuration@EnableMongoRepositories(mongoTemplateRef = &quot;mongoDB2&quot;)public class DB2Template { @Autowired @Qualifier(&quot;mongodb2&quot;) private MongoProperties mongoProperties; @Bean(&quot;mongoDB2&quot;) public MongoTemplate db2Template(){ return new MongoTemplate(db2Factory(mongoProperties)); } @Bean public MongoDbFactory db2Factory(MongoProperties mongoProperties){ return new SimpleMongoDbFactory(new MongoClient(mongoProperties.getHost(),mongoProperties.getPort()),mongoProperties.getDatabase()); }} 1234@Repositorypublic interface DB2Repository extends MongoRepository&lt;MongoDB2,String&gt;{} 省略DB1Template的配置，基本上都是差不多的 一般使用上述的配置如果都OK的话，则可以直接使用@Autowired注解使用。 123456789101112@Servicepublic class MongoService { @Autowired DB2Repository db2Repository; @Autowired DB1Repository db1Repository; public void mongoUpdate(){ db2Repository.save(new MongoDB2()); }} 但是使用AOP的方式的话，切Service还是Repository是需要选择的，首先因为在业务使用中，肯定是包含许多的Service的，如果以后需要再添加其他的Service，还需要添加切点，比较麻烦。 如果是切Repository的话，那么这就好办了，直接配置一个主Repository，然后切这个主Repository，这样就可以将Service和AOP进行解耦。从而在Service里面，可以随意使用其他的数据源，例如:Mysql数据源，Redis数据源等。更加灵活 切面写法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Slf4j@Component@Aspect@Order(-1)public class MongoAspect implements ApplicationContextAware { private ApplicationContext applicationContext; @Around(&quot;execution(* com.somersames.config.mongo.db2.DB2Repository.*(..))&quot;) public Object doSwitch(ProceedingJoinPoint joinPoint) throws Throwable { return aopTest1(joinPoint);// aopTest2(joinPoint); } private Object aopTest1(ProceedingJoinPoint joinPoint) throws Throwable { Field methodInvocationField = joinPoint.getClass().getDeclaredField(&quot;methodInvocation&quot;); methodInvocationField.setAccessible(true); ReflectiveMethodInvocation o = (ReflectiveMethodInvocation) methodInvocationField.get(joinPoint); Field targetField = o.getClass().getDeclaredField(&quot;target&quot;); targetField.setAccessible(true); Object target = targetField.get(o); Field modifiersField = Field.class.getDeclaredField(&quot;modifiers&quot;); modifiersField.setAccessible(true); Object singletonTarget = AopProxyUtils.getSingletonTarget(target); Field mongoOperationsField = singletonTarget.getClass().getDeclaredField(&quot;mongoOperations&quot;); mongoOperationsField.setAccessible(true); //需要移除final修饰的变量 modifiersField.setInt(mongoOperationsField,mongoOperationsField.getModifiers()&amp;~Modifier.FINAL); mongoOperationsField.set(singletonTarget, applicationContext.getBean(&quot;mongoDB1&quot;)); return joinPoint.proceed(); } private Object aopTest2(ProceedingJoinPoint joinPoint) throws Throwable { Field methodInvocationField = joinPoint.getClass().getDeclaredField(&quot;methodInvocation&quot;); methodInvocationField.setAccessible(true); ReflectiveMethodInvocation o = (ReflectiveMethodInvocation) methodInvocationField.get(joinPoint); Field h = o.getProxy().getClass().getSuperclass().getDeclaredField(&quot;h&quot;); h.setAccessible(true); AopProxy aopProxy = (AopProxy) h.get(o.getProxy()); Field advised = aopProxy.getClass().getDeclaredField(&quot;advised&quot;); advised.setAccessible(true); Object o2 = advised.get(aopProxy); if (o2 instanceof Advised) { Object o1 = ((Advised) o2).getTargetSource().getTarget(); Object o3 = AopProxyUtils.getSingletonTarget(o1); System.out.println(o3); Field mongoOperationsField = o3.getClass().getDeclaredField(&quot;mongoOperations&quot;); mongoOperationsField.setAccessible(true); Field modifiersField = Field.class.getDeclaredField(&quot;modifiers&quot;); modifiersField.setAccessible(true); //需要移除final修饰的变量 modifiersField.setInt(mongoOperationsField, mongoOperationsField.getModifiers() &amp; ~Modifier.FINAL); mongoOperationsField.set(o3, applicationContext.getBean(&quot;mongoDB1&quot;)); } return joinPoint.proceed(); } @Override public void setApplicationContext(org.springframework.context.ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; }} 在上述的代码中，提供了两种的AOP的写法，但是最终都是获取mongoOperations，然后通过applicationContext来替换。 对比AOP的Redis写法，这里可以看到在Spring中的AOP实现，最起码使用JDK动态代理和Cglib。所以在本文中，使用的是 Field h = o.getProxy().getClass().getSuperclass().getDeclaredField(“h”); 这个就是获取JDK动态代理的对象 至此，mongo的两种代理方式9最初版的代码编写完毕，后续可能需要对代码进行优化，从而避免每一次修改application.yml都需要手动添加Repository 完整代码可以访问https://github.com/Somersames/Multi-Resource","link":"/2019/03/13/Spring%E4%B8%ADAOP%E7%9A%84%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%AE%9E%E8%B7%B5(%E4%BA%8C)%E4%B9%8BMongo%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%87%E6%8D%A2/"},{"title":"Spring通过序列化返回json数据","text":"一般来讲在Spring中可以直接加@ResponeBody来直接返回Json格式的数据，但是这样又有点别扭，因为Stirng同时也可以返回视图名称，但是加了@ResponseBody之后便可以返回Json了，为了解决这个问题还有一种解决办法就是指定一个Result来实现序列化接口然后直接返回这个对象，最后在spring-mvc.xml这个配置文件中配置解析器就可以了。 添加依赖：123456789101112&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind --&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.5&lt;/version&gt; &lt;/dependency&gt; 配置spring-mvc.xml依赖：123456&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;/&gt; &lt;bean class=&quot;org.springframework.http.converter.json.MappingJackson2HttpMessageConverter&quot;/&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; 编写实体类：12345678910111213141516171819202122232425public class JsonResult implements Serializable{ private Map&lt;String,Object&gt; result; private int code; public JsonResult() { code =200; result =new HashMap&lt;String, Object&gt;(); } public void setParamter(String key ,String value){ result.put(key,value); } public void setCode(int code) { this.code = code; } public Map&lt;String, Object&gt; getResult() { return result; } public int getCode() { return code; }} 在这里需要注意下的是如果到时候需要返回对象的时候自动解析这个类需要添加get方法并且实现序列化接口。 1234567@RequestMapping(&quot;jsonresult&quot;) @ResponseBody public JsonResult testJsonResult(){ JsonResult jsonResult =new JsonResult(); jsonResult.setParamter(&quot;msg&quot;,&quot;这是一个测试的demo&quot;); return jsonResult; } 测试结果：","link":"/2018/04/05/Spring%E9%80%9A%E8%BF%87%E5%BA%8F%E5%88%97%E5%8C%96%E8%BF%94%E5%9B%9Ejson%E6%95%B0%E6%8D%AE/"},{"title":"Thread.sleep(0)-vs-Thread.yield()","text":"简介Thread.sleep(long) 和 yield() 都表示的是让出当前线程的 CPU 时间片，两者在执行的时候，都不会去释放自己已经持有的锁。 多线程在现代的处理器中，以 4核心CPU 为例，这表示在同一个时刻，只会有 4 个线程在并行执行，而在每一个核心内部，多个线程其实是顺序执行的，它们的执行顺序依赖于线程的调度算法。 线程调度算法目前主要的调度算法有如下几种「可能不全」： 先进先出（FIFO） 最短耗时优先算法（SJF） 时间片轮转算法（RR） 优先级排序调度算法（PS） 多级反馈队列算法（MLFQ） 以 时间片轮转算法 为例，多个线程每一个线程都会分到一定的执行时间，当本次执行时间结束以后，就会发生上下文切换。同理，对于其他的调度算法，其实本质上都是一致的，就是在同一个时刻，一个核心只能执行一个线程，多线程其实就是通过CPU核心轮流执行线程。 在多线程编程中，如果需要暂停当前线程的执行，可以调用Thread.sleep(long millis) 方法，来让出CPU 时间片，即表示在接下来的 millis 毫秒内，该线程不再参与 CPU 时间片的竞争。 线程的状态在 Java 中，可以通过执行 jstack pid 命令来查看线程的运行状态，如下图所示： 从上面两张图可以看到，目前在 Java 中，是已经含有四种状态了，那么还有两种没有列出来，它们分别是 NEW，TERMINATED NEW：这个状态表示的是线程刚刚创建，例如 new Thread(); RUNNABLE：其实这个状态，在 Java 中表示的是 就绪 和 运行 就绪 ：代表这个线程可以执行了，但是还在等待 CPU 的调度 运行 ：代表这个线程已经在执行中了 WAITING：表示该线程正在等待一些条件，常见于调用了Object.wait,Thread.join,LockSupport.park TIMED_WAITING：对于线程的这种状态，常见于调用了 Thread.sleep(long) 或者 Object.wait(long) 方法等 BLOCKED：常见于互斥锁的竞争 TERMINATED：线程终止 Sleep那么当一个线程调用了 Sleep 方法之后，会由 RUNNABLE 转为 TIMED_WAITING，这个表示在接下来的 long 毫秒内，该线程不再参与 CPU 资源的竞争，需要注意的是 sleep 方法不会释放所持有的锁。 Sleep(0)那么当调用 Sleep(0) 的时候，究竟会发生什么，查看下 Java 的源代码，发现是一个 native 方法，于是查看 hotspot 方法，其代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546JVM_ENTRY(void, JVM_Sleep(JNIEnv* env, jclass threadClass, jlong millis)) JVMWrapper(&quot;JVM_Sleep&quot;); if (millis &lt; 0) { THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), &quot;timeout value is negative&quot;); } if (Thread::is_interrupted (THREAD, true) &amp;&amp; !HAS_PENDING_EXCEPTION) { THROW_MSG(vmSymbols::java_lang_InterruptedException(), &quot;sleep interrupted&quot;); } // Save current thread state and restore it at the end of this block. // And set new thread state to SLEEPING. JavaThreadSleepState jtss(thread);#ifndef USDT2 HS_DTRACE_PROBE1(hotspot, thread__sleep__begin, millis);#else /* USDT2 */ HOTSPOT_THREAD_SLEEP_BEGIN( millis);#endif /* USDT2 */ EventThreadSleep event; if (millis == 0) { // When ConvertSleepToYield is on, this matches the classic VM implementation of // JVM_Sleep. Critical for similar threading behaviour (Win32) // It appears that in certain GUI contexts, it may be beneficial to do a short sleep // for SOLARIS if (ConvertSleepToYield) { os::yield(); } else { ThreadState old_state = thread-&gt;osthread()-&gt;get_state(); thread-&gt;osthread()-&gt;set_state(SLEEPING); os::sleep(thread, MinSleepInterval, false); thread-&gt;osthread()-&gt;set_state(old_state); } } // 忽略其他代码#ifndef USDT2 HS_DTRACE_PROBE1(hotspot, thread__sleep__end,0);#else /* USDT2 */ HOTSPOT_THREAD_SLEEP_END( 0);#endif /* USDT2 */JVM_END 从这段代码可以看到，在 JVM 中有一个选项，叫做 ConvertSleepToYield，这个参数默认是为 true 的，所以当调用 Sleep(0) 的时候，默认的会调用 Thread.yield() 方法。 那么如果关闭了这个选项的话，会调用 os::sleep(thread, MinSleepInterval, false); ,而 MinSleepInterval 的值是 1，所以如果关闭了这个选项的话，那么 JVM 首先会将当前的线程状态赋值为 old_state，然后通过 os::sleep 让其休眠 1ms，然后在将线程设置成原来的状态。 yield其实对于 yield 方法，jvm 也可以将其转换为 Sleep 方法： 12345678910111213141516JVM_ENTRY(void, JVM_Yield(JNIEnv *env, jclass threadClass)) JVMWrapper(&quot;JVM_Yield&quot;); if (os::dont_yield()) return;#ifndef USDT2 HS_DTRACE_PROBE0(hotspot, thread__yield);#else /* USDT2 */ HOTSPOT_THREAD_YIELD();#endif /* USDT2 */ // When ConvertYieldToSleep is off (default), this matches the classic VM use of yield. // Critical for similar threading behaviour if (ConvertYieldToSleep) { os::sleep(thread, MinSleepInterval, false); } else { os::yield(); }JVM_END 在 JVM 中，ConvertYieldToSleep 默认值是 false，所以如果不更改 JVM 的默认配置的话，yield 方法会调用 os::yield(); 方法。 总结其实对于 Thread.sleep(0) 和 yield 方法来讲，如果仅仅使用的是默认配置的话，那么它们最终调用的都是 OS 的 yield 方法。 那么其实对于它们来说，在执行的时候，都是让 CPU 再次发一个调度，如果当前的线程，没有被选中执行，那么它的状态就会由 RUNNABLE 变成 WAITING。","link":"/2020/07/07/Thread-sleep-0-vs-Thread-yield/"},{"title":"ThreadLocalMap 简单分析","text":"ThreadLocal在使用多线程的时候，如果需要保存一份线程自己私有的一部分变量，避免其他线程污染这个变量的话，一般都会自己手动 new 一个 ThreadLocal，如下代码： 12ThreadLocal&lt;String&gt; threadLocal = new ThreadLocal&lt;&gt;();threadLocal.set(&quot;test&quot;); 当需要在某一刻使用这个变量的时候，只需要手动调用下 1234567891011121314151617threadLocal.get();``` ## ThreadLocalMapThreadLocalMap 是 ThreadLocal 的一个静态内部类，每一个线程在创建的时候都会有这个 ThreadLocalMap 变量，它的作用就是存储每一个通过 `threadLocal.set()` 方法存入的值，本质上也是一个HashMap。先看下它的一个内部类：### Entry不同于 WeakHashMap 是继承自 HashMap 的Entry，ThreadLocalMap 的Entry 则是自己的一个内部类。```javastatic class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; }} 在这里可以看到的是，ThreadLocalMap.Entry 是以 ThreadLocal&lt;?&gt; 为 key 的，也就是 ThreadLocalMap 它的整体设计如下： 变量table：表示的是一个 Entry 数组threshold：下一次扩容的临界值，算法为 数组的长度的 2/3INITIAL_CAPACITY：数组的大小，默认是 16 Entry扩容table 的扩容是直接新建了一个 Entry[] 数组，将其长度设置为旧数组长度的 2 倍，然后通过一个 for 循环，将元素都重新移至新的 table 上，但是在移植的过程中，如果发现了 Entry 中的 Key 为空的话，那么就会直接将其 value 设置为 null 来帮助GC，避免内存的泄漏。 如果在移植的过程中发生了 Hash 碰撞，那么会直接将当前下标 + 1，然后判断该位置是否有元素，如果有的话，继续 + 1，直至没有元素。 新增过程当调用 threadLocal.set() 方法的时候，会首先判断 ThreadLocalMap 存不存在。从而进入不同的处理流程上来。 ThreadLocalMap 不存在如果不存在的话，就会调用 createMap 方法来进行初始化。而 createMap 直接就是通过默认值来初始化一个 ThreadLocalMap。 1234567ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY);} ThreadLocalMap 存在如果存在的话，这时直接以当前的 ThreadLocal 为 Key，通过 Hash算法 算出该对象在 table 中的下标，然后判断该下标是否有值(Entry是否为null)，如果有值的话，则判断 Entry 中的 key 是否与当前的 ThreadLocal 相等（地址比较），如果相等的话，则直接将 value 赋值，然后 return。 如果 Entry 中的 key 为 null 的话，则会执行 replaceStaleEntry 方法来找 key，同时在这个方法内部，还会通过 渐进式或者启发式 的方式来进行清除旧key操作。先大致列出其 set 的过程： 1. 获取该 key 的 threadLocalHashCode，然后与当前 table(Entry[]的长度) -1 进行 &amp; 运算，计算出下标。 Hash算法ThreadLocalMap 的 Hash算法 采用的是 斐波那契散列，其过程是用 0x61c88647 累加，然后用累加的结果与当前 table(Entry[]的长度) - 1 进行 &amp; 运算。 0x61c88647 转化为十进制是 1640531527， 1234private static AtomicInteger nextHashCode = new AtomicInteger();private static int nextHashCode() { return nextHashCode.getAndAdd(HASH_INCREMENT);} 在这里的 nextHashCode 是一个 AtomicInteger，因此可以保证其原子性，而且又是私有的静态变量，所以可以尽量保证每一个 ThreadLocal 都会得到一个唯一的 HashCode。 2. 找出 table[i] 中不为 null 的 Entry，在这个过程中会一直 i+1，如果 i+1 大于数组的最大下标的话，则直接从 0 开始寻找。 3. 当在遍历的过程中，如果发现 Entry 的 key 与当前的 ThreadLocal 对象相等的话，则直接将值替换，如果发现某一个 Entry 的 key 为 null 的话，则直接进行 replaceStaleEntry。然后return。 replaceStaleEntry方法该方法出现在 set方法 的第三步，即当判断到 Entry 中的 key 为 null，那么此时就会调用 replaceStaleEntry 来清除那些被回收了的 Key。在这个方法里面，每一个遍历都会将 staleSlot 赋值到 i。 123456789101112131415161718192021222324252627282930313233343536373839404142434445private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value,int staleSlot) { Entry[] tab = table; int len = tab.length; Entry e; int slotToExpunge = staleSlot;//首先将 `staleSlot` 赋值为 `i` ，通过 `i` 向前遍历，直至遍历到第一个 `Entry` 为 null，如果在遍历的过程中发现某些 `Entry` 的 `key` 为 null 的话，则将 i 赋值到 `slotToExpunge`。如果说遍历了完整的一圈没发现 key 为 null 的 Entry 的话，那么一定会在 `staleSlot` 这个地方停下来，因为进入这个方法的前提就是 `key == null`。 for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i;//随后再沿着传入的 `index` 的向后进行遍历，如果此时在遍历的过程中出现了 `Hash冲突`，则直接 `value` 赋值到当前 `Entry` 的 `value` 中。同时判断在第一个遍历中的 `slotToExpunge` 是不是 `staleSlot`，如果是的话，则直接当前的 `index` 设置为`slotToExpunge`，然后调用 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) {// 因为在 set 方法的遍历中，肯定不会出现 k == key，所以此时可以判断下，如果有的话，则直接赋值，然后就可以return 了。 if (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e;//第一个循环中，没有出现 key 为null 的 Entry，则直接将 slotToExpunge 赋值为 i， if (slotToExpunge == staleSlot) slotToExpunge = i;//expungeStaleEntry 方法会进行 rehash，直至往后遍历到第一个为 null 的 Entry停止，在此期间也会进行rehash，同时在此期间，如果遇到了 key 为 null 的 Entry，会将他们的 Entry 以及 value 都置为 null，便于 GC。//cleanSomeSlots 方法会以 log2(len) 的循环次数进行清除旧的 Entry，如果发现 Key 为 null 的 Entry，就会再次调用expungeStaleEntry 方法继续清理旧Key cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } //如果在遍历的途中，没发现 k == key，那么此时如果在向前的遍历中，也没有发现 Key 为 null 的，此时就会将slotToExpunge 设置为 i。 if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; }// 赋值 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value);//如果说之前发现 key 为null 的话，那么此时slotToExpunge 肯定就不会等于 staleSlot，于是触发清除旧Key的方法。 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);} 4. 在第三步的时候，直接找到了为 null 的Entry，则直接新建一个 Entry 然后赋值，5. 当上述步骤都处理完了以后，会通过 cleanSomeSlots 进行判断，如果有 Entry 的弱引用是否被回收了，则会进行 rehash，除非数组的长度已经达到了 threshold 并且未发现 Entry 的弱引用被回收了，才会进行 rehash。 12345tab[i] = new Entry(key, value);int sz = ++size;// 在这里需要注意的是只有没发生弱引用的清除才会进行Rehash，因为一旦出现了清除，则会在 expungeStaleEntry 做 rehash，所以此时就不必在做一次了。if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); set方法流程图 expungeStaleEntry(int staleSlot)方法该方法是 ThreadLocal 主要用于清除旧 Key，其代码如下： 12345678910111213141516171819202122232425262728293031323334353637private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); //当前的Entry的key是null，则直接将value和Entry一起全部设置为 null，帮助GC if (k == null) { e.value = null; tab[i] = null; size--; } else { // 重新Rehash，不过rehash的范围是 staleSlot 到下一个为 null 的 Entry int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i;} 在这里之所以是要进行重新 hash，是因为一旦出现某一个 Key 被回收以后，会导致后面的 Key 无法正确的 hash 到数组的正确下标下，从而导致每一次的 get 操作都会进行一次遍历，时间复杂度由 O(1) 直接退化为 O(n) Get方法ThreadLocal 的 get 方法如下： 12345678910111213public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; } } return setInitialValue();} 在这里需要注意的一个方法是 getEntry ,如果在调用的时候，发生了 Hash冲突，那么该方法会调用getEntryAfterMiss。 12345678910111213141516private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) { Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null;} 在这里可以发现，其实最终还是调用的 expungeStaleEntry，而该方法在之前已经说过了，所以 ThreadLocal 在 get 的时候，其实还会旧的 key 的清除。直至遍历到第一个 e == null 的对象，此时则表示该 key 不存在，于是直接返回 null。 总结总的来说，ThreadLocal 在 set 的过程中，首先会进行 Hash 找出下标，如果该下标的 Entry 为 null 的话，则直接赋值，如果不为 null 的话，则会进行遍历，直至找出第一个不为 null 的 Entry 然后赋值。 如果在遍历的过程中，发现了某些 Entry 的 Key 为 null 的话，则代表可以通过清理旧的 Entry 来进行赋值操作，那么其过程是，首先获取到在遍历过程中 Entry 的 Key 为 null 的下标，记为 staleSlot，然后向前遍历，直至第一个 Entry 为 null 止，然后记录在遍历的过程中，最后一个 Entry 的 Key 为 null 的下标。随后进行第二次的遍历，如果在往后的遍历过程中，出现了 Entry 的 key 与当前的 key 相等的话，则直接赋值。 然后判断如果在之前的第一个遍历中，所有的 Entry 的 key 都不为 null ，那么此时直接将当前下标赋值为 旧Key 清除的起点，随后先进行一个渐进式清理expungeStaleEntry，等这一步清理完毕以后，再进行一次启发式清理cleanSomeSlots，cleanSomeSlots会进行一次 log2(n) 次清理，以渐进式清理expungeStaleEntry 后的 index 为起点，在之后的 log2(n) 下标内，如果还是出现了 key 为 null 的 Entry，则还是会再进行 渐进式清理expungeStaleEntry。","link":"/2020/07/02/ThreadLocalMap-simple-analysis/"},{"title":"TreeSet和TreeMap的一点总结","text":"首先简单介绍下TreeSet和TreeMap的两种排序： 自然排序 通过comparator排序12345678910111213141516171819202122private static void compareWithCpmparator(){ TreeSet&lt;String&gt; treeSet =new TreeSet&lt;&gt;(); List&lt;String&gt; list =new ArrayList&lt;&gt;(); list.add(&quot;a&quot;); list.add(&quot;d&quot;); list.add(&quot;b&quot;); treeSet.addAll(list); Iterator&lt;String&gt; iterator =treeSet.iterator(); while (iterator.hasNext()){ System.out.println(iterator.next()); } Comparator&lt;String&gt; comparator1 = (Comparator&lt;String&gt;) treeSet.comparator(); if (comparator1 == null){ System.out.println(&quot;comparator1是空&quot;); }else { System.out.println(&quot;comparator1不是空&quot;); } } public static void main(String[] args) { compareWithCpmparator(); } 运行之后的结果如下:1234abdcomparator1是空 这段代码里面获取的comparator是空的，Debug一遍，发现这个方法其实调用的是NavigableMap里面的comparator123public Comparator&lt;? super E&gt; comparator() { return m.comparator();} 查看官网上对其的介绍：1234Comparator&lt;? super K&gt; comparator()Returns the comparator used to order the keys in this map, or null if this map uses the natural ordering of its keys.Returns:the comparator used to order the keys in this map, or null if this map uses the natural ordering of its keys 在调用这个方法的时候若是自然排序，那么会返回一个null。若是通过comparator进行排序的话当前集合采用的comparator。查看官网对reeSet的无参构造器的解释： /** Constructs a new, empty tree set, sorted according to the natural ordering of its elements. All elements inserted into the set must implement the {@link Comparable} interface. Furthermore, all such elements must be mutually comparable: {@code e1.compareTo(e2)} must not throw a {@code ClassCastException} for any elements {@code e1} and {@code e2} in the set. If the user attempts to add an element to the set that violates this constraint (for example, the user attempts to add a string element to a set whose elements are integers), the {@code add} call will throw a {@code ClassCastException}. 在使用TreeSet的时候，插入的元素需要实现Comparable这个接口，而刚刚的元素是String，查看String的代码发现: 1public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence { 确实实现了，再测试一个没有实现的元素： 123456789101112131415161718192021222324252627282930313233343536373839public class PojoTest { private int id; private String name; public PojoTest() { } public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public PojoTest(int id, String name) { this.id = id; this.name = name; }} private static void com(){ TreeSet&lt;PojoTest&gt; treeSet =new TreeSet&lt;&gt;(); treeSet.add(new PojoTest(1,&quot;a&quot;)); treeSet.add(new PojoTest(2,&quot;b&quot;)); treeSet.add(new PojoTest(3,&quot;c&quot;)); Iterator&lt;PojoTest&gt; iterator =treeSet.iterator(); while (iterator.hasNext()){ System.out.println(iterator.next().getName()); } } 运行结果如下: 123456Exception in thread &quot;main&quot; java.lang.ClassCastException: SetAndMap.TreeSetAndTreeMap.PojoTest cannot be cast to java.lang.Comparable at java.util.TreeMap.compare(TreeMap.java:1294) at java.util.TreeMap.put(TreeMap.java:538) at java.util.TreeSet.add(TreeSet.java:255) at SetAndMap.TreeSetAndTreeMap.TestTreeSet.com(TestTreeSet.java:77) at SetAndMap.TreeSetAndTreeMap.TestTreeSet.main(TestTreeSet.java:88) 很明显，所以放在TreeSet里面的元素要么是实现Comparable了的自然排序，要么是通过comparator来进行排序的。 最后附上一个标准的使用Comparator的方法： 1234567891011121314151617181920212223242526272829303132private static void construct(){ Comparator&lt;String&gt; comparator =new Comparator&lt;String&gt;() { @Override public int compare(String o1, String o2) { if(o1.toCharArray()[0] &gt;o2.toCharArray()[0]){ return -1; }else if(o1.toCharArray()[0] == o2.toCharArray()[0]){ return 0; }else{ return 1; } } }; TreeSet&lt;String&gt; treeSet =new TreeSet&lt;&gt;(comparator); List&lt;String&gt; list =new ArrayList&lt;&gt;(); list.add(&quot;a&quot;); list.add(&quot;d&quot;); list.add(&quot;b&quot;); treeSet.addAll(list); Iterator&lt;String&gt; iterator =treeSet.iterator(); while (iterator.hasNext()){ System.out.println(iterator.next()); } Comparator&lt;String&gt; comparator1 = (Comparator&lt;String&gt;) treeSet.comparator(); TreeSet&lt;String&gt; treeSet1 =new TreeSet&lt;&gt;(comparator1); treeSet1.add(&quot;c&quot;); treeSet1.add(&quot;g&quot;); treeSet1.add(&quot;a&quot;); Iterator&lt;String&gt; iterator1 =treeSet1.iterator(); while (iterator1.hasNext()){ System.out.println(iterator1.next()); } 自然排序：是实现Comparable接口并且重写了compareTo方法的 另一个comparator则是通过comparator并且重写compare方法","link":"/2018/05/24/TreeSet%E5%92%8CTreeMap%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%BB%E7%BB%93/"},{"title":"Vue中nextTick的使用","text":"这段时间一直在负责一个前后端项目，前端是Vue+ElemeUI，由于自己之前只是会简单的使用 Vue 的一些初级命令，自然只能慢慢的踩坑，然后再出坑….。例如数组无法触发Vue的视图更新 刚开始在使用 Vue 的时候，一直都是在 created 方法里面获取后端数据进行渲染，这样用起来倒也没什么问题，只不过今天突然看到了 Vue 的nextTick 方法，感觉比之前在created里面请求后端更加高级。所以顺便研究了一波。 使用场景nextTick常用于数据更新后，但是dom元素还未完成刷新，如何理解呢? 在 Vue 里面，更新 DOM 元素是异步的，也就是说当我们修改了数据之后，DOM元素并不会立即被刷新。参考深入响应式原理 如下Demo 1234567891011121314151617181920212223242526272829&lt;template&gt;&lt;div&gt;&lt;div :id='id'&gt;&lt;/div&gt;&lt;el-button type=&quot;button&quot; @click=&quot;nextClick&quot;&gt;点击测试&lt;/el-button&gt;&lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { data () { return { id: 'q' }; }, mounted () { }, methods: { nextClick () { this.id = 'm'; let obj = document.getElementById(this.id); var _this = this; this.$nextTick(() =&gt; { let one = document.getElementById(_this.id); console.log(one); }); console.log(obj); } }};&lt;/script&gt; 此时你在控制台会看到obj获取的是null，one获取的dom节点才是正确的。 StackOverFlow关于这个的另一个解释","link":"/2019/05/20/Vue%E4%B8%ADnextTick%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"Vue中使用router遇到无法跳转的问题","text":"今天在配置Vue的路由的时候，在router的index.js中配置了如下代码： 123456789101112131415161718192021import Vue from 'vue'import Router from 'vue-router'import Home from '@/pages/components/home/Home'import Tree from '@/pages/components/tree/Tree'Vue.use(Router)export default new Router({ routes: [ { path: '/', name: 'Home', component: Home }, { path: '/tree', name: 'Tree', component: Tree } ]}) 但是却在启动项目的时候，输入URL:localhost:8080/tree无法实现跳转，后来发现是在App.vue中没有设置&lt;router-view&gt;,最后加上去即可。 例如在App.vue中配置了一个&lt;router-view&gt;，输入URL:localhost:8080/tree就会跳转到了Tree.vue。那假设在Tree.vue中再配置配置一个&lt;router-view&gt;，这时候就会匹配的是localhost:8080/tree/XXX,当然也可以直接用&lt;router-link&gt;跳转到指定的组件中","link":"/2018/10/31/Vue%E4%B8%AD%E4%BD%BF%E7%94%A8router%E9%81%87%E5%88%B0%E6%97%A0%E6%B3%95%E8%B7%B3%E8%BD%AC%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"Vue引入styles的一个小坑","text":"在使用styus的时候，经常会定义一些css的常用变量，但是在引入styl文件一直使用的是@import '~styles/mixins'，然后就是一直在报错，后来经过了解发现，这种写法需要在build中的webpack.conf.js中设置style的别名， 12345678resolve: { extensions: ['.js', '.vue', '.json'], alias: { 'vue$': 'vue/dist/vue.esm.js', '@': resolve('src'), 'styles': resolve('src/assets/styles') } }, 然后将styles指向放styl的文件夹即可。","link":"/2018/10/13/Vue%E5%BC%95%E5%85%A5styles%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%9D%91/"},{"title":"WeakHashMap的实现原理","text":"简介什么是WeakHashMapWeakHashMap实现了 Map 接口，属于 Java 集合中的一员，其用法几乎和 HashMap 一致，但是由于它的 Entry 还继承了 WeakHashMap ，因此导致它的这个 Entry 在触发 FullGc 的时候是有可能可以被回收的。 以下测试，JVM参数统一为：-Xmx64M -Xms64M -XX:+PrintGCDetails 首先上一段代码： 123456HashMap&lt;byte[][],byte[][]&gt; map = new HashMap&lt;&gt;();for(;;){ byte[][] b = new byte[1024][1024]; map.put(b,new byte[1024][1024]); System.gc();} 很快就会看到程序抛出了 OOM异常 修改程序为 WeakHashMap 123456WeakHashMap&lt;byte[][],byte[][]&gt; map = new WeakHashMap&lt;&gt;();for(;;){ byte[][] b = new byte[1024][1024]; map.put(b,new byte[1024][1024]); System.gc();} 你会看到程序运行了很久都没有停下，那么证明了 WeakHashMap 确实会对一些没有其他引用的 key 进行删除了，那么此时如果将程序的代码再次修改下： 12345678List&lt;WeakHashMap&lt;byte[][],byte[][]&gt;&gt; list = new ArrayList&lt;&gt;();for(;;){ WeakHashMap&lt;byte[][],byte[][]&gt; map = new WeakHashMap&lt;&gt;(); byte[][] b = new byte[1024][1024]; map.put(b,new byte[1024][1024]); list.add(map); System.gc();} 此时也会很快的出现了 OOM异常。此时你心里可能会想的是 List 含有了对 WeakReference 的强引用，导致 GC 无法回收对象，但是这里需要注意的是，虽然 List 含有对 WeakHashMap 的强引用，但是 WeakHashMap 的Entry 是继承了 WeakReference 的。因此当 调用 System.gc() 的时候，WeakHashMap 的 Entry 由于没有地方引用，应该是要被回收的。 为了证实 WeakHashMap 的 key 确实是被 GC 回收了，下面再来看一个例子： 12345678List&lt;WeakHashMap&lt;byte[][],Object&gt;&gt; list = new ArrayList&lt;&gt;();for(;;){ WeakHashMap&lt;byte[][],Object&gt; map = new WeakHashMap&lt;&gt;(); byte[][] b = new byte[1024][1024]; map.put(b,new Object()); list.add(map); System.gc();} 此时将 WeakHashMap 的value 修改为一个小对象，你会发现再次运行的话，运行很长时间都不会出现 OOM 异常了。 那么在看了上面这么多的例子之后，说明 WeakHashMap 的 Entry 肯定是可以被回收的，但是具体如何被回收还是得看下源代码。 源代码eg1首先 HashMap 因为持有其对象的强引用，所以导致 GC 无法将其回收，从而导致了 OOM 异常。 eg2WeakHashMap 在运行了很长时间以后，一直未出现 OOM 异常，那么说明了其内部肯定有一些的操作来回收不可达对象。那么下面就来分析为什么会被回收。 EntryWeakHashMap 和 HashMap 的大致结构是一样的，但是有一个区别是，WeakHashMap在实现了Map.Entry&lt;K,V&gt; 的时候还继承了 WeakReference&lt;Object&gt; 。如下图所示： 可以看到，此时 Entry&lt;K,V&gt; 的 key 是继承了 WeakReference 的，所以在 System.gc 的时候，key是一定可以被GC的，那么还有一个就是 value 是如何被 GC 的，此时查看 put 方法： 在这里需要注意的是 Entry&lt;K,V&gt;[] tab = getTable(); 这一行代码是会通过 ReferenceQueue 来判断哪些 key 是可以被回收的。 12345678910111213141516171819202122232425262728private void expungeStaleEntries() { for (Object x; (x = queue.poll()) != null; ) { synchronized (queue) { @SuppressWarnings(&quot;unchecked&quot;) Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) { Entry&lt;K,V&gt; next = p.next; if (p == e) { if (prev == e) table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator e.value = null; // Help GC size--; break; } prev = p; p = next; } } }} 通过 queue.poll() 方法来获取被 GC 回收后的key，然后通过对key进行 hash ，取出下标，移动指针，将value 置为 null ，然后结束。至此，为什么 eg2 不会出现 OOM 的结果就出现了，那是因为 WeakHashMap 在进行 put 的之后，还手动调用了一次 System.gc，然后在 put 的时候调用了 expungeStaleEntries 方法，所以 GC 就可以把那些对象回收了。 eg3和eg4对于这两个例子，主要是因为在 for 循环里面是每一次直接 new 了一个 WeakHashMap，而且都是首次调用 put 方法，之后就没有进行任何处理。因此首次 put 进去的数据，无法触发 expungeStaleEntries，因此导致只有 key 可以被回收，而 value 则是无法被回收的（expungeStaleEntries 方法里面手动将 value 置为了 null，导致可以被 GC）。","link":"/2020/04/22/WeakHashMap%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"title":"算法系列「回溯」-排列、切割、子集","text":"回溯定义按照维基百科上给到的定义：回溯是一种最优选择法，如果发现某一些选择不符合目标，那么就会进行退回操作 在了解回溯算法之前，首先就不得不提到一个概念「递归」，递归 和 回溯 在基本的逻辑上其实是差不多的，只不过回溯都含有撤销操作，而递归则是一条路走到底，两者都属于暴力搜索法。 最经典的题目就属于「八皇后」 递归斐波拉契数 斐波那契数 （通常用 F(n) 表示）形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。509. 斐波那契数 这一题就是典型的递归，其公式表达为 F(x) = F(x - 1) + F(x - 2) 123456789101112131415class Solution { public int fib(int n) { return add(n); } private int add(int n) { if(n == 0){ return 0; } if(n == 1){ return 1; } return add(n-1) + add(n-2); }} 递归和循环的转换每一次的递归都可以转换为「循环」，递归其实就是将上一次方法的计算值带入到下一次的计算当中，而循环其实也是有这个能力的，例如将上面的递归改成循环： 12345678910111213141516171819202122class Solution { public int fib(int n) { return add(n); } private int add(int n) { if(n == 0){ return 0; } if(n == 1){ return 1; } int[] array = new int[n + 1]; array[0] = 0; array[1] = 1; int total = 1; for(int i = 2; i &lt;= n; i++){ array[i] = array[i-1] + array[i - 2]; } return array[n]; }} for 循环的版本还有很多，但是与本文的「回溯」没太多关系，感兴趣的可以去看力扣的官方题解，里面的方法比较多。 递归所不能解决的问题例如一个数组[1,2,2,3]，需要算出该数组有多少个不重复的子集，这就是递归所无法解决的问题，类似的还有组合、切割等 回溯回溯就是在递归的过程中不断判断当前条件与目标是否一致，如果不一致的话，那么就需要撤销当前的操作（俗称剪枝），回溯通常用来解决以下几个问题： 排列 切割 子集 组合 棋盘问题 排列 百度百科的定义：从n个不同元素中取出m（m≤n）个元素，按照一定的顺序排成一列，叫做从n个元素中取出m个元素的一个排列(permutation)。特别地，当m=n时，这个排列被称作全 排列(all permutation)。 46. 全排列 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 标准模版一： 123456789101112131415161718192021222324class Solution { public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) { List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); List&lt;Integer&gt; list = new ArrayList(); for(int i : nums){ list.add(i); } search(nums.length,list,result,0); return result; } private void search(int n,List&lt;Integer&gt; tempList,List&lt;List&lt;Integer&gt;&gt; result,int index) { if(index == n){ result.add(new ArrayList(tempList)); return; } for(int i = index; i &lt; n; i++){ Collections.swap(tempList,index,i); search(n,tempList,result,index + 1); Collections.swap(tempList,i,index); } }} 47. 全排列 II 给定一个可包含重复数字的序列 nums ，按任意顺序 返回所有不重复的全排列。 1234567891011121314151617181920212223242526272829303132class Solution { List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) { Boolean[] visited = new Boolean[nums.length]; Arrays.fill(visited,false); Arrays.sort(nums); search(new ArrayList&lt;&gt;(),visited,nums); return result; } private void search(List&lt;Integer&gt; list,Boolean[] visited,int[] nums) { if(list.size() == nums.length){ result.add(new ArrayList&lt;&gt;(list)); return; } for(int i = 0; i&lt; nums.length; i++){ if(visited[i]){ continue; } if(i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; visited[i - 1] == false){ continue; } visited[i] = true; list.add(nums[i]); search(list,visited,nums); visited[i] = false; list.remove(list.size() - 1); } }} 切割698. 划分为k个相等的子集 给定一个整数数组 nums 和一个正整数 k，找出是否有可能把这个数组分成 k 个非空子集，其总和都相等。 1234567891011121314151617181920212223242526272829303132333435class Solution { int[] nums = new int[]{}; public boolean canPartitionKSubsets(int[] nums, int k) { int sum = Arrays.stream(nums).reduce(0,Integer::sum); if(sum % k != 0){ return false; } Arrays.sort(nums); this.nums = nums; return backtracking(k,sum / k, 0, 0,new boolean[nums.length]); } private boolean backtracking(int k,int target,int total,int currentIndex,boolean[] used) { if(k == 1){ return true; } if(total == target){ return backtracking(k - 1, target, 0, 0,used); } for(int i = currentIndex; i&lt; nums.length; i++){ if(used[i]){ continue; } if(total + nums[i] &gt; target){ continue; } used[i] = true; if(backtracking(k,target,total + nums[i],i + 1,used)){ return true; } used[i] = false; } return false; }} 131. 分割回文串12345678910111213141516171819202122232425262728293031class Solution { String S = &quot;&quot;; List&lt;List&lt;String&gt;&gt; result = new ArrayList&lt;&gt;(); public List&lt;List&lt;String&gt;&gt; partition(String s) { this.S = s; backtracking(0,new ArrayList&lt;&gt;(),s); return result; } private void backtracking(int curr,List&lt;String&gt; list,String s) { if(curr == s.length()){ result.add(new ArrayList&lt;&gt;(list)); return; } for(int i = curr; i &lt; s.length(); i++){ String tmp = s.substring(curr,i + 1); if(isPalindrome(tmp)){ list.add(tmp); backtracking(i + 1,list,s); list.remove(list.size() - 1); } } } private boolean isPalindrome(String s){ if(s == null || s.length() == 0){ return false; } return s.equals(new StringBuilder(s).reverse().toString()); }} 子集78. 子集 给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（幂集）。解集 不能 包含重复的子集。你可以按 任意顺序 返回解集。 123456789101112131415161718192021class Solution {List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) { backtracking(0,nums,new ArrayList&lt;&gt;()); return result; } private void backtracking(int current, int[] nums, List&lt;Integer&gt; list) { if(current == nums.length){ result.add(new ArrayList&lt;&gt;(list)); return; } backtracking(current + 1, nums,list); list.add(nums[current]); backtracking(current + 1, nums,list); list.remove(list.size() - 1); }} 90. 子集 II 给你一个整数数组 nums ，其中可能包含重复元素，请你返回该数组所有可能的子集（幂集）。解集 不能 包含重复的子集。返回的解集中，子集可以按 任意顺序 排列。 for 循环版本 123456789101112131415161718192021222324252627class Solution { List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) { List&lt;Integer&gt; temp = new ArrayList&lt;&gt;(); Arrays.sort(nums); result.add(temp); backtracking(0,nums,temp); return result; } private void backtracking(int index,int[] nums,List&lt;Integer&gt; list) { if(index == nums.length){ return; } for(int i = index; i&lt; nums.length; i++){ if(i &gt; index &amp;&amp; nums[i] == nums[i - 1]){ continue; } list.add(nums[i]); result.add(new ArrayList&lt;&gt;(list)); backtracking(i + 1,nums,list); list.remove(list.size() - 1); } }} 递归版本 123456789101112131415161718192021222324252627class Solution { List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) { List&lt;Integer&gt; temp = new ArrayList&lt;&gt;(); Arrays.sort(nums); backtracking(0,nums,temp,new boolean[nums.length]); return result; } private void backtracking(int index,int[] nums,List&lt;Integer&gt; list,boolean[] used) { if(index == nums.length){ result.add(new ArrayList&lt;&gt;(list)); return; } backtracking(index + 1,nums,list,used); if(index &gt; 0 &amp;&amp; nums[index] == nums[index-1] &amp;&amp; !used[index - 1]){ return; } used[index] = true; list.add(nums[index]); backtracking(index + 1, nums, list, used); list.remove(list.size() - 1); used[index] = false; }}","link":"/2022/03/06/algorithm-backtracking-2/"},{"title":"Vue里面相同页面不同URL的刷新解决方案","text":"随着项目的收尾，使用Vue也有大概两个月时间了，在这期间也才遇到过了不少的问题，今天就来说下 Vue 中URL不同但是页面相同的解决办法。 现象假设有两个URL，都对应的是一个相同的 Vue 页面，URL分别是 view/account/1 和 edit/account/1，此时如果由 view/account/1 跳转至 edit/account/1，你会发现页面是不会刷新的， 从而直接影响了整体的功能。 于是去网上寻找解决方案，在一个 github 的 issue 里面，看到也有人反映过这个问题，不过作者的回复是采用 reload 方式进行强制刷新，也就是类似于 F5 那样，页面首先会白一下，然后就再出现元素。这样虽然可以实现页面上元素的一些加载，但是同时它的弊端也体现出来了，就是对用户极度的不友好，所以最后我们才用了一种Reload的方式来进行刷新的。 首先是在App.vue里面添加如下代码： 123456789101112131415161718192021222324252627282930313233&lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;router-view v-if='isAlive'/&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'App', provide () { return { reload: this.reload }; }, data () { return { isAlive: true }; }, methods: { reload () { this.isAlive = false; this.$nextTick(function () { this.isAlive = true; }); } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 然后再需要刷新的 Vue 页面直接通过 inject 来进行使用即可： 1234567891011121314151617181920212223242526272829303132333435363738 &lt;template&gt; &lt;div&gt; &lt;el-form :model=&quot;account&quot;&gt; &lt;el-form-item label=&quot;姓名&quot; prop=&quot;name&quot; &gt; &lt;el-input v-model=&quot;account.name&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;年龄&quot; prop=&quot;age&quot; &gt; &lt;el-input v-model=&quot;account.age&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;性别&quot; prop=&quot;gender&quot; &gt; &lt;el-input v-model=&quot;account.gender&quot; placeholder=&quot;请输入&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { inject: ['reload'], data () { return { account: { name: null, age: null, gender: null } }; }, watch: { '$route' (to, from) { this.reload(); } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 最后便可以实现页面状态的重新加载。","link":"/2019/07/24/Vue%E9%87%8C%E9%9D%A2%E7%9B%B8%E5%90%8C%E9%A1%B5%E9%9D%A2%E4%B8%8D%E5%90%8CURL%E7%9A%84%E5%88%B7%E6%96%B0%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"angular中使用Service","text":"今天使用 Angular 的时候，看到书中封装了自己得一个 LoggerService ，然后自己也想尝试下，顺便也写下这个记录。 新建一个Service得ts文件在 app 目录下建立一个 ts 文件，如下： 1234567891011export class LoggerService{ info(msg : any) { console.log(msg); } warn(msg: any){ console.warn(msg); } error(msg: any){ console.error(msg); }} 这个类就是封装了一层日志打印得函数，当然在这里也可以封装一些其他得函数。 将改服务添加至module修改 app.module.ts 文件，将这个日志服务加入到 app.module.ts 这个文件中。 1234import { LoggerService } from './loggerService';providers: [PersonService,LoggerService], 在组件中使用如果需要在组件中使用这个服务的话，可以直接在组件得 Component 中 import 即可。 12345678910111213import { LoggerService } from '../loggerService'//然后在构造函数中加入 constructor(private personService :PersonService,private logger: LoggerService) {}// 最后方法中使用： printItem(){ console.log(this.item) this.logger.info(&quot;Logger服务得日志&quot;); this.emitor.emit(&quot;测试回发数据&quot;) } 结束","link":"/2018/05/30/angular%E4%B8%AD%E4%BD%BF%E7%94%A8Service/"},{"title":"css中定位的学习","text":"首先页面代码如下： 1234567891011121314151617181920212223&lt;div id='div1'&gt; &lt;div id='div2'&gt; &lt;div id='div3'&gt; &lt;div id='div4'&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id='div22'&gt; &lt;/div&gt;&lt;/div&gt;&lt;style&gt;#div1{ width: 700px; height: 700px; background: red; margin-top: 50px; margin-left: 50px;}&lt;/style&gt; relative和absoluterelative单独使用，代码如下 1234567891011#div2{ width: 500px; height: 500px; background: rgb(17, 0, 255); position: relative; top: 20px; left: 20px; right: 0; bottom: 20px; }} 其显示结果如图： 说明一切都是没问题的，div2是按照div1来进行相对定位的。那么单独使用absolute呢 absolute单独使用12345678910#div2{ width: 500px; height: 500px; background: rgb(17, 0, 255); position: absolute; top: 20px; left: 20px; right: 0; bottom: 20px; } 可以看到使用absolute的div2并未按照div1 那如果div3也是absolute定位的话，那么此时会相对于div2定位还是相对于div1呢?测试如下： 12345678910111213141516171819202122232425#div1{ width: 700px; height: 700px; background: red; margin-top: 50px; margin-left: 50px; } #div2{ width: 500px; height: 500px; background: rgb(17, 0, 255); position: absolute; top: 20px; left: 20px; right: 0; bottom: 20px; } #div3{ width: 300px; height: 300px; background: green; position: absolute; left: 10px; top: 10px; } 此时页面的显示如下：也就是说，div3是按照div2来定位的，那么为什么就是div2是按照html定位的呢？解释就是，使用absolute定位的元素会一直向上寻找，直到找出包含position的一个元素，然后按照其定位，那么relative呢? 12345678910111213141516171819202122232425#div1{ width: 700px; height: 700px; background: red; margin-top: 50px; margin-left: 50px; } #div2{ width: 500px; height: 500px; background: rgb(17, 0, 255); position: relative; top: 20px; left: 20px; right: 0; bottom: 20px; } #div3{ width: 300px; height: 300px; background: green; position: relative; left: 10px; top: 10px; } 可以看到，其定位都是直接以其上级元素为标准的。 absolute的特性absolute 具有的特性之一就是其包裹性，也就是absolute的width的宽度是100%的时候，其宽度其实是内容的宽度，而不是真正的100%宽度。即被inline-block化了。 妙用112345678910111213&lt;div id='div-span-1'&gt;&lt;span id='span-1'&gt;adad&lt;/span&gt;&lt;/div&gt;&lt;style&gt; #div-span-1{ /* width: 700px; */ /* height: 200px; */ position: absolute; color: pink; border: 10px solid black } /* #span-1{ position: absolute; } */&lt;/style&gt; 注意此时上层div不可以设置width，这样就可以实现外层div的宽度自动是内容的宽度修改为如下，也可以使用： 12345678#div-span-1{ /* width: 700px; */ /* height: 200px; */ position: absolute; color: pink; border: 10px solid black; float: left; } absolute会破坏其父元素的宽度：12345678910111213141516&lt;div id='div-span-1'&gt;&lt;span id='span-1'&gt;&lt;/span&gt;&lt;/div&gt;&lt;style&gt;#div-span-1{ /* width: 700px; */ /* height: 200px; */ /* position: absolute; */ color: pink; border: 10px solid black; float: left; } #span-1{ width: 300px; height: 300px; position: absolute; border: 10px solid gray }&lt;/style&gt; 此时可以看到父元素的高度已经被破坏了","link":"/2018/10/13/css%E4%B8%AD%E5%AE%9A%E4%BD%8D%E7%9A%84%E5%AD%A6%E4%B9%A0/"},{"title":"RestTemplate 处理转义的相关细节","text":"在业务开发中，常见的 Http 请求开源框架有如下几个： JDK 自带的 Http 请求库 Apache 提供的 HttpCLient OkHttp 由 JDK 封装而来的 RestTempalte 其中因为我们是用的 Spring 框架，所以自然而言的优先选择 RestTemlate，优点非常的多，很多配置都可以做到低耦合，并且可以将请求和相应做单独的处理。 但是在使用的过程中，遇到了转义的坑了，在此记录下 场景复现其中后台有一个 GET 请求的接口，其入参分别是 source 和 value，为了简单描述这个问题，我就直接把入参拼接然后返回出去 现在启动服务然后用 Postman 测试这个接口： 可以看到确实正常的返回了，这说明这个接口确实没问题，那么换到项目中，使用 RestTemplate 来看看结果呢？ 使用方式其中 RestTemplate 的配置如下： 可以看到就是仅仅设置了下 Https 和 超时时间等等，那么测试如下： 可以看到打印的情况：res:11 所以证明这个方法其实也没有问题 异常在后续的测试中发现，一些很常见的条件，例如：{}、% 都无法搜索出相对应的文本，第一反应就是会不会是转义导致的，但是 builder.toUriString() 这个方法是会对输入的 value 值进行转义的，难不成还有地方会进行再次处理？ 于是本地尝试复现： 发现对方返回的确实是未转义的（服务器就是第一个图的，就是将入参decode然后返回），那么这就有意思了，到底是哪里对这个参数再进行了二次转移呢？ 于是本地抓一个包看看，发现确实是我这边直接给转义了两次 http://127.0.0.1:8087/list?source=1&amp;value=%25E6%25B5%258B%25E8%25AF%2595 这个URL decode 后是 http://127.0.0.1:8087/list?source=1&amp;value=%E6%B5%8B%E8%AF%95 而后面这个才是真正应该发送到对方系统的 转义既然找出了问题所在，那么是哪一个地方进行了二次转移呢？ RestTemplate首先在构建 URL 的地方，我们是手动给他 encode 了下，那是不是中途 RestTemplate 又再次的进行了 encode 了呢？ 于是进行了 debug，发现在如下方法确实会进行再次 encode 其中这个类会判断当前的 RestTemplate 的 encode 模式是不是 URI_COMPONENT，至于是什么时候设置的，在这里暂时先不说，继续往下走，就看到了 encode() 方法，其实参数就是在这里再次被 encode 了 至此，被二次encode 的原因已经找到了，那么在看下到底是因为什么字符被二次 encode 了呢？ 观察抓包的到的 URL 和 我们请求的 URL，会发现 % 被 再次 encode 成了 %25，所以就会导致下游得到的不是正确的入参。 解决方法解法一既然已经知道 RestTemplate 会默认的进行 encode，那么是不是只要在入参的地方不要自己 encode 就可以了呢？ 测试下来确实可以。 坑这里有一个需要注意下，如果不提前 encode，那么 restTenplate 如果发现你的 url 里面含有 {}，会将其认为是占位符 这是因为 RestTemplate 在 expandUriComponent 方法里面会检查下是否含有 {、:，如果有的话那么会通过正则来匹配 key ，最后通过 key 在 UriTemplateVariables 里面寻找 value 而我们在调用的时候没有赋值，自然就会报错了。 解法二由于上述的坑，所以最优的方法还是自己手动处理下，这样也方便后续替换请求的开源库，为了解决上述问题，可以使用这个API 即将请求的 String 入参替换为 URI 这样就可以解决问题了 最后之前提到过一个判断代码如下： 123456private URI createUri(UriComponents uric) { if (encodingMode.equals(EncodingMode.URI_COMPONENT)) { uric = uric.encode(); } return URI.create(uric.toString());} 这个 EncodingMode.URI_COMPONENT 是在初始化 RestTemplate 时默认设置的，而如果条件判断为 false 的情况下，那么会走如下方法： 和我们之前的修复方法一样，将 String 替换为 URI 了，开启的方法如下： 这样也是OK的","link":"/2022/03/31/correct-use-of-RestTemplate/"},{"title":"css记录","text":"在css的样式应用中，script是一个双标签，也就是说script是必须要通过标签闭合的也就是&lt;script&gt;&lt;/script&gt;,而不可以使用&lt;script XXX/&gt;这样写的话会导致浏览器解析不出来页面，而单标签的话则是可以通过/&gt;关闭。类似于&lt;meta&gt;标签的话则是不需要/&gt;但是在工作中的话是推荐加上闭合标签，以是的代码有很好的可读性。 同理在css中的p标签如果嵌套了一个h1标签的话会出现解析错误,是因为p标签的话只能出现行内元素和文本，若是出现了块级元素的话:浏览器的解析方法如下： 123456789&lt;p&gt;&lt;h1&gt;测试的标题&lt;/h1&gt;&lt;/p&gt;浏览器解析：&lt;p&gt;&lt;/p&gt;&lt;h1&gt;测试的标题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt; 垂直方向上的外边距会叠加，即宽的外边距会决定垂直方向上的距离 为设置了宽度的盒子添加边距啥的会导致盒子变得更宽","link":"/2018/03/30/css%E8%AE%B0%E5%BD%95/"},{"title":"ClassNotFound 和 NoClassDefFoundError 的区别","text":"在 Java 的开发中，这两个可能是让人比较头痛的异常了，因为出现这个异常，又得一大堆 jar 包冲突需要解决。 ClassNotFound按照 oracle 官方的描述：Class ClassNotFoundException Thrown when an application tries to load in a class through its string name using: The forName method in class Class. The findSystemClass method in class ClassLoader . The loadClass method in class ClassLoader. 也就是当 JVM 尝试着去加载某个类的时候，会发现 classPath 下面却没有这个类，那么就会直接报错，在日常开发中出现的常见的常见原因是，两处引用了不同版本的第三方包（maven）。 maven 中采用的是就近原则，并且 maven 中有一个规定就是：groupId、artifactId 相同的 jar 包，只会选择一个版本进行引入，而引入的版本如下： 路径最短者优先原则，路径相同先声明明优先原则 例如上图中的 1.4 版本，就不会被 maven 所选择，这是因为 spi_client 的路径是相同的，而 1.5 版本则更近，所以 maven 就会选择 1.5 的版本加载到 classPath。 还有一种就是这个情况： 此时 maven 就会选择 1.0 版本的 D，因为 1.0 版本的到达项目的更近。 此时，如果项目中使用了其他版本的 jar 包，就会出现 ClassNotFount 异常。 NoClassDefFoundError这个就比较特殊了，具体的含义就是项目编译、加载通过，但是在使用的时候却发现突然没有了 class 文件。 上图是我们最近遇到该异常的一个依赖关系图 service会通过 bean 容器获取到对应的 worker 然后调用对应的方法，但是由于公司的插件检测到了版本冲突，所以手动在 service 模块中去除了 worker 中的 api2.0 123456&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;XXX&lt;/groupId&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 因此 service 中实际上的 api 版本是 1.0 的，而一旦项目运行起来，在 service 中存在的是 1.0 版本的 api，如果 worker 中有方法以来 api2.0，那么就会直接报错。 最后正常情况下，message都会给出明显的提示，静态代码报错，直接看 Caused By，非静态的一般会提示具体的类名 解决方法 将冲突的版本的 jar 版本号统一 如果冲突的 jar 是由于 maven 的就近原则引入进来的，则直接排除掉即可 使用阿里的 pandora 自定义插件处理","link":"/2022/04/07/difference-between-ClassNotFound-and-NoClassDefFoundError/"},{"title":"Springboot自定义@EnableXX注解","text":"在SpringBoot中，经常可以看到许多以 @Enable 开头的注解，例如：@EnableAutoConfiguration，@EnableAsync……，那么我们是否可以自己定义一个注解呢？ 其实自定义注解最终都是利用到了 ImportBeanDefinitionRegistrar 这个类，通过手动的方式，将一个类注册成为 Bean，然后在进行一系列的操作，下面就来看下 ImportBeanDefinitionRegistrar ImportBeanDefinitionRegistrar这个类的代码如下： 可以看到这个类的结构很简单，就是一个方法，那么下面来看下这两个参数是什么意思。 AnnotationMetadata从字面的意思上可以看出来是一个注解的元数据，它的里面的的方法如下：都是一些获取注解信息的方法，那么第二个参数呢 BeanDefinitionRegistry从字面的意思上可以看出来这个类是用于注册 Bean 的，其中最常见的就是 registerBeanDefinition 方法。它提供了两个参数 String beanName, BeanDefinition beanDefinition，而具体使用，则来看看demo。 demo假设现在有一个需求是要写一个切面，这个切面负责打印controller 的log。但是可能某些系统有自己的日志格式，不太需要这个切面AOP，所以希望可以增加一个开关，然后是按需引用。下面就来开始写一个这样的demo，项目结构如下： 在这里为什么会初始化两个文件夹，是因为 @SpringBootApplication 这个注解会默认将当前目录以及它的下级目录下的 Bean 注入到容器中，所以新建一个同级目录anno 就是为了不让 @SpringBootApplication 加载切面。 然后启动 Application 这个类，其中 RestControllerTest 就是一个很简单的 Controller，如下： 然后请求http://localhost:8081/api/query/1，可以看到在控制台没有任何的输出，那就说明自定义的切面还没有生效，此时在 Application 上添加我们的自定义注解，添加后代码如下： 1234567@SpringBootApplication@EnableLogpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class); }} 此时在此启动项目，你会发现已经已经在控制台有我们的日志log了。实现的细节见下面几个类： 自定义注解@interface首先，我们需要定义一个注解，其代码如下： 12345678@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.TYPE})@Documented@Import({LogRegister.class})public @interface EnableLog {} 在这里使用了 @Import 这个注解，由于不是本文的重点，因此不讲述其作用,（后面讲自动装配的原理时会讲解）然后这里有一个 LogRegistrar 类，这个类就是实现自定义注解的关键。 LogRegister 这个类的作用，就是让 BeanDefinitionRegistry 将我们的 LogAop 注册成一个Bean，只有当注册成一个 Bean 以后，该切面才会生效。 LogAop 该类就是一个切面类，负责打印请求的耗时。自此，一个自定义注解就完成了，当第三方想接入该日志log的时候，就可以直接使用 @EnableLog 来开启。 ps：注意调整@Pointcut 的切点，否则会切不到 这就是最简单的自定义注解了，至于里面涉及到的原理，后续可能会写一些文章来补充。 如果需要查看源代码的话，可以访问如下 github 地址： https://github.com/Somersames/spring-doc-sample/tree/master/anno-test","link":"/2020/05/26/define-a-enable-anno-in-springboot/"},{"title":"git代码同步之间的冲突","text":"在使用git的时候单独一人进行push和pull的时候是不会出现代码冲突的，但是当团队中有多人的时候进行协作的时候难免会造成代码间同步问题。具体就是git pull的时候会提示线上代码会覆盖本地的代码。然后就不让pull，最后也不让push。查询了下解决办法： 方法一：123git stashgit pull git stash pop 但是这种方法pull下来的代码会导致IDEA识别不了。也就是java文件会直接不显示，最后是关闭IDEA然后再次打开才解决这个问题的。重新导入的时候选择maven 方法二：这个方法主要应对的是push提交不上去，但是pull却又显示是线上最新版本 12git reset --hardgit push","link":"/2018/04/08/git%E4%BB%A3%E7%A0%81%E5%90%8C%E6%AD%A5%E4%B9%8B%E9%97%B4%E7%9A%84%E5%86%B2%E7%AA%81/"},{"title":"优雅的dubbo服务端校验","text":"在业务的开发过程中，肯定会有一些数据字段的校验，例如手机号格式，用户名格式等等。 如果这个放到每一个具体的接口去判断的话，首先是和业务代码耦合，每一个接口的实现方都需要在代码中判断一系列的校验，而且后续如果需求产生变更，那么每一个在业务中进行判断的方法都需要改变，非常的耗时且不优雅 解决方案目前无论是 spring全家桶 还是 dubbo，通用的做法就是通过 Hibernate Validator 来进行入参的校验，如果是配合 spring 使用的话，那么可以直接引入如下的 pom 文件： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt; &lt;version&gt;${spring-boot.version}&lt;/version&gt;&lt;/dependency&gt; 本文主要是以 dubbo 为例子，其实如果用的是 spring 配合 HTTP 请求，实现上大致是差不多的 服务端加入注解校验 更加优雅的做法是通过配置文件来控制提示信息 然后在开启服务端校验，虽然服务端和客户端均可以配置，如果这个因为业务规则是必须做校验，那么最好还是服务端开启。 因为你不知道客户端会靠不靠谱 然后定义一个方法，如下： 当我们开启参数注解以后，正常情况下，如果客户端用在户名不传递的情况下是无法得到正确的结果，那么开启一个客户端来调用试一下： 客户端 调用后的日志返回如下： 可以看到，客户端直接报错了，显然这种返回并不是非常的友好，如果参数不合法就直接提示报错，那么消费端可能还需要解析报错信息来进行处理，例如是否发送告警等等。 所以一个统一的返回是非常有必要的，不仅仅是可以优化业务的处理流程，而且也会提高系统的性能 Java 中抛出异常，需fillInStackTrace信息，因此会造成一定的性能损耗 Dubbo 文档在 dubbo 文档中，有专门的一个章节来讲述 参数校验 参数校验 按照文档的配置，写好服务端和客户端以后，会发现虽然拦住了，但是 consumer 端却直接报错了，如下 dubbo 服务端的版本是 2.7.4.1 这个报错会出现在低版本的 dubbo，如果版本较高，则不会有此类报错，主要原因还是 hessian 序列化的原因，但是因为不涉及到本文内容，如果不感兴趣可以忽略此节，而且网上的解释也比较多 看起来是无法序列化一些东西，于是在 issue 里面搜索了下相关问题，发现还真有人遇到了Fix When using hibernate-validator, class ‘org.hibernate.validator.internal.metadata.descriptor.ConstraintDescriptorImpl’ is wrong with hessian deserialization 但是呢这个 PR 并没有被合并到 master，原因在这里： 意思就是 dobbo 的开发者并不建议通过一个自定义的异常来处理验证失败的消息，然后提交 PR 的人问了下有不有更好的解决办法，dubbo 的维护人员就建议可以自定义一个 Filter 来处理此类问题。 疑问？我的服务端如果用的是新版本（2.7.8）已经不会报这个序列化的错误了，是不是已经有人修复了呢？ 于是去 github 上的 issue 上搜了下，还真发现了一个合并记录，这个老哥在 ValidationFilter 中新增一个 catch，而捕获的异常正好是 ValidationException，那是不是正好是这个改动，正好修复了上述的序列化问题呢？ avoid of serialization exception for javax.validation.ConstraintViolationException 这几行的改动是修复这个 bug： Provider参数验证时，javax.validation.ConstraintViolationException序列化异常 ![image-20220329011534492]而 ConstraintViolationException 和 ValidationException 的关系图如下：(https://szhtc-1252780558.cos.ap-shanghai.myqcloud.com/%E6%96%87%E7%AB%A0/how-to-validator-dubbo-server-in-server/image-20220329011534492.png) 可以看到 ConstraintViolationException 正好是 ValidationException 的一个子类，所以这个异常恰好被捕获而且被转换为 ValidationException，也因此避免了序列化异常了。 好了，到这里也就基本介绍了 dubbo 配合 validation 使用的现状 低版本序列化异常 高版本客户端直接报错了 那么如何进行优化呢？ 自定义 Filter既然作者是建议自定义一个 Filter，那么就参照 dubbo 维护者的建议来处理： Dubbo 的自定义的 Filter 是通过 SPI 机制来实现的，需要在 resource 目录下新建一个文件，项目结构图如下： 自定义Filter：其中主要的功能就是重写 invoke 方法 然后在配置中将其开启即可： 这样返回的信息里面就会是我们自定义的 Response 了 后续这样通过一个 Filter 就可以让 consumer 得到真实的返回值，从而方便做业务的区分，当然也可以自定义一些自定义的注解来配合做更加复杂的处理了，不过这也是后话了。 参考 Dubbo服务如何优雅的校验参数","link":"/2022/03/07/how-to-validator-dubbo-server-in-server/"},{"title":"浅谈 synchronized 和 volatile","text":"一、锁从细节上来说，锁分为 乐观锁、悲观锁 乐观锁适用于读多写少的场景，一般是通过 CAS 进行操作，因为不用加锁，所以性能上比悲观锁优秀太多 悲观锁适用于写多读少的场景，性能开销比较大。 1：乐观锁在 Java 中的 Unsafe 的 compareAndSwapInt 就是用到了这个特性 但是乐观锁还会产生一个 ABA 的问题，一般是通过 version版本号 的方式来解决另外一个就是竞争问题，如果大量的线程都在进行 CAS 操作，那么势必会造成某些线程 CAS 操作非常耗时，会白白的浪费非常多的 CPU 资源，甚至造成 CPU 100% 的情况 2：自旋锁当一个线程获取不到锁的时候会进行阻塞，而阻塞一个线程是需要进行上下文切换，这些都是需要 CPU 进行操作，加入切换一个线程的上下文所需要的时间是 10，而代码的同步快执行一次是 5，那么此时进行线程的上下文切换是不值得 而现在的 CPU 一般都是多核心处理器，为了提高鲜绿，自选锁也就出现了。两个线程并行执行，让后面那个线程不进入阻塞状态，而是自旋等待 CPU 获取锁。 在 Java 中的 AtomicInteger 中也是用到了这个特性 12345678public final int updateAndGet(IntUnaryOperator updateFunction) { int prev, next; do { prev = get(); next = updateFunction.applyAsInt(prev); } while (!compareAndSet(prev, next)); return next;} 会一直进行 cas 操作，直至设置成功 2.1：缺点自选锁的缺点就是需要依赖临界资源的执行时间，如果临界资源执行的时间太长，会导致 CPU 时间被浪费 3：适应性自选锁为了弥补自选锁的缺点，在 JDK1.6 里面引入了适应性自选锁，线程的自选状态会根据锁获取的状态以及上一次自选获取锁的时间来决定此次的自选次数 二、synchronzedsynchronzed 是 Java 中的一个关键字，由 JVM 负责实现，可以加在代码块、实例方法、静态方法上，加在不同的地方，锁住的对象是不同的。代码块：锁住的是 () 内的对象实例方法：锁住的是当前对象实例静态方法：锁住的是当前对象的Class synchronzed 具有原子性、互斥性、可重入性、不可中断性 1：原子性首先来解释下原子性的定义：一个操作要么全部执行完成，不执行。那么 synchronized 是如何保证的呢？synchronized 修饰的代码块或者方法，在同一个时刻，只有一个线程可以获得锁并执行，其他线程只能等锁释放以后才可以执行，否则会阻塞。这样就保证了在同一个时刻，只有一个线程可以对变量进行修改，保证了原子性 2：可重入性对于同一个线程加的锁，该线程可以在任何时间再次执行同步代码块内部的方法 3：实现细节如果对实例方法或者静态方法加锁，在编译后的 class 文件会出现两个 flag，代表这个方法是一个同步方法，其他线程在执行这个方法的时候必须获得锁如果修饰的是实例方法或者静态方法，那么在反编译的字节码里面可以看到如下这个关键字： 如果修饰的是代码块，那么编译成字节码后的代码如下： 3.1：MarkWord在谈及 synchronized 的时候，先了解下 MarkWord，Java 的对象头由三部分组成：Mark World、对象引用指针、数组长度（数组才有） 而 synchronized 的一些操作主要就在 MarkWord 中 下面是来自于 markOop.hpp 中关于 MarkWord 区域的描述 12345678910111213// 32 bits:// --------// hash:25 ------------&gt;| age:4 biased_lock:1 lock:2 (normal object)// JavaThread*:23 epoch:2 age:4 biased_lock:1 lock:2 (biased object)// size:32 ------------------------------------------&gt;| (CMS free block)// PromotedObject*:29 ----------&gt;| promo_bits:3 -----&gt;| (CMS promoted object)//// 64 bits:// --------// unused:25 hash:31 --&gt;| unused:1 age:4 biased_lock:1 lock:2 (normal object)// JavaThread*:54 epoch:2 unused:1 age:4 biased_lock:1 lock:2 (biased object)// PromotedObject*:61 ---------------------&gt;| promo_bits:3 -----&gt;| (CMS promoted object)// size:64 -----------------------------------------------------&gt;| (CMS free block) 如果画成图则是如下形式 3.2：加锁过程synchronized 在 JDK1.6 之后的到了明显的优化，由之前的直接到重量级锁变为如下几个步骤： 无锁、偏向锁、轻量级锁、重量级锁 3.3无锁这是最理想的情况，当处于无锁的状态下时，Mark World 存放的是对象的 hashCode，以及当前对象的分代年龄。这里所说的 HashCode 指的是一致性 HashCode，也就是通过 Object::hashCode 或者 System::identityHashCode(Object) 方法 得到的 关于一致性 HashCode： 因为一致性 HashCode 是一个随机数，第二次计算肯定与第一次计算得到不同的结果，而在 JVM 中一个对象的 HashCode 前后调用需要保持一致，所以这也是为什么一个对象生成过一致性 HashCode 以后便无法再次进入偏向锁状态 而如果一个对象正处于偏向锁状态，但是立即收到了重新计算一致性 HashCode 的请求，那么此时就会马上被膨胀为重量级锁 3.4偏向锁：引入偏向锁的目的是为了解决多线程竞争不激烈的情况下，例如一个程序在大多数情况下只有一个线程进行一次或者多次的访问，对于这种情况，就没必要进行加锁的操作了。在 JDK6 之前，synchronized 之所以性能很差和每次加锁都是重量级锁有关 持有偏向锁的线程不会主动的撤销自己所持有的偏向锁，如果此时发生了竞争，那么当前的业务线程需要向 VmThread 请求进入到全局的安全点，一旦进入到安全点，就会尝试撤销偏向锁，如果此时发现之前持有偏向锁的线程已经退出同步代码块或者已经结束，则直接进行 CAS 替换 MarkWord 的 ThreadId。 否则，此时会将偏向锁撤销，并且设置锁标记为轻量级锁，持有偏向锁的线程不会主动的撤销偏向锁 123456789101112131415161718void ObjectSynchronizer::fast_enter(Handle obj, BasicLock* lock, bool attempt_rebias, TRAPS) { if (UseBiasedLocking) { if (!SafepointSynchronize::is_at_safepoint()) { BiasedLocking::Condition cond = BiasedLocking::revoke_and_rebias(obj, attempt_rebias, THREAD); // 如果是已经撤销并且重新偏向成功，直接返回 if (cond == BiasedLocking::BIAS_REVOKED_AND_REBIASED) { return; } } else { // 如果已经在安全点，已经发生竞争，直接尝试撤销并且升级 assert(!attempt_rebias, &quot;can not rebias toward VM thread&quot;); BiasedLocking::revoke_at_safepoint(obj); } assert(!obj-&gt;mark()-&gt;has_bias_pattern(), &quot;biases should be revoked by now&quot;); } // 升级轻量级锁 slow_enter (obj, lock, THREAD) ;} 3.5轻量级锁：如果一个锁需要升级为轻量级锁，会进行如下操作： 首先检查是否是无锁状态，如果是的话，则在当前的线程栈桢里面新建一个 LockRecord 的记录，官方命名为 displaced mark，然后将对象的 MarkWord 拷贝一份进 displaced mark，再通过 CAS 操作将对象中的 MarkWord 指针更新为 LockRecord 的地址，并且将 LockRecord 中的 owner 更新为原 MarkWord 如果是已经有锁的状态下，会检查对象的 MarkWord 指针是否指向自己的一个栈桢，如果是的话，则代表是自己重入，那么直接执行即可 否则会进行自旋，如果达到一定的阈值后，还无法获取到锁，则直接升级为重量级锁 123456789101112131415161718192021222324252627282930313233343536373839void ObjectSynchronizer::slow_enter(Handle obj, BasicLock* lock, TRAPS) { markOop mark = obj-&gt;mark(); // 必须是非偏向锁状态 assert(!mark-&gt;has_bias_pattern(), &quot;should not see bias pattern here&quot;); // 判断是否有锁，如果 JVM 没有开启偏向锁，那么此时可能会一种无锁的状态 if (mark-&gt;is_neutral()) { // Anticipate successful CAS -- the ST of the displaced mark must // be visible &lt;= the ST performed by the CAS. // 设置 displaced MarkWord lock-&gt;set_displaced_header(mark); if (mark == (markOop) Atomic::cmpxchg_ptr(lock, obj()-&gt;mark_addr(), mark)) { TEVENT (slow_enter: release stacklock) ; return ; } // Fall through to inflate() ... } else if (mark-&gt;has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark-&gt;locker())) { assert(lock != mark-&gt;locker(), &quot;must not re-lock the same lock&quot;); assert(lock != (BasicLock*)obj-&gt;mark(), &quot;don't relock with same BasicLock&quot;); lock-&gt;set_displaced_header(NULL); return; }#if 0 // The following optimization isn't particularly useful. if (mark-&gt;has_monitor() &amp;&amp; mark-&gt;monitor()-&gt;is_entered(THREAD)) { lock-&gt;set_displaced_header (NULL) ; return ; }#endif // The object header will never be displaced to this lock, // so it does not matter what the value is, except that it // must be non-zero to avoid looking like a re-entrant lock, // and must not look locked either. lock-&gt;set_displaced_header(markOopDesc::unused_mark()); ObjectSynchronizer::inflate(THREAD, obj())-&gt;enter(THREAD);} 升级为轻量级锁以后，竞争锁的线程会自旋几次，避免升级至重量级锁，这个自旋次数不是一个固定值，而是由 JVM 动态来决定的 如果自旋次数达到阈值，那么会直接升级为重量级锁，或者 JVM 判断之前几次自旋都没有获取到锁，那么也就不用再自选了，因此可能会直接升级到重量级锁 3.6重量级锁重量级锁是通过 mutex 来实现的，锁的状态会被改成「10」，并且 MarkWord 里面存储的是指向重量级锁的指针，所有等待的线程都会被挂起 三、volatilevolatile 和 synchronized 不同的是，volatile 只有可见性，没有互斥性 1可见性在计算机发展的早前，CPU 多是单核的，虽然仍然可以并发的执行程序，但是不会存在可见性问题，因为所有的数据要么存在于内存中，要么存在于 CPU 的缓存中，所以任何线程在数据写入和读取的时候一定都是从同一个地方获取的。 随着计算机的发展，多核心的 CPU 出现了，这个时候程序执行的速度可以加快，但是前辈们发现计算机从内存中获取数据的速度依然很慢，因此考虑要不要直接在 CPU 内部做一个缓存，将内存中的数据和指令批量读取一部分来到 CPU 本地来，然后进行处理，最后将处理完数据再回写到内存中 但是这样就会产生一个可见性问题：假设有两个 CPU 同时将一个变量 a++ 读取到了自己的 CPU 缓存中，同时执行了指令 +1，于是在回写的时候，都将 a 的值写为 1 了，但是程序正常执行的话，a 的值应该是 2，而不是1 1.1总线加锁 总线是与所有 CPU 相连接的一个主线路，可以理解为所有的 CPU 指令都必须经过总线当一个 CPU 读取变量 a 到自己本地的时候，会向总线发送一个 LOCK 信号，此时其他 CPU 便会暂停执行，直至第一个 CPU 执行完毕 1.2MESI 协议MESI 是一种基于实效的缓存一致性协议，通俗来说就是当一个 CPU 修改了一个变量的值以后，其他的 CPU 会立马感知到并且将自己的本次缓存设置为无效，而 MESI 对应的四个状态分别是 每一次 CPU 从内存中读取数据的时候，都会向其他 CPU 发送一个事件，其他 CPU 接收到该事件以后，都会给到相应。关于向消息总线发送的消息，感兴趣的话可以去看下维基百科 2禁止重排序volatile 关键字还有一个作用就是禁止重排序，该实现该功能依赖于内存屏障，在 hotspot 中，内存屏障如下有如下几个：LoadLoad、LoadStore、StoreLoad、StoreStore 而在 openJdk 里面，对这几个屏障有很详细的描述openJdk关于这几个的描述 在 Java 中，对于这几个命令可以简化为如下：该图表示的是在第一个操作之后，JVM 需要在第二个操作之前加入的一个内存屏障，正是这些操作才会使得 volatile 可以拥有禁止重排序的功能","link":"/2021/05/30/difference-between-synchronized-and-volatile/"},{"title":"谈谈HTTPS","text":"最近正好在看极客时间的「趣谈网络协议」，目前已经看了大半部分，对于之前 HTTPS 概念比较模糊的地方现在都差不多大致了解了，于是正好选择来记录下。 HTTPSHTTPS 是在原来的 HTTP 协议基础上进行加密通信，所以本质上 HTTPS 还是基于 TCP/IP 协议进行数据传输，而一个标准的 TCP/IP 协议族中，如下图 HTTPS 的 TLS/SSL 层就工作在应用层和运输层之间。 HTTP 的弊端在网络传输的过程中，HTTP 是以明文的方式进行交互的，所以在一些重要的交互接口，很容易被中间人截获到报文信息，从而威胁系统安全。 明文指的是应用层按照 HTTP 协议的标准将信息在网络上传输，例如一个登录接口：有如下两个字段： 1234{ &quot;userName&quot;:&quot;a&quot;, &quot;password&quot;:&quot;b&quot;} 只要这个网络包所经过设备，均可以对其进行抓包，从而知道用户的用户名和密码。 如果将时间拨回到2010年附近，那个时候上个网页，经常会弹出一些广告之类的，如果是非服务端返回的，那么就是中间人在这中间篡改了返回的报文。 优化既然 HTTP 的弊端是报文是明文，那么只要对其加密即可，使得中间人得到的都是密文，这样就算被截获，没有密钥也无法进行解密。 目前常见的加密算法有：「非对称」和「对称」，其中非对称加密的运算速度稍慢于对称加密，因此 HTTPS 选择的是用混合加密算法，即「非对称」算法交换「对称」算法的密钥，后续在数据传输的过程中采用「对称」算法进行数据的处理。 HTTPS 的解决方案 上图是一个完整的 HTTPS 握手图， 其中生成相关密钥的步骤是 3-7，也就说说在第 7 步完成以后，后续的通讯都会基于「对称」加密算法将报文进行加密通信。 那么客户端和服务端是如何约定相关密钥的呢？客户端和服务端各自会随机生成一个随机数，分别记为 a1、a2，服务端将自己的 CA 证书再下发到客户端，证书里面包含公钥信息， 此时客户端再生成一个随机数字，记为 pre-master，然后通过公钥将 pre-master 进行加密传输给服务端。 这个时候，客户端和服务端均有三个数：a1、a2、pre-master，然后客户端和服务端采用相同的算法，算出一个 secret-master 数，该数就是以后采用的「对称」算法的密钥了。 于是后续便可以通过该密钥进行加密通信了。 中间人攻击HTTPS 在生成密钥的时候，全部都是明文信息，那么肯顿过存在中间人攻击的情况，具体如下： 如果在客户端和服务端之间有一个中间人，此人生成了自己的证书，并且截获了a1、a2，然后将自己m1、m2 分别给了服务端和客户端，此时 M 所拥有的是 a1、a2、m1、m2、pre-master、服务端CA证书的公钥，自己的CA证书的公私钥。 那么此时 M 就完全可以解密出 C 和 S 之间的通信内容。 其实在业务开发中，如果有过抓包经验，都用过 charles、finder 这类软件，这类软件在抓取 HTTPS 的报文信息的时候，采取的就是上述的中间人方案。 CA 认证为了避免上述情况发生，需要一个第三方的独立结构为这些证书所背书，因为中间人是不知道真实的服务端的私钥的，因此只要客户端将 pre-matser 通过服务端的公钥进行加密以后，M 就无法得到 pre-master，也就无法算出真正的密钥了。 这个第三方独立结构就是：CA 认证，具体的含义可以看 CA认证 简单来说就是，CA 机构会根据你的域名、持有人信息等等，会采用「非对称」加密算法为你生成一对公私钥，其中公钥内置在证书中，私钥交由申请者保管，并且不允许泄漏。 客户端收到服务端证书以后，会查看其 ROOT 证书是不是被信任的，如果是非授信的，则此次连接会被拒绝，下图失败的的 CA 证书 可以看到百度的顶级证书是 GlobalSign Root CA，而此证书正好已经内置于操作系统中，所以可以确保此次的握手连接中，CA 证书没有被篡改，下图是 mac 系统内置的证书。 CA 认证是如何防止中间人攻击的还是回到刚才的这个例子。 M 如果给客户端下发的是自己的 CA 证书，那么客户端会验证这份证书的真实性，因为 M 是无法伪造 S 的证书，所以 C 查看证书信息后就会直接拒绝连接，因此可以防止中间人攻击。 如果 M 将 S 的证书原封不动的发给 C，那么由于 M 拿不到该证书对应的私钥，所以也无法进行解密（无法解密也就拿不到pre-master），从而也无法得到最后的「对称」算法的密钥，因此也防止了中间人攻击。 HTTS 连接密钥生成的过程首先 C 端会额外在生成一个随机数字，称为 pre-master，此时 C 端 会将 pre-master 通过公钥进行加密，然后将密文发送给服务端，此时服务端会通过私钥将密文解成明文，然后得到 pre-master。 此时 C 和 S 端则都通过 PRF 算法，将（a1、a2、pre-master）用算法算出最终的 secret-master，后续的报文就会通过 secret-master 进行加密传输了。 Charles上述说了 CA 证书是如何防止中间人攻击的，那么 charles 为什么还可以抓得到 HTTPS 的报文呢？ 如果我们安装 charles 以后，开启抓包，打开浏览器就会有如下提示： 可以看到浏览器已经检测到了非信任证书了，而我们能抓到 HTTPS 的报文，是因为已经将 charles 的证书信任过了。 此时另一个问题就来了，既然只要将证书信任下就可以抓到 HTTPS 的报文，那么还有不有更加高级的做法呢？ 其实查看 HTTPS 的交互流程，之所以出现这个情况，是因为客户端直接信任了中间人的证书，而信任的逻辑是由第三方来决定的，所以才导致了这种情况。 如果不信任任何非官方颁发的 CA 证书，一般是不会有什么问题的。 防止抓包SSL Pinning为了解决上述的问题，可以将服务端的证书直接下发到客户端，在握手阶段的时候，就和收到的服务端证书进行对比，如果发现不一致就直接拒绝握手。 锁定方式一般由两种： 证书锁定：但是会过期，如果客户端不更新证书，则会导致后续的访问都会出现问题 特征锁定：将证书中的某些字节码/公钥提取出来内置于APP中 自定义协议目前市面上的抓包程序都是针对 HTTP 协议，那么只需要在 TCP/UDP 上自定义应用层的协议，就可以完美的避免抓包了，这里可以看一些美团的一个技术文章：美团点评移动网络优化实践 APP中设置代理检测这一种就是先检测是否开了代理，如果开了则设置 Proxy 为 NO_PROXY","link":"/2022/04/23/https-miscellaneous-talk/"},{"title":"jackson中的范型","text":"在 jackson 将字符串转为对象的时候，如果是不带有范型的数据类，那么在 strig 转 obj 的时候不会出现什么问题，如果你的对象带有范型的话，那么就需要注意了，稍不注意就会抛出如下异常 java.util.LinkedHashMap cannot be cast to XX 出现该原因的原因在于 jackson 转换对象的时候，如果没有识别到原始类型，会默认将其转为 LinkedHashMap，后续一旦使用该类，就会抛出上述错误demo如下： 如果你是使用如下方法进行 string 转 object，那么范型会被映射称为 LinkedHashMap 1public &lt;T&gt; T readValue(String content, Class&lt;T&gt; valueType) 一、jackson 是如何赋值的（默认构造器和多参数构造器）无参构造器对于含有无参数构造器的对象，那么处理流程很简单，就是直接通过 _defaultCreator.call() 来进行对象的创建，通过 newInstance() 来实例化，最后通过反射 invoke 方法进行赋值 有参数构造器如果含有多个构造函数，那么就必须指明需要用哪一个构造函数进行初始化，否则代码会直接报错 no Creators, like default constructor, exist 很明显，这个表示没有指定一个构造器作为 Jackson 实例化使用，因为对于多参数构造器，Jackson 不知道用赋值的顺序，所以需要人为进行指定但是如果是两个 String 的入参，那么如何进行字段映射？，其实 jackson 还是不知道 这个时候就需要指定一个构造器了@JsonCreator，配合 @JsonProperty 指定字段。因为对于多个参数的构造器，需要明确告诉 Jackson 如何对其进行赋值，否则会抛出以下错误 Argument #0 of constructor [constructor for xyz.somersames.model.Producer (3 args) 123456@JsonCreatorpublic Producer(@JsonProperty(&quot;name&quot;) String name, @JsonProperty(&quot;age&quot;) String age, @JsonProperty(&quot;friend&quot;) T friend) { this.name = name; this.age = age; this.friend = friend;} 二、范型擦除如果你在代码中这样写，在编译期间都是不通过的 12345private void testType(){ Food&lt;Apple&gt; food = new Food&lt;Apple&gt;(); Food&lt;LinkedHashMap&gt; linkedHashMapFood = new Food&lt;LinkedHashMap&gt;(); food = linkedHashMapFood;} 那么为什么在运行期间可以将 LinkedHashMap 赋值到 food 上去呢，这就是范型擦除，在运行的时候 JVM 其实是不知道你的范型类型的 但是在 Jackson 是可以实现范型的转换，那么就要在运行期间获取到该类的原始类型，官方推荐如下方法： 12345public &lt;T&gt; T readValue(String content, TypeReference&lt;T&gt; valueTypeRef)//demoobjectMapper.readValue(&quot;XXX&quot;, new TypeReference&lt;XXX&gt;() {}); TypeReference 是一个抽象方法，如果你的 java 基础还可以的话，一眼就可以看到这就是一个匿名内部类，匿名内部类其实可以带有范型的原始信息的 匿名内部类如果一个类中含有匿名内部类，那么在编译以后会形成两个 class 文件的，demo 如下： 在编译以后会生成两个文件 很明显，第二个 class 文件是带有原始的信息的，这说明匿名内部类是可以继承自原始类型的 PS：这一点其实和 JDK 的动态代理类似，都是通过新生成一个 class 文件，但是作用却是不同的 JackSon 获取原始类型下面我以一个简单的例子来表示这两者的区别： 打印结果如下： 注意红框内部的区别，可以看到 ListWithInit 里面是一个匿名内部类，而上面所说了匿名内部类是会重新生成一个class文件的，从而导致 ListWithInit 是可以拿到范型信息的，把 B 编译以后生成的 B$1.class 代码如下 1234567import java.util.ArrayList;class B$1 extends ArrayList&lt;String&gt; { B$1(B var1) { this.this$0 = var1; }} 可以看到编译后的 class 文件实际上已经把原始的范型信息包含了，而普通的 ArrayList 因为其父类还是 AbstractList&lt;E&gt;，所以在运行期间是拿不到原始的范型信息的 最后在 JDK 中通过 getActualTypeArguments 就可以拿到范型了 三、反射设置范型1234567private void invokeTest() throws Exception { Class clazz = Class.forName(&quot;xyz.somersames.demo.Food&quot;); Food&lt;Apple&gt; food = new Food&lt;Apple&gt;(); Method m1 = clazz.getDeclaredMethod(&quot;setT&quot;,Object.class); m1.invoke(food,new LinkedHashMap&lt;String,String&gt;()); System.out.println(food.getT());} 总结其实说了那么多，Jackson 的转换大致流程就是先通过构造器来进行对象的实例化，最后通过反射进行字段的赋值操作","link":"/2021/04/16/jackson%E4%B8%AD%E7%9A%84%E8%8C%83%E5%9E%8B/"},{"title":"我就用了一个 assert，竟然让我损失了一杯星巴克","text":"虽然有那么点标题党，但是确实让我损失了一杯星巴克，耽误了测试小姐姐的时间～～ 简介事情的起因是这样的，我们有一个接口是对外修改状态用的，例如状态有1，10，20，30，40，50 等等，状态的流转在业务上来说只允许操作一次，不然会导致一些重复的事件触发。 所以为了防止并发修改导致的系统问题，我们在修改数据库的时候，做了一个 CAS 判断 12update tbl set status = ? where status = ? and order_id = ? 一）事件现场在代码中，我们的 update 操作是会返回对应的影响的行数的，上面的 SQL，因为 order_id 是主键。所以在更新成功的时候，一定会返回 1。 于是我们的代码就是这样了： 12assert assertTest.updateOrder() &gt; 0; 然后呢，这代码在本地运行的非常符合要求，本地冒烟测试案例都是 100% 成功，于是我将这个代码提交到指定分支，然后就去弄别的去了。。 二）本地不是好好的吗后来测试找过来了，后面有几个案例通过不了，接口返回更新成功，然后数据库实际却没变，还是原来的值，关键是还可以多次修改，数据库也没变… 此时我心里想？不应该啊，我在本地测试这么久，为啥发到测试环境就有问题🤔️ 本着我本地是好的，因此我首先怀疑是测试操作不当。。于是亲自去看了一番操作。。没想到操作确实没问题。 2.1）难道是主从延迟第一时间想到难道主从延迟，但是转念一想，就算从库同步主库的数据有问题，那么我的写操作全部是在主库，应该是 CAS 失败，不应该操作成功。 所以这个直接被否定了。 2.2）测试环境的包不对我的第二感觉就是，难道测试环境的版本被其他人覆盖了？后来我看了下发版的 commitid，与我最后一次的提交对的上。 所以这个又被被否定了。 三）怀疑人生此时这 case 直接弄得我怀疑人生。。于是一番冷静下来，首先查看日志请求，因为公司用的 cat，于是上去看了下，却发现 update 操作完全没有执行。 也就是 assert assertTest.updateOrder() &gt; 0; 这一行一直没有执行。WTF，我本地不是好点吗？？ 于是我又在本地自己 debug，发现一切正常，此时我就懵逼了，难道我用法不对。 我直接在项目中搜索了所有的 assert 关键字，发现 JDK 也是这样用的，感觉没多大毛病。 3.1）继续怀疑人生既然 JDK 都这样在用，那么我这样也应该没什么问题吧，后来我转念一想，既然我遇到问题了，那么是不是其他人也会遇到问题。 直接上 stackoverflow 上去找找看。。还真找到了 这老哥代码和我一致，但是也不生效，他的代码如下： 12345public void withdraw(double d){ double diff = balance - d; assert (diff&gt;=0 ) :&quot; Insufficient funds!&quot;; balance = diff;} 这个时候，有一个回答道： 看那个声望值，一看就是一个大佬，这个大佬回答道，assert 是默认被关闭的，需要通过 JVM 的配置来打开，于是我先看下我本地的 idea 配置，发现确实打开了 但是！！！测试环境的没打开。 四）解决方案首先想到的是直接修改项目的 JVM 配置，但是考虑后续如果有人在跑我的代码的时候，也忘记打开这个开关了，估计会懵逼，所以索性直接批量替换了 assert，直接用 Spring 自带的 Assert 处理了。 五）后续虽然问题解决了，但是我想了下，既然这个 assert 是依赖 JVM 配置的，那么如果不开启，是不是就代表着 JDK 所有的断言全部失败了？ 然后我又抽查了几个 assert 关键字用的地方，发现大多数都是在 JavaFX 这个包里面，那么我就在好奇，为什么这个关键字大多数出现在 javafx 包里面。 在 StackOverFlow 上我看到了这个问题： 提问者提到了一句话当 debug 的时候，非常的有用，我突然想到，我们现在用的各种软件，都有一个功能：调试模式 那是不是意味着 JavaFX 开发的软件，如果打开调试功能，就可以直接修改 JVM 配置，然后开启 assert 关键字。 这样 FX 内置的包就会通过断言将一些信息提前暴露出来。 六）后续后续就是中秋节来，然后因为没有用过 JavaFX 开发软件，所以只能是自己猜测了","link":"/2022/09/07/java-assert-incorrect-use/"},{"title":"java validation 的国际化","text":"背景在一个完整的项目里面，肯定是有各种各样的入参校验的，如果业务上的一些逻辑校验，可以放在 Service 层面进行，但是如果是 Controller 里面的校验，直接可以用 validation 进行验证。配合注解可以很方便的实现各种各样的入参校验。如下： 1234567891011121314151617181920212223242526public class User { @NotBlank(message = &quot;用户名称不能为空&quot;) private String name; @Max(value = 18) @Min(value = 10) private Integer age; public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; }} 然后在Controller里面 123456789@PostMapping(value = &quot;test&quot;)public String testError(@Valid @RequestBody User user ,Errors errors){ if(errors.hasErrors()){ for (ObjectError err : errors.getAllErrors()) { return err.getDefaultMessage(); } } return &quot;OK&quot;;} 当入参的 JSON 如果 name 是空的话，就会直接返回 用户名称不能为空，当然这里是做了一些简化，返回的应该还有 Code 和 Message。这样的话一个简单的 Controller 检验入参就实现的，但是为了扩展性和可维护性，还需要考虑国际化以及可配置化。 扩展性如果在之后需要更改一个校验的提示语，那么在上面的代码里面是需要修改代码的，那么最好的办法就是把这些提示信息都加到配置文件中，那么以后需要修改某些提示的话，直接改配置文件即可。在 Spring 官方文档的 4.7.1 中有一个类 MessageCodesResolver 可以用来实现错误信息的可配置化 MessageCodesResolver这个类需要和spring.mvc.message-codes-resolver-format一起来使用，根据官方文档的提示，查看DefaultMessageCodesResolver.Format，会发现这个参数有两个枚举，一个是 PREFIX_ERROR_CODE，另一个是 POSTFIX_ERROR_CODE。这两个什么意思呢，简单来讲，根据上面的 User 类，如果我要配置当 name 不为空的提示语，下面两种枚举对应在配置文件中的键值是不一样的。 PREFIX_ERROR_CODE NotBlank.user.name = 用户名称不能为空 POSTFIX_ERROR_CODE。 user.name.NotBlank = 用户名称不能为空 个人偏向于第二种写法的，因为感觉第二种更加符合阅读习惯。 messgae配置文件既然需要做成配置文件，那么在 application.yml 里面把一些属性都配置好，如下： 12345spring: mvc: message-codes-resolver-format: postfix_error_code messages: basename: i18n/validation 然后在 Resources 下面新建一个 validation.properties 123user.name.NotBlank = 用户名称不能为空user.age.Max = 年龄最高不能高于18user.age.Min = 年龄最高不能低于10 Controller 校验12345678910111213@AutowiredMessageSource messageSource;@PostMapping(value = &quot;test/cn&quot;)public String testErrorCn(@Valid @RequestBody User user ,Errors errors){ if(errors.hasErrors()){ for (ObjectError err : errors.getAllErrors()) { String msg = messageSource.getMessage(err, Locale.CHINA); return msg; } } return &quot;OK&quot;;} 在这里暂时先不管 Locale.CHINA 这个参数的含义，首先通过 errors 先判断入参是否有问题，有的话直接通过 messageSource.getMessage() 方法直接返回错误信息即可。 PsotMan测试1234POST /test/cn HTTP/1.1Host: localhost:8071{&quot;age&quot;:15} 返回：用户名称不能为空 那么以后如果需要修改返回提示的话，直接修改配置文件即可，从而不需要修改代码。 国际化如果需要将该提示国际化，直接修改配置文件即可，新建两个配置文件validation_zh_CN.properties，validation_en_US.properties，然后分别新建提示配置如下： 123456789validation_zh_CN.properties：user.name.NotBlank = 用户名称不能为空user.age.Max = 年龄最高不能高于18user.age.Min = 年龄最高不能低于10validation_en_US.properties：user.name.NotBlank = user name can not be nulluser.age.Max = the max gae can not grater than 18user.age.Min = the max gae can not less than 10 此时在Controller里面代如下： 12345678910111213141516171819202122232425262728293031323334353637383940@RestController@ResponseBodypublic class StartController { @Autowired MessageSource messageSource; @PostMapping(value = &quot;test&quot;) public String testError(@Valid @RequestBody User user ,Errors errors){ if(errors.hasErrors()){ for (ObjectError err : errors.getAllErrors()) { return err.getDefaultMessage(); } } return &quot;OK&quot;; } @PostMapping(value = &quot;test/cn&quot;) public String testErrorCn(@Valid @RequestBody User user ,Errors errors){ if(errors.hasErrors()){ for (ObjectError err : errors.getAllErrors()) { String msg = messageSource.getMessage(err, Locale.CHINA); return msg; } } return &quot;OK&quot;; } @PostMapping(value = &quot;test/en&quot;) public String testErrorEn(@Valid @RequestBody User user ,Errors errors){ if(errors.hasErrors()){ for (ObjectError err : errors.getAllErrors()) { String msg = messageSource.getMessage(err, Locale.US); return msg; } } return &quot;OK&quot;; }} User的代码如下： 1234567891011121314151617181920212223242526public class User { @NotBlank private String name; @Max(value = 18) @Min(value = 10) private Integer age; public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; }} 当我们请求 test/en 和 test/cn的时候就会出现不同的提示， test/en user name can not be null test/cn 用户名称不能为空 这其中重要的实现就是 Locale.US 和 Locale.CHINA，在这里实现的话，最好是根据用户的选择语言来动态的切换实现Local的转换。","link":"/2019/12/09/java-validation-%E7%9A%84%E5%9B%BD%E9%99%85%E5%8C%96/"},{"title":"java异步IO的回调机制","text":"异步在Java的nio里面，经常遇到的一个词语是回调，一个主线程负责分发各种请求至子线程，同时子线程处理完毕之后通知主线程，这其中就涉及到了回调机制。 在Java中，异步IO的回调方式主要是CallBack和Future。 由于Future获取结果是一种阻塞的方式，所以本次就主要来了解Callback回调方式的运行机制。 由于在异步IO里面，主线程不需要等待子线程来获取结果，所以可以极大的提高程序运行的效率，但是子线程必须在完成之后通知父线程，于时这就引出了回调。 在Java中回调是通过一个匿名对象来实现，每一个线程子线程在运行的时候都会传入一个匿名的对象，然后子线程完成任务之后，通过调用该对匿名对象来进行回调 代码：回调接口12345public interface ComplateHande { void complated(); void fial();} 子线程1234567891011121314151617181920212223242526272829303132public class ClientServer extends Thread{ private ComplateHande complateHande; private int sleepTime; private String message; public ClientServer(ComplateHande complateHande,String message,int sleepTime) { this.message= message; this.sleepTime = sleepTime; this.complateHande = complateHande; } public ClientServer() { } @Override public void run() { try { comulate(); } catch (InterruptedException e) { e.printStackTrace(); } } private void comulate() throws InterruptedException { Thread.currentThread().sleep(sleepTime*1000); System.out.println(this.message); this.complateHande.complated(); }} Main方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class MainServer { public static void main(String[] args) throws InterruptedException { ClientServer clientServer1 =new ClientServer(new ComplateHande() { @Override public void complated() { System.out.println(&quot;处理完毕&quot;); } @Override public void fial() { System.out.println(&quot;处理失败&quot;); } },&quot;一号任务&quot;,2); clientServer1.start(); ClientServer clientServer2 =new ClientServer(new ComplateHande() { @Override public void complated() { System.out.println(&quot;2处理完毕&quot;); } @Override public void fial() { System.out.println(&quot;处理失败&quot;); } },&quot;二号任务&quot;,1); clientServer2.start(); ClientServer clientServer3 =new ClientServer(new ComplateHande() { @Override public void complated() { System.out.println(&quot;3处理完毕&quot;); } @Override public void fial() { System.out.println(&quot;2处理失败&quot;); } },&quot;三号任务&quot;,1); clientServer3.start(); ClientServer clientServer4 =new ClientServer(new ComplateHande() { @Override public void complated() { System.out.println(&quot;4处理完毕&quot;); } @Override public void fial() { System.out.println(&quot;处理失败&quot;); } },&quot;四号任务&quot;,3); clientServer4.start(); Thread.currentThread().sleep(1000); System.out.println(&quot;自己处理自己的事情+1&quot;); Thread.currentThread().sleep(1000); System.out.println(&quot;自己处理自己的事情+1&quot;); Thread.currentThread().sleep(1000); System.out.println(&quot;自己处理自己的事情+1&quot;); Thread.currentThread().sleep(1000); System.out.println(&quot;自己处理自己的事情+1&quot;); Thread.currentThread().sleep(10000); }} 运行结果123456789101112二号任务2处理完毕三号任务3处理完毕自己处理自己的事情+1一号任务自己处理自己的事情+1处理完毕四号任务4处理完毕自己处理自己的事情+1自己处理自己的事情+1 其他方式在nio里面，其实还有其他的实现方式，比如通过建立一个主线程的阻塞队列，然后分发任务至子线程，子线程若处理完毕，则提交任务至主线程","link":"/2019/01/24/java%E5%BC%82%E6%AD%A5IO%E7%9A%84%E5%9B%9E%E8%B0%83%E6%9C%BA%E5%88%B6/"},{"title":"java信号量的简单了解","text":"在Java语言里面，Semaphore 的作用是可以控制对于同一个临界资源，允许多少个线程同时执行。当线程执行到临界区域的时候，需要先向 Semaphore 申请一个令牌，此时 Semaphore 会判断现有的的令牌是不是小于0，如果小于0，则阻塞当前线程，直至有线程将令牌归还回来。 申请到了permits如果一个线程直接申请到了permits，则是直接通过CAS操作将 state 减一即可，然后线程继续执行。 无法申请到permits当无法申请到 Semaphore 的 permits 的时候，则会将当前的线程进行阻塞，直到有线程执行完毕，释放了 permit。 Semaphore 的类关系那么在 Semaphore 里面，如果 permits 已经被降到0以下，按照信号量的规则，应当将当前线程阻塞，直到 permits 大于 0。查看 Semaphore 的类组织结构，会发现它的一些操作全部都是依赖于自己的一个 Sync 变量，此外还有两个变量用于实现公平锁和非公平锁，都是继承自 Sync，而 Sync 则是继承自 AbstractQueuedSynchronizer (下文简称AQS)。所以 Semaphore 的实现全部是依赖于 AQS。以下为 Semaphore 的类图： 下面是一个例子来说明 Semaphore 是如何工作的。 如下Demo 12345678910111213141516171819202122232425262728293031323334public class SemaphoreTest implements Runnable { private static final Semaphore semaphore = new Semaphore(1); private int no; public static void main(String[] args) throws InterruptedException { for(int i =0 ;i&lt; 2 ;i++){ new Thread(new SemaphoreTest(i)).start(); } Thread.currentThread().sleep(1000000); } public void run() { System.out.println(no + &quot;start&quot;); try { semaphore.acquire(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(no); System.out.println(&quot;semaphore&quot;); try { Thread.currentThread().sleep(10000); semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } } public SemaphoreTest(int no) { this.no = no; }} 在这个例子中，首先获取到 permits 的线程会执行，而第二个线程则会被阻塞，直到 permits 被释放。这也是 Semaphore 的目的，控制有多少个线程可以同时的访问某一个资源。 设置permits在 Semaphore 里面，直接使用 AQS 里面的state变量来决定允许多少个线程同时访问一个资源，通过 Semaphore 的构造函数就可以发现： 1234567891011121314public Semaphore(int permits) { sync = new NonfairSync(permits);}static final class NonfairSync extends Sync { private static final long serialVersionUID = -2694183684443567898L; NonfairSync(int permits) { super(permits); } protected int tryAcquireShared(int acquires) { return nonfairTryAcquireShared(acquires); }} 该super就是调用 Sync 的构造函数，然后调用 AQS 里面的 setState方法，最后设置 AQS 的 state。 获取permits当一个线程尝试调用 acquire 方法的时候，最终会通过 NonfairSync 的 tryAcquireShared 方法，在该方法里面，会获取当前线程的 state，然后减去入参带过来的参数（默认是 1 ），最后判断是否小于 0，若大于 0 的话，则直接采用 CAS 的操作将剩余的 state 替换掉。小于 0 的话，直接返回并且会进入另一个方法。 12345678910111213final int nonfairTryAcquireShared(int acquires) { for (;;) { int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; }}protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} 由于 state 是由 volatile 修饰的，所以说当一个线程原子性的修改了 state，另一个线程也会立即获取到 state 的最新状态的。这个方法不是一个原子性的，但是由于 CAS 操作是原子性的，那么最终还是能保证一致性的。如下： 如果一个线程在获取到了 available 恰好为 1， 准备减去 1 的时候，此时另一个线程恰好 CAS 执行完毕，将 state 更新为 0 了，此时第一个线程在执行 CAS 的时候，由于 expect 已经修改为 0 了，所以此时 CAS 操作一定不成功。 如果在 getState 之前，state 已经被修改成 0 的，那么由于 remaining &lt; 0，所以直接返回 remaining。 线程阻塞在这里先简单介绍下线程的阻塞方式：首先生成一个 Node，然后再判断下队列的尾部 tail 是不是为 null，如果是的话初始化 head 和 tail，并且将当前线程的 Node 的前置节点指向 head，然后通过 CAS 操作将 tail 设置为当前线程创建的 node， 最后将 head 的后置节点指向该节点。 然后再判断前置节点是不是 head，是的话就再次尝试获取 state，若获取不到则直接将前一个节点的 waitStatus 修改为 -1（即后置节点的线程需要被唤醒）然后直接通过LockSupport.park(this)将其阻塞。 当 CAS 操作失败，一般都是 state 已经小于 0 了，此时就会进入 doAcquireSharedInterruptibly 方法里面，在该方法里面会使用 AQS 里面的 FIFO 队列，来实现对于临界资源的控制。 详细Semaphore 首先会进行创建一个 Node，在首次阻塞某一个线程的时候，由于 tail 为null，所以会直接进入 enq 方法，在该方法里面会将 tail 和 head 都设置为一个新的 node，然后展开第二次的循环，最后在将当前线程的 node 的 prev 指向 head，通过CAS操作将 tail 指定为该node，最后将 head 的 next 指向 该node（注意这一步为非原子性的，所以会影响到后面的release方法的一个判断） 1234567891011121314151617181920212223242526272829private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node;}private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } }} 到这里，一个 FIFO 队列就生成了，此时 head 为一个node（waitStatus为0，next为被阻塞的线程），当从 addWaiter 方法返回的时候，Semaphore 还会进行一个判断，如果当前线程的前置节点是 head，就再次尝试一次获取 state，如果获取不到的话就将前置节点的 waitStatus 设置成 -1，然后通过LockSupport.park(this)将本线程中断，至此该线程被阻塞了。 如果在判断前置节点是 head 之后，然后通过 tryAcquireShared 获取到了 state 那么此时就会调用 setHeadAndPropagate 将自己设置为头节点，同时判断是否需要唤醒后续的节点 释放假如一个线程执行完毕之后，是会调用 release 方法来释放资源的，首先还是以 CAS 操作原子性的增加 state， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667protected final boolean tryReleaseShared(int releases) { for (;;) { int current = getState(); int next = current + releases; if (next &lt; current) // overflow throw new Error(&quot;Maximum permit count exceeded&quot;); if (compareAndSetState(current, next)) return true; }}private void doReleaseShared() { /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; }}private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread);} 当 CAS 操作成功之后返回 true，然后调用 doReleaseShared 方法，在该方法里面，会首先将 head 的状态更改为 0，然后然后通过 unparkSuccessor 来唤醒后面的线程，在这里需要注意的是那一个 for 循环里面的代码。 为什么for循环的时候需要从尾节点开始在这里其实是由于前面在设置尾节点的时候 CAS 虽然是一个原子性操作，但是在 CAS 操作之后，紧跟着 pred.next = node 这一步为非原子性的，所以就导致了有可能从头遍历的时候会断开掉，所以此时就需要从尾节点开始，因为这样一定不会断开的，也是最有保证的。","link":"/2019/12/04/java%E4%BF%A1%E5%8F%B7%E9%87%8F%E7%9A%84%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3/"},{"title":"java泛型的一些思考","text":"为什么需要泛型在Java中其实Object这个类已经可以解决大部分的泛型问题了，那么现在为什么还需要泛型了，一种说法是为了安全，因为在编译期使用泛型的话便可以基本确定这个参数的类型了，但是使用Object的话，由于Object是所有类型的超类，所以这会给代码造成一定的安全性问题。 1234567public void testList(T t){ List&lt;Object&gt; list =new ArrayList&lt;&gt;(); List&lt;T&gt; list1 =new ArrayList&lt;&gt;(); list.add(1); list.add(&quot;1&quot;); list.add(t); } 在上面的例子中，list1由于其类型是T，这就会导致在向list1中插入数据的时候只能是泛型T，但是对于list而言，由于Object是所有类的超类，那么这个list就可以插入任何值了，所以这就导致了一些安全问题 关于Java的协变和逆变：在Java里面数组是协变的。对于数组来讲一个Object数组是可以存放任何值： 123Object[] o =new Object[12];o[0]=1;o[1]=&quot;1&quot;; 而对于集合来说呢？显然一个List肯定是不允许同时存放int和String,所以对于集合来讲得话，它就不是协变得。这是因为集合在运行得时候有一个类型擦除，也就是说List&lt;String&gt;最后在编译器看来就是一个List，也就是它的原生类型。所以对于集合来讲得话就是一个逆变得。 泛型得用处泛型有什么用呢？或者说泛型得优缺点在哪里呢？泛型由于在编译期并不知道他会是什么类，所以泛型在构造得时候是一个非常麻烦的事情，类似于如下： 1T t1 =new T(); 这行代码明显是错误得，为什么呢？因为T在编译期间并不知道这个是什么类，既然都不知道这个是什么类，那么又怎会知道调用哪一个构造函数呢？","link":"/2018/04/04/java%E6%B3%9B%E5%9E%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"title":"layui使用总结","text":"前言由于自己平时对前端的css和js学的不是太好，而现在又需要自己来写一个前端页面，无意间在GitHub中看到了layui，所以抱着尝试的心态，学习了一下，现在主要是自己做一个总结。可能之后会学习Vue等前端跨框架 关于Layui的table组件首先Layui的table组件：在Layui中创建一个table组件需要先写一个table标签：&lt;table class=&quot;layui-hide&quot; id=&quot;test&quot; lay-filter=&quot;demo&quot;&gt;&lt;/table&gt;，在这之中 id 是需要在table.render中使用的。例如： 1234567891011121314table.render({ elem: '#test' ,height: 315 ,url: '/user/getUser' //数据接口 ,page: true //开启分页 ,cols: [[ //表头 {field: 'id', title: 'ID', width:80, sort: true, fixed: 'left'} ,{field: 'age', title: '年龄', width:80} ,{field: 'dataname', title: '用户名', width:80} ,{field: 'sex', title: '性别', width:80} ,{field: 'password', title: '密码', width:80, sort: true} ,{fixed: 'right', width: 165, align:'center', toolbar: '#barDemo'} ]] }); 在这段js之中elem后面的值表示的就是table标签之后的id，而这段代码表示的就是在table中异步加载数据，后面的toolbar 表示的是需要创建三个button。注意最后一行的toolbar : #barDemo 它会跟下面的三个按钮一起对应。并且将这三个按钮一起添加到那个表格的后面，这之后的三个标签的创建方式如下: 12345&lt;script type=&quot;text/html&quot; id=&quot;barDemo&quot;&gt; &lt;a class=&quot;layui-btn layui-btn-primary layui-btn-xs&quot; lay-event=&quot;detail&quot;&gt;新增&lt;/a&gt; &lt;a class=&quot;layui-btn layui-btn-xs&quot; lay-event=&quot;edit&quot;&gt;编辑&lt;/a&gt; &lt;a class=&quot;layui-btn layui-btn-danger layui-btn-xs&quot; lay-event=&quot;del&quot;&gt;删除&lt;/a&gt;&lt;/script&gt; 这三个后的lay-event的值在之后会用到的。 123table.on('tool(demo)', function(obj){ //注：tool是工具条事件名，demo是table原始容器的属性 lay-filter=&quot;对应的值&quot; var data = obj.data //获得当前行数据 ,layEvent = obj.event; //获得 lay-event 对应的值 function(obj)的obj传入进来的是一些参数，obj.data 是获取所选中的数据行的一些值，obj.Event 获取的是前面的lay-event的值，在之后可以判断 if(layEvent === 'detail')，然后就可以及进行操作了。 123456789layer.open({ type: 1, title: &quot;用户信息修改&quot;, closeBtn: 1, area: 'auto', shadeClose: true, skin: 'yourclass', content: ''// 在这里可以写html标签然后会在那个面板显示了。 }) 12345layer.confirm('真的删除行么', function(index){ obj.del(); //删除对应行（tr）的DOM结构 //向服务端发送删除指令 layer.close(index); $.ajax({ confirm可以弹出一个框来确认是否进行删除以免用户的误操作，而obj.del()则是可以删除该行。layer.close()则是可以关闭次对话框。而lay.message则是一个弹窗，用于在页面上提示用户进行了什么操作。 和Jquery一起使用这个UI框架是可以和Jquery一起来使用，Jquery可以发起异步请求从而来进行一些数据操作。 使用截图：","link":"/2018/03/22/layui%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"},{"title":"maven下载快照的问题","text":"今天在使用maven下载一个快照文件的时候，只在 settings.xml 文件中配置了镜像源，并没有配置 release 版本和 snapshoot 版本,所以就导致了在下载快照文件的时候一直出现问题：Missing artifact 大意就是说找不到这个jar的pom文件啥的，然后看了下本地的仓库，也并没有看到下载的文件夹。 解决办法：在 pom.xml 中设置快照的下载地址，配置如下： 12345678910&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;仓库的ID&lt;/id&gt; &lt;!-- &lt;name&gt;Spring Milestones&lt;/name&gt; --&gt; 如果没有可以忽略 &lt;url&gt;https://repo.spring.io/libs-milestone&lt;/url&gt; 快照仓库的URL &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; 打开镜像 &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 最后解决了，如果为了方便，其实可以在 settings.xml 中直接配置，以减少后期多个pom.xml配置的麻烦","link":"/2018/07/17/maven%E4%B8%8B%E8%BD%BD%E5%BF%AB%E7%85%A7%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"leetcode上一道求出数组前三大的数字","text":"题目如下: 123456Given scores of N athletes, find their relative ranks and the people with the top three highest scores, who will be awarded medals: &quot;Gold Medal&quot;, &quot;Silver Medal&quot; and &quot;Bronze Medal&quot;.Input: [5, 4, 3, 2, 1]Output: [&quot;Gold Medal&quot;, &quot;Silver Medal&quot;, &quot;Bronze Medal&quot;, &quot;4&quot;, &quot;5&quot;]Explanation: The first three athletes got the top three highest scores, so they got &quot;Gold Medal&quot;, &quot;Silver Medal&quot; and &quot;Bronze Medal&quot;. For the left two athletes, you just need to output their relative ranks according to their scores. 也就是一个乱序的数组，然后将数组中的前三大的数组换成指定的字符串。其中有一个解法是比较有新意的，其思路如下： 另外再开辟一个数组，然后数组的长度是原数组中数字最大的那个值，那么在进行遍历的时候，新建的数组中最后三位肯定是最大的，然后依次将其原索引找出来即可。 具体的代码如下: 12345678910111213141516171819202122232425262728293031public String[] findRelativeRanks(int[] nums) { String[] s =new String[nums.length]; int max =0; for(int i : nums){ if(i&gt; max){ max= i; } } int[] index =new int[max+1]; for(int i =0 ;i&lt; nums.length ;i++){ index[nums[i]] =i+1; //在这里加一是因为如果不加一那么第一个索引的数组之在index里面会是0，不好区分。 } int place=1; for(int i =index.length-1 ;i&gt;=0 ;i--){ if(index[i] !=0 ){ if(place ==1 ){ s[index[i] -1]=&quot;Gold Medal&quot;; }else if(place ==2){ s[index[i] -1]=&quot;Silver Medal&quot;; }else if(place ==3){ s[index[i] -1]=&quot;Bronze Medal&quot;; }else{ s[index[i] -1]=String.valueOf(place); } place++; } } return s; }","link":"/2018/07/25/leetcode%E4%B8%8A%E4%B8%80%E9%81%93%E6%B1%82%E5%87%BA%E6%95%B0%E7%BB%84%E5%89%8D%E4%B8%89%E5%A4%A7%E7%9A%84%E6%95%B0%E5%AD%97/"},{"title":"maven和json以及spring的一些问题","text":"maven和IDEA的一个问题在 IDEA 中可以正常使用maven的一些命令来进行 clean 和 complime ，但是在使用IDEA的build功能时一直提示某些包找不到，解决办法：执行 mvn clean 命令清除缓存，然后删除 .idea 这个文件夹中的文件 如果还是解决不了则可以直接换一个 maven ，最好的解决办法则是每一个项目，一个 maven。 阿里的Json包和对象之间的转换今天有一个新的需求是将一个 Json 字符串转换成一个Json对象，此时可以调用JSON.parseObject( new TypeReference(XXX),json串)来讲一个 Json 字符串转为一个对象 spring中读取配置文件相关的问题在 spring 中可以通过 @Value 这个注解来获取到配置文件中的一些配置，但是记住….不要再使用 new 关键字来再初始化","link":"/2018/07/18/maven%E5%92%8Cjson%E4%BB%A5%E5%8F%8Aspring%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"},{"title":"mongo多数据源的使用","text":"简介在开发的过程中，不可避免的会使用多数据源，但是相对于Mysql的多数据源，Mongo的多数据源配置还是比较容易的。 首先在pom.xml中引入mongo的驱动jar包以及springboot和mongo的一个jar包 原理Mongo的多数据源无非是首先读取配置文件，生成MongoProperties，通过MongoProperties来生成一个MongoTemplate,最后通过Repository来操作Mongo。 而多数据源就是生成多了MongoTemplate，然后通过多个MongoTemplate所对应的Repository来操作Mongo数据库 代码123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; &lt;!-- Mongo相关--&gt;&lt;!-- https://mvnrepository.com/artifact/org.mongodb/mongo-java-driver --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.mongodb/bson --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;bson&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt;&lt;/dependency&gt;","link":"/2018/12/05/mongo%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"mybatis中多条语句插入和主键返回","text":"在Mybatis的使用中，有时候会出现需要一对多的场景，尤其是在插入的过程中，即假设存在A，B两表。A表对B表是一对多的关系，在插入数据的过程中需要先插入A表，通过A表返回的主键然后再进行B表的查询，这个时候一般有两种操作。 方法一：首先获取A表的主键，然后通过for循环进行B表的插入方法二：使用mybatis的foreach进行多条语句的插入 在这里的话主要是记录下第二种方法，即通过mybatis的foreach来实现多条语句的插入: 建立数据库表1234567891011121314151617mysql&gt; select * from clazz;+----------+------------+| CLAZZ_ID | CLAZZ_NAME |+----------+------------+| 1 | 一年级 |+----------+------------+1 row in set (0.00 sec)mysql&gt; select * from student;+----------+--------+----------+| CLAZZ_ID | STU_ID | STU_NAME |+----------+--------+----------+| 1 | 101 | a || 1 | 102 | b |+----------+--------+----------+2 rows in set (0.00 sec) 在数据库建立的两张表会发现班级表和学生表是一对多的关系，此时如果一个班级需要插入许多个学生的话，此时就要使用mybatis的foreach进行多条语句的插入了 占位符在这里得话需要插入多条语句得时候不能使用#，而是应该使用$。在这里为什么是这个美元的符号，暂时得猜测是$解析得为静态得，也就是说mybatis会讲集合里面的值直接填充进来，类似于在ApplicationContext.xml中动态的加载jdbc.properties，需要使用$符号。新建Dao层类： 123public interface clazzStudent { Integer insertToStudent(List&lt;Map&lt;String,Object&gt;&gt; map);} 然后建立与之对应的mapper文件： 123456&lt;insert id=&quot;insertToStudent&quot; &gt; insert into student(CLAZZ_ID,STU_ID,STU_NAME) values &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (${item.get(&quot;clazz&quot;)},${item.get(&quot;stu&quot;)},${item.get(&quot;name&quot;)}) &lt;/foreach&gt; &lt;/insert&gt; 最后建立测试类： 123456789101112131415161718192021222324public static void main(String args[]) { Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); clazzStudent clazz = sqlSession.getMapper(clazzStudent.class); List&lt;Map&lt;String,Object&gt;&gt; list =new ArrayList&lt;Map&lt;String, Object&gt;&gt;(); for(int i=0 ;i&lt;3;i++){ Map&lt;String,Object&gt; itemmap =new HashMap&lt;String, Object&gt;(); itemmap.put(&quot;clazz&quot;,1); itemmap.put(&quot;stu&quot;,i); itemmap.put(&quot;name&quot;,&quot;'adsads'&quot;); list.add(itemmap); } int n=clazz.insertToStudent(list); System.out.println(n); sqlSession.commit(); } finally { sqlSession.close(); } } 在这里需要注意得是 &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt;在这个里面的话collection=&quot;list&quot;这里的list其实不需要和main方法中得list名称对应，也就是说在Java得main方法中将 List&lt;Map&lt;String,Object&gt;&gt; list =new ArrayList&lt;Map&lt;String, Object&gt;&gt;()这个list改成list1也可以插入多条语句，这里参数list与collection中得名称无关。 separator分隔符，作用是foreach下面的语句执行完了之后之后用什么进行分割。而item则代表得是里面得那个map， indexindex代表的是索引，目前暂时没用到，所以就写一个索引 利用Java对象进行迭代修改代码之后也可以利用Java对象进行传参 123456789101112131415161718192021Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); clazzStudent clazz = sqlSession.getMapper(clazzStudent.class); List&lt;Student&gt; list1 =new ArrayList&lt;Student&gt;(); for(int i=0 ;i&lt;3;i++){ Student student =new Student(); student.setCLAZZ_ID(1); student.setSTU_ID(i); student.setSTU_NAME(&quot;测试&quot;); list1.add(student); } int n=clazz.insertToStudentPojo(list1); System.out.println(n); sqlSession.commit(); } finally { sqlSession.close(); } 但是使用对象传参的话需要注意得是在mapper文件中使用#符号，修改如下： 123456&lt;insert id=&quot;insertToStudentPojo&quot;&gt; insert into student(CLAZZ_ID,STU_ID,STU_NAME) values &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (#{item.CLAZZ_ID},#{item.STU_ID},#{item.STU_NAME}) &lt;/foreach&gt; &lt;/insert&gt; 用List传值：新建Dao层方法 1Integer insertToStudentList(List&lt;String&gt; list); 新建mapper方法： 123456&lt;insert id=&quot;insertToStudentList&quot;&gt; insert into student(STU_NAME) values &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (${item}) &lt;/foreach&gt; &lt;/insert&gt; 测试方法： 12345678910111213141516171819public static void insertList(){ Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); clazzStudent clazz = sqlSession.getMapper(clazzStudent.class); List&lt;String&gt; list1 =new ArrayList&lt;String&gt;(); for(int i=0 ;i&lt;3;i++){ list1.add(String.valueOf(i)); } int n=clazz.insertToStudentList(list1); System.out.println(n); sqlSession.commit(); } finally { sqlSession.close(); } } 区别在这里需要对Java对象传参使用得#和map传参得$进行一下区分。可以看下打印日志：java对象传参使用#传参得日志： 123DEBUG [main] - ==&gt; Preparing: insert into student(CLAZZ_ID,STU_ID,STU_NAME) values (?,?,?) , (?,?,?) , (?,?,?) DEBUG [main] - ==&gt; Parameters: 1(Integer), 0(Integer), 测试(String), 1(Integer), 1(Integer), 测试(String), 1(Integer), 2(Integer), 测试(String)DEBUG [main] - &lt;== Updates: 3 java使用List进行传参: 1234DEBUG [main] - ==&gt; Preparing: insert into student(CLAZZ_ID,STU_ID,STU_NAME) values (1,0,'adsads') , (1,1,'adsads') , (1,2,'adsads') DEBUG [main] - ==&gt; Parameters: DEBUG [main] - &lt;== Updates: 33 可以看到打印出来的日志发现$和#在解析上得区别，$这个是先解析，生成了一个sql之后再执行，而#则是一个占位符，是在后期的时候动态加入的。 主键返回mybatis的主键返回目前仅仅了解通过对象来返回。需要在mapper文件中加入一行配置即可： 123456789&lt;insert id=&quot;insertToStudentPojo&quot;&gt; &lt;selectKey keyProperty=&quot;CLAZZ_ID&quot; order=&quot;AFTER&quot; resultType=&quot;java.lang.Integer&quot;&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; insert into student(CLAZZ_ID,STU_ID,STU_NAME) values &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (#{item.CLAZZ_ID},#{item.STU_ID},#{item.STU_NAME}) &lt;/foreach&gt; &lt;/insert&gt; keyProperty=&quot;CLAZZ_ID&quot;表示的是需要返回的字段的值 123456789101112131415161718192021222324public static void pojo(){ Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); clazzStudent clazz = sqlSession.getMapper(clazzStudent.class); List&lt;Student&gt; list1 =new ArrayList&lt;Student&gt;(); for(int i=0 ;i&lt;3;i++){ Student student =new Student(); student.setCLAZZ_ID(1); student.setSTU_ID(i); student.setSTU_NAME(&quot;测试&quot;); list1.add(student); } int n=clazz.insertToStudentPojo(list1); System.out.println(list1.get(2).getCLAZZ_ID()); System.out.println(n); sqlSession.commit(); } finally { sqlSession.close(); } } 需要注意的是 System.out.println(list1.get(2).getCLAZZ_ID());因为是多条语句的插入，肯定是需要返回的最后一条插入后的主键，所以通过该方法便可以得到插入后的主键 总结目前就暂时这么多了，源码以后再了解了。","link":"/2018/04/09/mybatis%E4%B8%AD%E5%A4%9A%E6%9D%A1%E8%AF%AD%E5%8F%A5%E6%8F%92%E5%85%A5%E5%92%8C%E4%B8%BB%E9%94%AE%E8%BF%94%E5%9B%9E/"},{"title":"mybatis和mysql得相关记录","text":"首先对于Mybatis来说，如果是直接复制mysql里面的语句粘贴到mybatis的mapper文件里面去的话很容易导致user读取出错，假设在mysql中select * from user XX，若直接把这条sql语句复制到mapper文件中的话会导致user会成为mapper文件中的关键字","link":"/2018/04/10/mybatis%E5%92%8Cmysql%E5%BE%97%E7%9B%B8%E5%85%B3%E8%AE%B0%E5%BD%95/"},{"title":"MySql和时间相关的查询(二)","text":"DATE_ADD():这个函数可以将日期往前加上规定的年，月或者日，从而方便统计，例如需要统计本月的某些数据的话，一般来讲肯定是只需要大于本月月初即可，但是为了考虑程序的健壮性的话肯定是需要再加一个限制条件，比如说小于下个月1号。那么就需要一个一个DATE_ADD()函数： 123456SELECT *FROM tableWHERE time_columns &gt; CONCAT(LEFT(NOW() - INTERVAL 0 MONTH, 7), '-01') AND time_columns &lt; DATE_ADD(CURDATE()-DAY(CURDATE())+1,INTERVAL 1 MONTH); 这条sql便是求的本月的数据。同时在这里也对当前时间的函数进行一个对比： 123456789101112131415161718192021222324mysql&gt; SELECT CURDATE();+------------+| CURDATE() |+------------+| 2018-03-28 |+------------+1 row in set (0.00 sec)mysql&gt; select now();+---------------------+| now() |+---------------------+| 2018-03-28 15:07:15 |+---------------------+1 row in set (0.00 sec)mysql&gt; select CURTIME();+-----------+| CURTIME() |+-----------+| 15:07:39 |+-----------+1 row in set (0.00 sec) 顺便在这里再记录下mybatis中的别名的作用，在resultType中返回的通常是全路径包名，但是其实配置好别名的话其实是可以直接返回别名&gt; 深入浅出mybatis技术原理与实现P17以及后面的mapper文件的resultType 另外就是mybaytis的$和#的区别，其实$这个在配置文件中是很经常使用的，例如: 1234567&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; init-method=&quot;init&quot; destroy-method=&quot;close&quot;&gt; &lt;!-- 基本属性 url、user、password --&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${url}&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;${password}&quot; /&gt;&lt;/bean&gt; 在这里就是用的$,而且在mybatis的官方文档里面也提到过了关于#和$的区别” String Substitution By default, using the #{} syntax will cause MyBatis to generate PreparedStatement properties and set the values safely against the PreparedStatement parameters (e.g. ?). While this is safer, faster and almost always preferred, sometimes you just want to directly inject an unmodified string into the SQL Statement. For example, for ORDER BY, you might use something like this: ORDER BY ${columnName} Here MyBatis won’t modify or escape the string. NOTE It’s not safe to accept input from a user and supply it to a statement unmodified in this way. This leads to potential SQL Injection attacks and therefore you should either disallow user input in these fields, or always perform your own escapes and checks. 也就是说在这里的话$是一个静态占位符,但是在mybatis的源文件中一直没找到关于解析$的类和方法，只找到了关于解析#的方法： 123456public SqlSource parse(String originalSql, Class&lt;?&gt; parameterType, Map&lt;String, Object&gt; additionalParameters) { SqlSourceBuilder.ParameterMappingTokenHandler handler = new SqlSourceBuilder.ParameterMappingTokenHandler(this.configuration, parameterType, additionalParameters); GenericTokenParser parser = new GenericTokenParser(&quot;#{&quot;, &quot;}&quot;, handler); String sql = parser.parse(originalSql); return new StaticSqlSource(this.configuration, sql, handler.getParameterMappings()); } 关于这个${}的话在mybatis里面是会直接将我们所写的sql放入到预定的sql语句中，而不是像#{}那样会进行一个替换，所以会造成一个安全问题","link":"/2018/03/28/mysql%E5%92%8C%E6%97%B6%E9%97%B4%E7%9B%B8%E5%85%B3%E7%9A%84%E6%9F%A5%E8%AF%A2-%E4%BA%8C/"},{"title":"mysql的一些总结","text":"前言今天突然有一个写sql的机会，但是是手写，不像之前那样可以在数据库上做测试。这突然让我感觉有的语法有点生疏了，所以乘着这个机会来做一个全部的梳理。 groupby和where的顺序：今天是有两表做一个等值连接查询的，在这里应该是先where之后再进行group by，group by 是对where条件过滤之后再进行分组处理，所以where在前。 group by在这里之前一直对group by有一个比较错误的认识，先看一下表结构： 1234567891011121314151617181920212223242526272829mysql&gt; SELECT e.employeeNumber,e.firstName ,e.extension, o.state ,o.officeCode FROM employees e ,offices o WHERE e.officeCode = o.officeCode;+----------------+-----------+-----------+------------+------------+| employeeNumber | firstName | extension | state | officeCode |+----------------+-----------+-----------+------------+------------+| 1002 | Diane | x5800 | CA | 1 || 1056 | Mary | x4611 | CA | 1 || 1076 | Jeff | x9273 | CA | 1 || 1143 | Anthony | x5428 | CA | 1 || 1165 | Leslie | x3291 | CA | 1 || 1166 | Leslie | x4065 | CA | 1 || 1188 | Julie | x2173 | MA | 2 || 1216 | Steve | x4334 | MA | 2 || 1286 | Foon Yue | x2248 | NY | 3 || 1323 | George | x4102 | NY | 3 || 1102 | Gerard | x5408 | NULL | 4 || 1337 | Loui | x6493 | NULL | 4 || 1370 | Gerard | x2028 | NULL | 4 || 1401 | Pamela | x2759 | NULL | 4 || 1702 | Martin | x2312 | NULL | 4 || 1621 | Mami | x101 | Chiyoda-Ku | 5 || 1625 | Yoshimi | x102 | Chiyoda-Ku | 5 || 1088 | William | x4871 | NULL | 6 || 1611 | Andy | x101 | NULL | 6 || 1612 | Peter | x102 | NULL | 6 || 1619 | Tom | x103 | NULL | 6 || 1501 | Larry | x2311 | NULL | 7 || 1504 | Barry | x102 | NULL | 7 |+----------------+-----------+-----------+------------+------------+23 rows in set (0.00 sec) 在这之前一直是想通过groupby达到如下的效果： 12345678mysql&gt; SELECT e.employeeNumber,e.firstName ,e.extension, o.state ,o.officeCode FROM employees e ,offices o WHERE e.officeCode = o.officeCode and o.officeCode=3;+----------------+-----------+-----------+-------+------------+| employeeNumber | firstName | extension | state | officeCode |+----------------+-----------+-----------+-------+------------+| 1286 | Foon Yue | x2248 | NY | 3 || 1323 | George | x4102 | NY | 3 |+----------------+-----------+-----------+-------+------------+2 rows in set (0.00 sec) 其实这种只能是通过一个内连接加一个条件过滤来进行实现，但是在之前的话是一直在记忆里面认为groupby可行，其实是不可行的 ，为什么这样说了，因为group by按照条件分组之后他每组只会有一条，那么在这里就出现了一个问题，既然groupby之后显示的是一条，那么这两行如何显示呢？所以这种通过groupby来显示的是不可行的。那么，groupby适合什么呢？groupby适合的是按照分组进行统计的，例如求最大薪水，平均薪水等，而不是列出某一个分组里面的所有人的。列出所有人这是where擅长的，如下数据表： 1234567891011121314151617mysql&gt; SELECT o.orderNUmber, o.orderDate, od.priceEach FROM orders o ,orderdetails od WHERE o.orderNumber =od.orderNumber limit 0,10;+-------------+------------+-----------+| orderNUmber | orderDate | priceEach |+-------------+------------+-----------+| 10100 | 2003-01-06 | 136.00 || 10100 | 2003-01-06 | 55.09 || 10100 | 2003-01-06 | 75.46 || 10100 | 2003-01-06 | 35.29 || 10101 | 2003-01-09 | 108.06 || 10101 | 2003-01-09 | 167.06 || 10101 | 2003-01-09 | 32.53 || 10101 | 2003-01-09 | 44.35 || 10102 | 2003-01-10 | 95.55 || 10102 | 2003-01-10 | 43.13 |+-------------+------------+-----------+10 rows in set (0.00 sec) 那么按照orderNumber 进行groupby便可以求出每一个订单的总价了，也说明了groupby只是适合于分组统计，而不是适用于分组展示： 12345678910111213141516SELECT o.orderNUmber, o.orderDate ,SUM(od.priceEach) FROM orders o ,orderdetails od WHERE o.orderNumber =od.orderNumber GROUP BY od.orderNumber limit 0,10;+-------------+------------+-------------------+| orderNUmber | orderDate | SUM(od.priceEach) |+-------------+------------+-------------------+| 10100 | 2003-01-06 | 301.84 || 10101 | 2003-01-09 | 352.00 || 10102 | 2003-01-10 | 138.68 || 10103 | 2003-01-29 | 1520.37 || 10104 | 2003-01-31 | 1251.89 || 10105 | 2003-02-11 | 1479.71 || 10106 | 2003-02-17 | 1427.28 || 10107 | 2003-02-24 | 793.21 || 10108 | 2003-03-03 | 1432.86 || 10109 | 2003-03-10 | 700.89 |+-------------+------------+-------------------+10 rows in set (0.01 sec) 这才是groupby的正确用法 having count函数：having count函数可以配合group by 对分组之后的数据进行筛选，这是where过滤所达不到的，就比如上表，需要过滤出订单价格大于1000的，那么在用where的时候则会显得很乏力： 12345678910111213141516mysql&gt; SELECT o.orderNUmber, o.orderDate ,SUM(od.priceEach) AS total FROM orders o ,orderdetails od WHERE o.orderNumber =od.orderNumber GROUP BY od.orderNumber HAVING TOTAL &gt; 1000 LIMIT 0,10;+-------------+------------+---------+| orderNUmber | orderDate | total |+-------------+------------+---------+| 10103 | 2003-01-29 | 1520.37 || 10104 | 2003-01-31 | 1251.89 || 10105 | 2003-02-11 | 1479.71 || 10106 | 2003-02-17 | 1427.28 || 10108 | 2003-03-03 | 1432.86 || 10110 | 2003-03-18 | 1338.47 || 10117 | 2003-04-16 | 1307.47 || 10119 | 2003-04-28 | 1081.44 || 10120 | 2003-04-29 | 1322.67 || 10122 | 2003-05-08 | 1598.27 |+-------------+------------+---------+10 rows in set (0.00 sec) 这就是having配合groupby的便捷性，那么where适合什么形式的过滤呢？我个人认为where比较适合于在groupby之前就进行数据的过滤，这样会比较好，而having count则适合于groupby之后的一个过滤。 OrderBy函数：orderby函数可以对结果集进行排序，他既可以和where搭配使用，又可以和groupby搭配使用，又可以和having一起搭配使用： 1234567891011121314151617mysql&gt; SELECT o.orderNUmber, o.orderDate ,SUM(od.priceEach) AS total FROM orders o ,orderdetails od WHERE o.orderNumber =od.orderNumber GROUP BY od.orderNumber HAVING TOTAL &gt; 1000 order by total LIMIT 0,10;+-------------+------------+---------+| orderNUmber | orderDate | total |+-------------+------------+---------+| 10341 | 2004-11-24 | 1003.19 || 10293 | 2004-09-09 | 1004.59 || 10246 | 2004-05-05 | 1006.78 || 10420 | 2005-05-29 | 1014.01 || 10311 | 2004-10-16 | 1033.82 || 10380 | 2005-02-16 | 1034.10 || 10412 | 2005-05-03 | 1034.15 || 10278 | 2004-08-06 | 1034.86 || 10361 | 2004-12-17 | 1052.87 || 10271 | 2004-07-20 | 1054.03 |+-------------+------------+---------+10 rows in set (0.01 sec) Mysql的总结：其实mysql的连接无非是左连接，右连接，内连接，全连接，也就left join ,right join ,inner join 和union等，其实在这里group by 和having count都可以对多列进行一个排序或者过滤，而不仅仅是一行","link":"/2018/03/29/mysql%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/"},{"title":"MySql的幻读(二)","text":"定义在InnoDB里面，是通过快照读来实现RC和PR隔离级别的区分，因为在RC隔离级别下，每一次的select都是一个快照读，所以是可以读取到已经提交的数据，从而导致幻读。所以在RC隔离级别下，快照读和当前读都是可以出现幻读。 但是在PR的隔离级别下，由于快照读仅仅只生成一次，所以在PR级别下的快照读是无法出现幻读的，但是当前读确实可以出现幻读。 查看Mysql官方对于幻读的定义。 The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. 也就是说一个事物同一条查询语句查询出来了两个不同的集合就可以称之为幻读。 ANSI SQL 隔离级别在ANSI SQL 隔离级别的定义中PR级别是可以出现幻读的。但是在InnoDB引擎里面自己通过GAP锁和Next-Key锁使得PR隔离级别下无法出现幻读。原因就在于InnoDB的GAP锁 GAP锁在InnoDB里面，GAP锁是用于防止幻读的一个手段，具体的操作是首先当我们新增一个数据的时候，InnoDB会在此时加一个GAP锁从而防止其他事务对该区间的一些数据操作，导致幻读的出现。 复现PR级别：123456789101112131415mysql&gt; select * from test_lock.test;+---------+-----------+----------------+------------+-------------+| user_id | user_name | user_password | is_deleted | phone |+---------+-----------+----------------+------------+-------------+| a | zhangsan | 123456 | 0 | 15112345678 || b | lisi | lisi123456 | 0 | 15112345678 || c | wangwu | wangwu123456 | 0 | 15112345678 || d | caocao | caocao123456 | 0 | 15112345678 || e | liubei | liubei123456 | 0 | 15112345678 || f | zhangfei | zhangfei123456 | 0 | 15112345678 || g | guanyu | guanyu123456 | 0 | 15112345678 || h | daqiao | daqiao123456 | 0 | 15112345678 || m | xiaoqiao | xiaoqiao123456 | 0 | 15112345678 |+---------+-----------+----------------+------------+-------------+9 rows in set (0.00 sec) 事物一： 12345678910111213mysql&gt; start transaction ;Query OK, 0 rows affected (0.00 sec)mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_lock.test where user_id &gt; 'g' and user_id &lt; 'm' for update ;+---------+-----------+---------------+------------+-------------+| user_id | user_name | user_password | is_deleted | phone |+---------+-----------+---------------+------------+-------------+| h | daqiao | daqiao123456 | 0 | 15112345678 |+---------+-----------+---------------+------------+-------------+1 row in set (0.00 sec) 事物二： 1234567mysql&gt; start transaction ;Query OK, 0 rows affected (0.00 sec)mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678'); 此时在PR隔离级别下会阻塞。 此时查看数据库中的锁： 123456789101112ysql&gt; select * from performance_schema.data_locks;+--------+----------------------------------------+-----------------------+-----------+----------+---------------+-------------+----------------+-------------------+------------+-----------------------+-----------+------------------------+-------------+-----------+| ENGINE | ENGINE_LOCK_ID | ENGINE_TRANSACTION_ID | THREAD_ID | EVENT_ID | OBJECT_SCHEMA | OBJECT_NAME | PARTITION_NAME | SUBPARTITION_NAME | INDEX_NAME | OBJECT_INSTANCE_BEGIN | LOCK_TYPE | LOCK_MODE | LOCK_STATUS | LOCK_DATA |+--------+----------------------------------------+-----------------------+-----------+----------+---------------+-------------+----------------+-------------------+------------+-----------------------+-----------+------------------------+-------------+-----------+| INNODB | 140169161014704:1068:140169076344152 | 235275 | 50 | 16 | test_lock | test | NULL | NULL | NULL | 140169076344152 | TABLE | IX | GRANTED | NULL || INNODB | 140169161014704:7:4:14:140169076341272 | 235275 | 50 | 16 | test_lock | test | NULL | NULL | PRIMARY | 140169076341272 | RECORD | X,GAP,INSERT_INTENTION | WAITING | 'm' || INNODB | 140169161013840:1068:140169076338200 | 235274 | 47 | 20 | test_lock | test | NULL | NULL | NULL | 140169076338200 | TABLE | IX | GRANTED | NULL || INNODB | 140169161013840:7:4:14:140169076335256 | 235274 | 47 | 20 | test_lock | test | NULL | NULL | PRIMARY | 140169076335256 | RECORD | X | GRANTED | 'm' || INNODB | 140169161013840:7:4:15:140169076335256 | 235274 | 47 | 20 | test_lock | test | NULL | NULL | PRIMARY | 140169076335256 | RECORD | X | GRANTED | 'h' |+--------+----------------------------------------+-----------------------+-----------+----------+---------------+-------------+----------------+-------------------+------------+-----------------------+-----------+------------------------+-------------+-----------+5 rows in set (0.00 sec) 可以看到在m列由于GAP锁和INSERT_INTENTION互相冲突，导致事物二无法进行插入 事物一由于进行for update查询，所以会对区间加一个GAP锁事物二由于是新增，所以会加一个插入意向锁由于GAP锁阻塞住了插入意向锁，导致事物二无法进行插入 此时事物二也会在这里阻塞住，而在RC隔离级别下，事物二是不会等待的。但是如果仔细一点，其实这里还可以发现另一个锁，就是插入意向锁INSERT_INTENTION LOCK INSERT_INTENTION_LOCKMysql官方文档中将INSERT_INTENTION定义为一个GAP锁，但是它的意义和真正的GAP锁之间是有天大的差别的，INSERT_INTENTION_LOCK并不会阻塞GAP锁，但相反GAP会阻塞INSERT_INTENTION_LOCK，并且该锁的锁定范围是插入行一直到下一个索引，这一整个区间如下例子：事物一： 12345678mysql&gt; start transaction ;Query OK, 0 rows affected (0.00 sec)mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into test_lock.`test` values('i','daqiao','daqiao123456',0,'15112345678');Query OK, 1 row affected (0.00 sec) 事物二： 123456789mysql&gt; start transaction ;Query OK, 0 rows affected (0.00 sec)mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)mysql&gt; update test_lock.`test` set user_id='l' where user_id = 'k';Query OK, 0 rows affected (0.00 sec)Rows matched: 0 Changed: 0 Warnings: 0 此时查看事物中锁： 123456789mysql&gt; select * from performance_schema.data_locks;+--------+----------------------------------------+-----------------------+-----------+----------+---------------+-------------+----------------+-------------------+------------+-----------------------+-----------+-----------+-------------+-----------+| ENGINE | ENGINE_LOCK_ID | ENGINE_TRANSACTION_ID | THREAD_ID | EVENT_ID | OBJECT_SCHEMA | OBJECT_NAME | PARTITION_NAME | SUBPARTITION_NAME | INDEX_NAME | OBJECT_INSTANCE_BEGIN | LOCK_TYPE | LOCK_MODE | LOCK_STATUS | LOCK_DATA |+--------+----------------------------------------+-----------------------+-----------+----------+---------------+-------------+----------------+-------------------+------------+-----------------------+-----------+-----------+-------------+-----------+| INNODB | 140169161014704:1068:140169076344152 | 235293 | 50 | 48 | test_lock | test | NULL | NULL | NULL | 140169076344152 | TABLE | IX | GRANTED | NULL || INNODB | 140169161014704:7:4:14:140169076341272 | 235293 | 50 | 48 | test_lock | test | NULL | NULL | PRIMARY | 140169076341272 | RECORD | X,GAP | GRANTED | 'm' || INNODB | 140169161013840:1068:140169076338200 | 235288 | 47 | 32 | test_lock | test | NULL | NULL | NULL | 140169076338200 | TABLE | IX | GRANTED | NULL |+--------+----------------------------------------+-----------------------+-----------+----------+---------------+-------------+----------------+-------------------+------------+-----------------------+-----------+-----------+-------------+-----------+3 rows in set (0.00 sec) 可以看到事物二确实已经执行成功了，而且事物一的插入意向锁并未阻塞事物二的插入语句。 总结在Mysql的InnoDB中，幻读的解决方案是采用了一个间隙锁GAP锁来实现的","link":"/2019/08/27/mysql%E7%9A%84%E5%B9%BB%E8%AF%BB-%E4%BA%8C/"},{"title":"netty的一些概念","text":"这里面的部分概念参考了《Apress JavaI.O . NIO and NIO2》 BufferNIO的一些操作基础就是Buffer Channels它的具体作用是帮助 DMA 快速的从硬盘上获取和写入数据 Selector选择器，目的是在异步模式中可以通过一个线程来实现哪些IO操作已经完成了。","link":"/2018/05/31/netty%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/"},{"title":"nginx出现403的总结","text":"今天在使用nginx的时候访问首页总是提示403forbidden，经过各种查询之后，总结为如下几种原因： 访问的资源权限不足，最好将nginx访问的资源权限修改为755或者777 SeLinux的设置为true，需要将其修改为false 如果出现首页的访问资源不是指定的目录的话，可以在/etc/nginx/nginx.conf中添加一条语句root XXX，XXX代表的是资源目录。其他的暂时没发现什么问题","link":"/2018/05/14/nginx%E5%87%BA%E7%8E%B0403%E7%9A%84%E6%80%BB%E7%BB%93/"},{"title":"pandas的transform和apply","text":"q区别transform是Pandas里面Groupby的一个方法，主要作用是对groupby之后的dataframe进行处理，接收的参数一个是一个Series 123456789101112131415In [18]: df = pd.DataFrame({'B': ['one', 'one', 'two', 'three','two', 'two', 'one', 'three'], ...: 'C': [1,2,3,4,5,6,7,8], 'D': [11,12,13,14,15,16,17,18]})In [19]: dfOut[19]: B C D0 one 1 111 one 2 122 two 3 133 three 4 144 two 5 155 two 6 166 one 7 177 three 8 18 那么需要对其groupby之后求C的平均值怎么办 1234567891011121314151617181920n [23]: df.groupby('B').transform(lambda x : x.mean())Out[23]: C D0 3 131 3 132 4 143 6 164 4 145 4 146 3 137 6 16In [24]: df.groupby('B').apply(lambda x : x.mean())Out[24]: C DBone 3.333333 13.333333three 6.000000 16.000000two 4.666667 14.666667 同样是一个lambda表达式，那么为什么会出现两种不同的结果： 123456789101112131415161718192021222324252627282930313233343536373839In [40]: df1 = pd.DataFrame({'B': ['one', 'two', 'three','four'], ...: 'C': [1,2,1,1], 'D': [11,12,13,14]})In [41]: df1Out[41]: B C D0 one 1 111 two 2 122 three 1 133 four 1 14In [42]: def fun1(x): ...: print x ...: In [43]: print df1.groupby('C').transform(fun1)0 one2 three3 fourName: B, dtype: object0 112 133 14Name: D, dtype: object B D0 one 112 three 133 four 141 twoName: B, dtype: object1 12Name: D, dtype: object B D0 one 111 two 122 three 133 four 14 从上面可以发现trandform每次传入的是一个series，即按照C分组之后，首先传入的是‘B’列，然后是‘D’列，然后是’B’’D’列，那么对于apply来说他传入的是什么呢： 123456789101112131415In [45]: df1.groupby('C').apply(fun1) B C D0 one 1 112 three 1 133 four 1 14 B C D0 one 1 112 three 1 133 four 1 14 B C D1 two 2 12Out[45]: Empty DataFrameColumns: []Index: [] 打印结果如上，说明apply和transform在接受参数上的差异(一个接受的是DataFrame，一个是Series)，上图中的打印第一个和第二个重复问题待会再解释，那么假设有如下的lambda表达式： lambda x : return [‘C’]-x[‘D’]结果会怎样显示? 新增加一列： 12345678910111213141516In [49]: df1['E']=20In [50]: df1Out[50]: B C D E0 one 1 11 201 two 2 12 202 three 1 13 203 four 1 14 20In [51]: df1.groupby('C').transform(lambda x :(x['D']-x['E']))Out[51]: B D E0 one 11 201 two 12 202 three 13 203 four 14 20 那么出现这样的情况的原因是什么呢？如前面所示的，transform传入的是一个series","link":"/2017/12/10/pandas%E7%9A%84transform%E5%92%8Capply/"},{"title":"pandas的操作使用","text":"pandas读取csv文件在pandas读里面包含了几个函数分别用来读取csv或者excel文件：read_csv方法用来读取一个csv文件一般常用参数是path:用于指定一个文件及其目录sep表示读取该csv文件的时候是以什么制表符读取的，一般是’,’usecols表示需要读取csv的多少列，这是一个绝对索引，0代表的是第一列，参数为一个list Pandas的结构：pandas经常使用的结构一般是dataframe和series,DataFrame类似于二维数组，或者sql里面的一张表，若在创建的时候为指定索引的话则默认从0开始一次递增为索引： 123456789101112import pandas as pdIn [15]: df_2=pd.DataFrame(data=['a','b','c'])In [16]: df_2Out[16]: 00 a1 b2 c用上述方式创建的时候是不需要指定index的，但是用标量创建的时候是需要指定index的：pd.DataFrame({'a':'A','b':'B'})ValueError: If using all scalar values, you must pass an index 基本实例：读取北京2017年07月01日的空气质量情况： 12345678910111213 df_20170101 =pd.read_csv(path,sep=',',decimal=',',usecols=[0,1,2,3,4,5,6,7,8,9]) #path是我的文件地址 date hour type dongfour 天坛 官园 万寿西宫 奥体中心 农展馆 万柳0 20170101 22 PM2.5 469 357 476 416 453 398 4681 20170101 22 PM2.5_24h NaN NaN NaN NaN NaN NaN NaN2 20170101 22 PM10 594 449 548 474 467 469 5183 20170101 22 PM10_24h NaN NaN NaN NaN NaN NaN NaN4 20170101 22 AQI 494 405 484 444 469 432 4795 20170101 23 PM2.5 470 351 500 403 417 392 4596 20170101 23 PM2.5_24h NaN NaN NaN NaN NaN NaN NaN7 20170101 23 PM10 558 467 583 469 443 480 5298 20170101 23 PM10_24h NaN NaN NaN NaN NaN NaN NaN9 20170101 23 AQI 480 401 500 435 445 428 473 假设需要获取天坛的22点到23点情况，那么只需要使用df_20170101[‘天坛’]： 123456789100 3571 NaN2 4493 NaN4 4055 3516 NaN7 4678 NaN9 401 这就是pandas的另一种结构：series，类似于一个竖着地数组，那么假设现在需要获取天坛的22点的pm2.5 12345678一种方式是知道索引:In [6]: df_20170101.loc[0]['天坛']Out[6]: 357.0一种则是获取22点并且type为PM2.5的时候的天坛In [7]: df_20170101.loc[(df_20170101['hour']==22) &amp; (df_20170101['type']=='PM2.5'),'天坛']Out[7]: 0 357Name: 天坛, dtype: float64 那么这个数据中有很多的NaN值，其中PM2.5_24h的值发现都为NaN，则这个值其实是可以被剔除的，那么常用的做法就是： 12345678910111213141516171819202122232425In [8]: df_20170101.loc[~df_20170101['dongfour'].isnull()]Out[8]: date hour type dongfour 天坛 官园 万寿西宫 奥体中心 农展馆 万柳0 20170101 22 PM2.5 469 357 476 416 453 398 4682 20170101 22 PM10 594 449 548 474 467 469 5184 20170101 22 AQI 494 405 484 444 469 432 4795 20170101 23 PM2.5 470 351 500 403 417 392 4597 20170101 23 PM10 558 467 583 469 443 480 5299 20170101 23 AQI 480 401 500 435 445 428 473至于为什么选择dongfour是因为选择其他的也是一样的，都可以，但是如果需要将NaN的值替换成0的话只需要执行In [9]: df_20170101.fillna(0)Out[9]: date hour type dongfour 天坛 官园 万寿西宫 奥体中心 农展馆 万柳0 20170101 22 PM2.5 469 357 476 416 453 398 4681 20170101 22 PM2.5_24h 0 0 0 0 0 0 02 20170101 22 PM10 594 449 548 474 467 469 5183 20170101 22 PM10_24h 0 0 0 0 0 0 04 20170101 22 AQI 494 405 484 444 469 432 4795 20170101 23 PM2.5 470 351 500 403 417 392 4596 20170101 23 PM2.5_24h 0 0 0 0 0 0 07 20170101 23 PM10 558 467 583 469 443 480 5298 20170101 23 PM10_24h 0 0 0 0 0 0 09 20170101 23 AQI 480 401 500 435 445 428 473就会将NaN值全部转为0 Group实例：若现在需要统计dongfour的22到23点的PM2.5平均值，那么需要对type进行groupby: 123456789101112In [29]: df_20170101['dongfourmean']=df_20170101.groupby('type')['dongfour'].apply(lambda x : pd.Series(np.mean(x),index=x.index))In [30]: df_20170101Out[30]: date hour type dongfour 天坛 官园 奥体中心 农展馆 万柳 dongfourmean0 20170101 22 PM2.5 469 357 476 453 398 468 469.52 20170101 22 PM10 594 449 548 467 469 518 576.04 20170101 22 AQI 494 405 484 469 432 479 487.05 20170101 23 PM2.5 470 351 500 417 392 459 469.57 20170101 23 PM10 558 467 583 443 480 529 576.09 20170101 23 AQI 480 401 500 445 428 473 487.0因为df_20170101['dongfourmean']是一个Series所以返回的时候也是需要Series 若现在需要求的是dongfour到万柳的22点的PM2.5平均值则是可以先对df_20170101进行一个转置操作： 12345678910111213141516In [35]: df_TOut[35]: 0 2 4 5 7 9date 20170101 20170101 20170101 20170101 20170101 20170101hour 22 22 22 23 23 23type PM2.5 PM10 AQI PM2.5 PM10 AQIdongfour 469 594 494 470 558 480天坛 357 449 405 351 467 401官园 476 548 484 500 583 500奥体中心 453 467 469 417 443 445农展馆 398 469 432 392 480 428万柳 468 518 479 459 529 473dongfourmean 469.5 576 487 469.5 576 487In [38]: np.mean(df_T.ix[3:8][0])Out[38]: 430.60000000000002 注意细节：pandas进行group之后是不可以在用loc进行操作的，即在pandas里面GroupBy objects进行一些操作的话可以采用apply，而不能使用loc若非要使用loc则在使用groups之后转化成一个dataframe之后再进行一些dataframe的操作","link":"/2017/12/03/pandas%E7%9A%84%E6%93%8D%E4%BD%9C%E4%BD%BF%E7%94%A8/"},{"title":"shiros自定义异常的跳转","text":"在shiro中经常需要对特定的异常及进行特殊的处理。一般来讲在shiro中配置的话是通过如下代码： 1234567891011121314151617181920&lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;property name=&quot;loginUrl&quot; value=&quot;/login.jsp&quot; /&gt; &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/unauthorized.jsp&quot; /&gt; &lt;!--&lt;property name=&quot;filters&quot;&gt;--&gt; &lt;!--&lt;map&gt;--&gt; &lt;!--&lt;entry key=&quot;roles&quot; value-ref=&quot;roles&quot; /&gt;--&gt; &lt;!--&lt;entry key=&quot;perms&quot; value-ref=&quot;perms&quot; /&gt;--&gt; &lt;!--&lt;/map&gt;--&gt; &lt;!--&lt;/property&gt;--&gt; &lt;!-- 过滤链定义 --&gt; &lt;property name=&quot;filterChainDefinitions&quot;&gt; &lt;value&gt; &lt;!--/role/** authc--&gt; &lt;!--/login/main authc--&gt; &lt;!--api/logincheck authc--&gt; &lt;!--message/detail role[EMPLOYEE]--&gt; &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; 但是在这里配置的话会有一个问题，就是通过注解@RequireRoles()这个来配置的权限会导致这里的配置一直无法生效，也就是当无权限的人访问需要特定权限的URL的时候就会直接在页面上显示500，而不是返回我这个指定的URL，后来查询得知有如下几个需要注意的： 使用配置的方式配置权限的话该xml可以生效 使用注解配置权限的话但是使用xml方式配置错误跳转页面不会跳转至指定URL 所以在项目中使用注解配置权限的话需要在xml配置文件中配置一个异常和其处理的相关url，如下： 12345678910&lt;!-- 定义需要特殊处理的异常，用类名或完全路径名作为key，异常页名作为值 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.handler.SimpleMappingExceptionResolver&quot;&gt; &lt;property name=&quot;exceptionMappings&quot;&gt; &lt;props&gt; &lt;prop key=&quot;org.apache.shiro.authz.NestedServletException&quot;&gt;redirect:/unauthorized&lt;/prop&gt; &lt;prop key=&quot;org.apache.shiro.authz.UnauthenticatedException&quot;&gt;redirect:/unauthorized&lt;/prop&gt; &lt;prop key=&quot;org.apache.shiro.authz.AuthorizationException&quot;&gt;redirect:/unauthorized&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt;","link":"/2018/04/14/shiros%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%82%E5%B8%B8%E7%9A%84%E8%B7%B3%E8%BD%AC/"},{"title":"Spring Cloud使用ELK日志记录(四)","text":"在ELK的使用过程中，遇到了一点困难， 所以正好写这一篇文章来记录下： 问题一：logstash连接elasticsearch报错这个问题在这边是由于logstash连接报错，查看了下错误日志发现是由于解析模板出错，后来改了下之后便可以连接了 问题二：kibama可以连接上elasticsearch但是一直读取不了数据&quot;Couldn't find any Elasticsearch dataYou'll need to index some data into Elasticsearch before you can create an index pattern &quot;这个问题出现的原因比较复杂，需要仔细排除，不过当出现这个问题的时候可以先看下Elasticsearch 是否含有数据，若是没有数据则添加数据即可。http://localhost:9200/_cat/indices。当这一步没问题了以后，若发现kibana还是无法显示数据则可以在kibana的DevTool里面直接模拟数据。如下： 1234POST test/doc{ &quot;filed&quot;: &quot;myData&quot;} 此时刷新http://localhost:9200/_cat/indices,若还是未出现数据，则表示可能是logstash，elasticsearch和kibana连接的过程出错了，此时就需要检查log日志。下面进入连接部分 连接:首先在微服务中引入logstash所需要的jar，然后编写logstash.xml。具体如下： 123456&lt;!-- https://mvnrepository.com/artifact/net.logstash.logback/logstash-logback-encoder --&gt; &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; 编写logstash.xml。在这里直接将springcloud与Docker微服务实战的logstash.xml拿来使用了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot; /&gt; ​ &lt;springProperty scope=&quot;context&quot; name=&quot;springAppName&quot; source=&quot;spring.application.name&quot; /&gt; &lt;!-- Example for logging into the build folder of your project --&gt; &lt;property name=&quot;LOG_FILE&quot; value=&quot;${BUILD_FOLDER:-build}/${springAppName}&quot; /&gt; ​ &lt;property name=&quot;CONSOLE_LOG_PATTERN&quot; value=&quot;%clr(%d{yyyy-MM-dd HH:mm:ss}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr([${springAppName:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-B3-ParentSpanId:-},%X{X-Span-Export:-}]){yellow} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}&quot; /&gt; &lt;!-- Appender to log to console --&gt; &lt;appender name=&quot;console&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;!-- Minimum logging level to be presented in the console logs --&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- Appender to log to file --&gt; &lt;appender name=&quot;flatfile&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;file&gt;${LOG_FILE}&lt;/file&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;${LOG_FILE}.%d{yyyy-MM-dd}.gz&lt;/fileNamePattern&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; ​ &lt;!-- Appender to log to file in a JSON format --&gt; &lt;appender name=&quot;logstash&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;file&gt;${LOG_FILE}.json&lt;/file&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;${LOG_FILE}.json.%d{yyyy-MM-dd}.gz&lt;/fileNamePattern&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class=&quot;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder&quot;&gt; &lt;providers&gt; &lt;timestamp&gt; &lt;timeZone&gt;UTC&lt;/timeZone&gt; &lt;/timestamp&gt; &lt;pattern&gt; &lt;pattern&gt; { &quot;severity&quot;: &quot;%level&quot;, &quot;service&quot;: &quot;${springAppName:-}&quot;, &quot;trace&quot;: &quot;%X{X-B3-TraceId:-}&quot;, &quot;span&quot;: &quot;%X{X-B3-SpanId:-}&quot;, &quot;parent&quot;: &quot;%X{X-B3-ParentSpanId:-}&quot;, &quot;exportable&quot;: &quot;%X{X-Span-Export:-}&quot;, &quot;pid&quot;: &quot;${PID:-}&quot;, &quot;thread&quot;: &quot;%thread&quot;, &quot;class&quot;: &quot;%logger{40}&quot;, &quot;rest&quot;: &quot;%message&quot; } &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;/encoder&gt; &lt;/appender&gt; ​ &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;console&quot; /&gt; &lt;appender-ref ref=&quot;logstash&quot; /&gt; &lt;!--&lt;appender-ref ref=&quot;flatfile&quot;/&gt; --&gt; &lt;/root&gt;&lt;/configuration&gt; 编写完成这个微服务之后，需要去设置ELK相关的config elasticsearch在使用elasticsearch的时候并不需要过多的设置。甚至可以不用进行设置，所以这里可以直接忽略掉 logstash在使用Logstash的时候需要设置一个config文件，该文件指定了读取日志的路径，以及日志的过滤方式，和日志的输出方式所以在这里需要自己配置下：如下: 12345678910111213141516171819input { file { codec =&gt; json path =&gt; &quot;C:\\ELK\\log\\build\\microservice-provider-user.json&quot; }}filter { grok { match =&gt; { &quot;message&quot; =&gt; &quot;%{TIMESTAMP_ISO8601:timestamp}\\s+%{LOGLEVEL: severity}\\s+\\[%{DATA:service},%{DATA:trace},%{DATA:\\span},%{DATA:exportable}\\]\\s+%{DATA:pid}---\\s+\\[%{DATA:thread}\\]\\s+%{DATA:class}\\s+:\\ s+%{GREEDYDATA:rest}&quot;} }}output { elasticsearch { hosts =&gt; &quot;localhost:9200&quot; } } input-&gt;path指定的是读取日志的路径，而output-&gt;elasticsearch则是代表将日志输出到elasticsearch kibanakibana的配置其实不需要很多，在这里也仅仅配置了连接elasticsearch的相关设置 123elasticsearch.url: &quot;http://localhost:9200&quot;elasticsearch.username: &quot;elastic&quot;elasticsearch.password: &quot;changeme&quot; 最后依次启动elasticsearch，logstash，kibana。最后将包含logstash相关的微服务打包成jar，然后开启注册中心，最后运行该项目，随便访问一个连接，应该是可以在elasticsearch看到刚刚的数据 总结 elasticsearch获取不到数据 此时应该检查logstash的日志观察是否模板出错，另外需要注意的是在匹配的时候不可以使用*.json，一定是需要文件名称加上json，如上配置，否则会导致无法是被 配置好了还是无法获取数据 可以在kibana的控制台手动添加数据，然后访问http://localhost:9200/_cat/indices?v，如果还是未出现数据，则表示 kibana 和 elasticsearch 的连接除了问题，需要排查。目前提供了两种思路，第一种是在kibana的配置中手动配置 elasticsearch 的地址和用户名以及密码。第二种则是在 kibana 的控制台手动添加数据: 如果kibana连接正常则会在http://localhost:9200/_cat/indices?v这里出现刚刚添加的数据。没出现的话就需要仔细检查了","link":"/2018/04/23/spring-cloud%E4%BD%BF%E7%94%A8ELK%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95-%E5%9B%9B/"},{"title":"SpringCloud初步配置之起步","text":"spring cloud是一个微服务治理框架，主要解决的是以前单应用过于臃肿的一些难点，在这里记录下初次使用springcloud的一些过程 创建一个springboot应用在这里使用的是IDEA的spring Initializr创建的，创建好了之后将application.properties改成application.yml,其项目结构图如下： 在这里需要注意的是由于在这里的Test文件中含有一个@RunWith注解，所以需要在pom.xml中引入springboot的test包，否则项目可能启动不起来 引入spring cloud包1234567891011121314151617181920212223242526&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.5.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR7&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这是官网的以一个配置，但是在引入本地的时候还是需要稍微做点修改。修改之后的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Camden.SR7&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 在这里添加了&gt;spring-cloud-starter-eureka-server,而且在这里引入的jar都不需要添加版本号。 修改配置文件123456789101112spring: application: name: stock-serviceserver: port: 8083eureka: client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://localhost:8084/eureka/ 最后便可以启动项目了。 结果运行出来的结果如下：","link":"/2018/04/18/spring-cloud%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3/"},{"title":"Springboot的maven间接依赖","text":"在项目中经常使用 maven 来管理项目，但是有时候对于 maven 的细节还是了解的不是很清楚，因此今天复习下。 maven项目首先开始建立一个最简单的 maven 项目，其配置如下图： 可以看到最上面一行是 xml 的文件描述符，然后再是 project，在这里引入 xsd 文件。 XSD(XML Schemas Definition)XML Schema，描述了 xml 文档的结构，用于判断其是否符合 xml 的格式要求 然后下面就是 groupId，通常是公司的域名，artifactId 通常指的是项目名称。 Springboot项目按照官方的指导，在项目中首先引用 spring-boot-starter-parent，修改后的 pom.xml 如下： 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;maven-test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;finalName&gt;maven-test&lt;/finalName&gt;&lt;/build&gt;&lt;/project&gt; 当准备在启动类上加 @SpringBootApplication 注解的时候，此时 IDEA 会提示找不到这个注解。这是正常的，因为 parent 只是把这个项目的配置和依赖信息统一化了，使得 子pom 就不用关心版本问题，例如在项目中引入spring-boot-starter-web，当配置了 parent 之后，只需要在 子pom 中如下配置： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 那么它的配置信息就会自动的从 parent 中读取。例如刚刚的spring-boot-starter-web信息，它的版本信息如下： tips:使用命令mvn -Dverbose dependency:tree就可以像这样打印 jar 的依赖 那么 springboot 又是怎样来自动识别版本号的呢，此时就就涉及到了spring-boot-dependencies spring-boot-dependenciesspring-boot-dependencies 是 spring-boot-starter-parent 的一个 parent，可以在 spring-boot-starter-parent 的 pom 文件中看到。打开 spring-boot-dependencies 文件，你会发现它里面几乎全部都是一些配置信息，而刚刚的spring-boot-dependencies 版本号就是来自于此。 到目前为止，可以基本理清 springboot 的依赖关系了。 打包在工程中，随便写一个 controller，然后执行mvn package，此时会在 target 目录下出现一个 jar 包，然后运行 jar 包，正常启动OK。 替换parent既然 spring-boot-starter-parent 是依赖于 spring-boot-dependencies的，那么可不可以直接将parent 设置为spring-boot-dependencies呢，修改 pom 文件如下: 然后执行mvn package，执行的时候是成功的，但是当你用 java -jar maven-test.jar 的时候，你会发现提示如下： 12target java -jar maven-test.jarmaven-test.jar中没有主清单属性 原因首先分析下两个的 maven log。 spring-boot-starter-parent作为parent spring-boot-dependencies作为parent 可以看到第二次的打包插件是 maven-jar-plugin，也就是说 springboot 的项目一些资源并没有打包进来，查看 spring-boot-maven-plugin 插件，发现它是来自于 spring-boot-starter-parent 里面的，但是在文章的开头部分，是已经手动的将其引入到了 pom 文件，那么修改 parent 以后未执行的话，最有可能就是版本号的缺失导致的，于是修改pom： 12345678 &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.11.RELEASE&lt;/version&gt; &lt;/plugin&gt;&lt;/plugins&gt;&lt;finalName&gt;maven-test&lt;/finalName&gt; 然后运行mvn package： 1234[INFO] --- spring-boot-maven-plugin:2.1.11.RELEASE:repackage (repackage) @ maven-test ---[INFO] Replacing main artifact with repackaged archive[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS 你会发现此时打包出来的 jar 文件已经可以运行了。 进阶那么假设项目已经有了自己的 parent，如果还想用 spring-boot-dependencies 来进行统一的一个全局版本控制，那么有如下的解决办法 在自己的parent中设置parent为 spring-boot-starter-parent，那么根据 maven 的继承属性，所有的 子pom 也就顺带继承了 spring-boot-starter-parent","link":"/2020/05/19/springboot-indirect-maven-rely/"},{"title":"springboot多数据源-sqlSessionFactory","text":"在SpringBoot中，动态的切换数据源的方式有两种，一种是通过AbstractRoutingDataSource来通过注解实现，另一种则是通过配置不同的SqlSessionFactory来读取不同文件夹的mapper，从而实现多数据源。代码如下： DataSourceOneConfig 12345678910111213141516171819202122232425262728293031323334@Configuration@MapperScan(basePackages = {&quot;xyz.somersames.dao.one&quot;} ,sqlSessionTemplateRef = &quot;dataSourceOneSqlSessionTemplate&quot;)public class DataSourceOneConfig { @Primary @Bean(name = &quot;dataSourceOneT&quot;) // 这里后面加一个T是防止Spring出现skiped mapperFactoryBean 错误，导致无法注入 @ConfigurationProperties(prefix = &quot;spring.datasource.one&quot;) public DataSource getDataSource(){ return new DruidDataSource(); } @Primary @Bean(name = &quot;dataSourceOneSqlSessionFactory&quot;) public SqlSessionFactory setSqlSessionFactory(@Qualifier(&quot;dataSourceOneT&quot;) DataSource dataSource) throws Exception { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:mapper/one/*.xml&quot;)); return bean.getObject(); } @Primary @Bean(name = &quot;dataSourceOneTransactionManager&quot;) public DataSourceTransactionManager setTransactionManager(@Qualifier(&quot;dataSourceOneT&quot;) DataSource dataSource) { return new DataSourceTransactionManager(dataSource); } @Primary @Bean(name = &quot;dataSourceOneSqlSessionTemplate&quot;) public SqlSessionTemplate setSqlSessionTemplate(@Qualifier(&quot;dataSourceOneSqlSessionFactory&quot;) SqlSessionFactory sqlSessionFactory) throws Exception { return new SqlSessionTemplate(sqlSessionFactory); }} DataSourceTwoConfig 123456789101112131415161718192021222324252627282930@Configuration@MapperScan(basePackages = {&quot;xyz.somersames.dao.two&quot;} ,sqlSessionTemplateRef = &quot;dataSourceTwoSqlSessionTemplate&quot;)public class DataSourceTwoConfig { @Bean(name = &quot;dataSourceTwoT&quot;) // 这里后面加一个T是防止Spring出现skiped mapperFactoryBean 错误，导致无法注入 @ConfigurationProperties(prefix = &quot;spring.datasource.two&quot;) public DataSource getDataSource(){ return new DruidDataSource(); } @Bean(name = &quot;dataSourceTwoSqlSessionFactory&quot;) public SqlSessionFactory setSqlSessionFactory(@Qualifier(&quot;dataSourceTwoT&quot;) DataSource dataSource) throws Exception { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:mapper/two/*.xml&quot;)); return bean.getObject(); } @Bean(name = &quot;dataSourceTwoTransactionManager&quot;) public DataSourceTransactionManager setTransactionManager(@Qualifier(&quot;dataSourceTwoT&quot;) DataSource dataSource) { return new DataSourceTransactionManager(dataSource); } @Bean(name = &quot;dataSourceTwoSqlSessionTemplate&quot;) public SqlSessionTemplate setSqlSessionTemplate(@Qualifier(&quot;dataSourceTwoSqlSessionFactory&quot;) SqlSessionFactory sqlSessionFactory) throws Exception { return new SqlSessionTemplate(sqlSessionFactory); }} 当添加了这两个配置以后，dataSourceOneT 会扫描 mapper/one 下面的 xml 文件，而 dataSourceTwoT 会扫描 mapper/two 下面的 xml 文件。然后添加配置文件 application.yml 1234567891011121314server: port: 8091spring: datasource: one: username: root password: 123456 url: jdbc:mysql://localhost:3306/test?useSSL=false&amp;serverTimezone=Asia/Shanghai type: com.alibaba.druid.pool.DruidDataSource two: username: root password: 123456 url: jdbc:mysql://localhost:3306/test_lock?useSSL=false&amp;serverTimezone=Asia/Shanghai type: com.alibaba.druid.pool.DruidDataSource 在这里顺带说一句，SqlSessionFactory 负责打开 SqlSession，一个SqlSession代表的就是一次回话，但是如果你在方法上加了一个 @Transactional 注解，那么这个方法里面的所有数据库操作都会认为是一个SqlSession","link":"/2020/03/19/springboot%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90-sqlSessionFactory/"},{"title":"springcloud的其他组件使用记录之Config(四)","text":"在使用Spring Config的时候遇到一些坑，在这里记录下，顺便梳理下这个使用。 Spring Cloud Config使用Spring Config来配合git做一个配置文件管理，需要一个Config的服务端和一个Config的客户端，服务端主要是和git仓库进行一个连接，而config的客户端是连接服务端来刷新配置服务的。在Spring Cloud Config里面客户端需要使用Spring4.0出现的一个注解@Value配合一起使用 Spring Config服务端需要引入一下几个文件： 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; 新建一个application.yml 123456789101112spring: application: name: microservice-server # name可以随便填写，代表这个服务的ServiceId cloud: config: server: git: uri: https://gitee.com/somersames/sprincloud-config.git # git的地址 username: yourname password: yourpasswordserver: port: 8099 # 服务开启的端口 新建一个启动类： 1234567@SpringBootApplication@EnableConfigServerpublic class ConfigApplication { public static void main(String[] args) { SpringApplication.run(ConfigApplication.class); }} @EnableConfigServer代表的是将这个微服务作为Config的服务器 配置文件随后在git服务器中新建几个文件，并且按照peoperties的格式输入内容，例如profile=ad如下： 123microservice-foo.propertiesmicroservice-foo-dev.propertiesmicroservice-foo-test.properties 分别代表的是默认配置和开发环境以及测试环境的配置文件。 开启服务端运行服务端，然后访问URL，其中URL的格式为localhost:port/默认的项目名称/分支.格式这个URL的格式有很多种格式，具体的可以百度之后再自己尝试 1{&quot;name&quot;:&quot;microservice-foo&quot;,&quot;profiles&quot;:[&quot;dev.json&quot;],&quot;label&quot;:&quot;master&quot;,&quot;version&quot;:&quot;7ac2a341dfe7959b809b7d5ec70b980970208b91&quot;,&quot;state&quot;:null,&quot;propertySources&quot;:[{&quot;name&quot;:&quot;https://gitee.com/somersames/sprincloud-config.git/microservice-foo.properties&quot;,&quot;source&quot;:{&quot;profile&quot;:&quot;default-1.0-changeewafasf&quot;}}]} 添加客户端客户端不负责直接和git进行通信，而是直接和Config的服务端进行通信获取最新的数据 新建一个工程并且添加一个配置文件bootstrap.yml，没错，是bootstrap.yml，然后再新建一个配置文件application.yml.在bootstrap.yml中添加内容： 12345678spring: application: name: microservice-foo # 这里的名称填写项目的名称，也就是在之前获取的json里面的那个name cloud: config: uri: http://localhost:8099/ # 填写Config服务端地址 profile: dev #项目环境 label: master #项目分支 在Application.yml里面添加内容： 12server: port: 8088 # 服务开启的端口，任意即可 新建一个Controller 123456789101112@RestController@RequestMapping(&quot;config&quot;)public class FeignController { @Value(&quot;${profile}&quot;) // 这里的profile不是随便取得，这里取得是上述josn字符串里面的propertySources 下的 source 里面的那个键，在这个例子里面就是profile, private String profile; @RequestMapping(value = &quot;/profile&quot;,method = RequestMethod.GET) public String hello(){ return this.profile; }} 其他无需改动，然后访问localhost:8099/config/profile，可以看到如下结果： 刷新有时候需要比如动态刷新git的最新配置的话，需要引入一个新的包： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;version&gt;1.2.7.RELEASE&lt;/version&gt; &lt;!--根据自己的版本自己选择合适的 --&gt; &lt;/dependency&gt; 然后在Controller里面添加注解@RefreshScope,但是需要注意的是高版本需要在bootstrap.yml里面添加一个配置**management:security:enabled: false**否则会导致修改之后请求refresh刷新不出来。 自己通过git修改那个文件之后继续如下操作： 然后POSTMAN发出一个POST请求， 如果在高版本里面不添加那个配置会导致刷新不出来，如果自己刷新不出来，请尝试添加那个配置,另外那个额外添加的配置需要按照格式自己调整下。刷新之后如下： 那个cvcvcv就是刚刚修改的内容 这个就差不多结束了","link":"/2018/04/22/springcloud%E7%9A%84%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"},{"title":"springcloud的服务注册与负载均衡","text":"在springcloud中，服务注册和负载均衡分别是eureka和ribbon。其中eureka是一个服务注册组件，ribbon则是一个负载均衡组件。如果开启了hystrix之后ribbon就默认已经开启了。 Eureka在使用Eureka的时候服务端开启之后直接在客户端的application.yml中加入如下代码即可: 1234eureka: client: serviceUrl: defaultZone: http://localhost:8083/eureka/ 其中defaultZone代表的是服务端的地址，开启服务之后在eureka的服务面板便会看到这个注册信息。如果需要在这个客户端中加入一些元数据的话可以加入如下配置： 1234instance: prefer-ip-address: true metadata-map: mydata: testydata 其中metedata-map下面就代表的是自己定义的元数据，这个会在其他客户端请求查看微服务信息的时候一同显示出来。 ribbon负载均衡组件，可以将请求均匀的分布在相同的微服务实例上，需要几个微服务都是同一个相同名称，端口可以不同。新建两个客户端,第一个客户端端口号是8085 123456789spring: application: name: eurekamovieeureka: client: serviceUrl: defaultZone: http://localhost:8083/eureka/server: port: 8085 第二个客户端端口是8087 123456789spring: application: name: eurekamovieeureka: client: serviceUrl: defaultZone: http://localhost:8083/eureka/server: port: 8087 这两个服务建立并开启之后，新建一个服务端，然后通过ribbon访问eurekamovie这个服务名称，这样请求就会均匀的分布在这两个服务上。 建立一个RestTemplate自动注解类 12345678@Bean@LoadBalancedpublic RestTemplate restTemplate() { return new RestTemplate();}@AutowiredLoadBalancerClient loadBalancerClient; 最后在Controller里面的方法里面通过调用该服务名称完成调用。 1234RestTemplate restTemplate = new RestTemplate();ServiceInstance instance = loadBalancerClient.choose(&quot;EUREKAMOVIE&quot;);URI uri = instance.getUri();User user =this.restTemplate().getForEntity(uri+&quot;aa&quot;,User.class).getBody(); 最后可以在另两个服务中可以看到请求按照一个一次的方式均匀的分布在这两个实例中。另外就是如果需要自己编写负载均衡的规则的话。可以参照如下代码： 1234567@Configurationpublic class RibbonConfig { @Bean public IRule ribbonule(){ return new RandomRule(); }} 自定义的规则只需要返回一个IRule规则即可。","link":"/2018/04/20/springcloud%E7%9A%84%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"title":"spring中使用MappingJackson2HttpMessageConverter遇到的一个坑","text":"今天遇到的一个问题就是在Spring中如果继承 WebMvcConfigurerAdapter 然后实现 configureMessageConverters 方法来实现一个 Json 的转换的时候，此时会出现一个情况就是： 如果在Controller里面的那个参数是String的话就会一直提示一个错误 org.springframework.http.converter.HttpMessageNotReadableException&quot;,&quot;message&quot;:&quot;JSON parse error: Can not deserialize instance of java.lang.String out of START_OBJECT token; nested exception is com.fasterxml.jackson.databind.JsonMappingException: Can not deserialize instance of java.lang.String out of START_OBJECT token\\n at [Source: java.io.PushbackInputStream@35b22c23; line: 1, column: 1]&quot;,&quot;path&quot;:&quot;/tetsjson&quot;}刚开始一直找不出原因，于是在 Spring 源码中 DEBUG :在如下两处打断点发现是可以获取请求体的： 123456789101112131415161718192021222324@Override public HttpInputMessage beforeBodyRead(HttpInputMessage request, MethodParameter parameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) throws IOException { for (RequestBodyAdvice advice : getMatchingAdvice(parameter, RequestBodyAdvice.class)) { if (advice.supports(parameter, targetType, converterType)) { request = advice.beforeBodyRead(request, parameter, targetType, converterType); } } return request; } @Override public Object afterBodyRead(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) { for (RequestBodyAdvice advice : getMatchingAdvice(parameter, RequestBodyAdvice.class)) { if (advice.supports(parameter, targetType, converterType)) { body = advice.afterBodyRead(body, inputMessage, parameter, targetType, converterType); } } return body; // 可以正常解析出请求的body } 但是一旦加上请求头Content-Type : application/json.在方法进入 beforeBodyRead 里面的 for循环 之后便跳转到了 org.springframework.web.servlet.mvc.method.annotationgetMatchingAdvice() 方法，然后继续走下去发现有一处提示如下： 123456789@Overridepublic boolean supports(MethodParameter methodParameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) { // ConverType :&quot;class org.springframework.http.converter.json.MappingJackson2HttpMessgaeConverter&quot; return (AbstractJackson2HttpMessageConverter.class.isAssignableFrom(converterType) &amp;&amp; methodParameter.getParameterAnnotation(JsonView.class) != null); // methodParamter: &quot;method 'XXX' 'Controller方法' paramter 0&quot;} 1234567891011public &lt;A extends Annotation&gt; A getParameterAnnotation(Class&lt;A&gt; annotationType) { Annotation[] anns = getParameterAnnotations(); for (Annotation ann : anns) { //annotationType: “interface conk fast erxml. jackson. annotation. JsonView” ann: “org.springframework.web.bind. annotation. RequestBody(requiredtrue) if (annotationType.isInstance(ann)) { return (A) ann; } } return null;} 也就是说在上面已经讲Json转成了 json.MappingJackson2HttpMessageConverter 然后在 org.springframework.http.converter.json.AbstractJackson2HttpMessageConverter的read()方法中： 1234567@Override public Object read(Type type, Class&lt;?&gt; contextClass, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException { JavaType javaType = getJavaType(type, contextClass); return readJavaType(javaType, inputMessage); //type :&quot;class java.lang.String&quot; contextClass :&quot;class com.XXX.controller.UserContoller&quot; } 后来就DEBUG不下去了。。。 原因出现该BUG是因为使用了 MappingJackson2HttpMessageConverter 之后会将Json解析成对象，是String肯定会报错。 解决办法：要么在 @RequestBody里面使用对象接收，要么修改 MappingJackson2HttpMessageConverter方法将Json转成对象","link":"/2018/04/27/spring%E4%B8%AD%E4%BD%BF%E7%94%A8MappingJackson2HttpMessageConverter%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91/"},{"title":"spring连接redis的一些做法以及mybatis的一些感想","text":"Redis启动首先开启redis服务，windows的redis下载在github windows的redis下载地址,然后解压出来最后开启那个redis-server。启动之后显示如下图： spring配置：配置在Spring中也可以通过配置文件redis.properties配置，但是由于在这个项目中的配置文件已经太多了，所以选择使用类的方式进行配置: 123456789101112131415161718192021@Configurationpublic class RedisConfig { @Bean JedisConnectionFactory jedisConnectionFactory() { JedisConnectionFactory connectionFactory = new JedisConnectionFactory(); connectionFactory.setHostName(&quot;127.0.0.1&quot;); connectionFactory.setPort(6379); return new JedisConnectionFactory(); } @Bean @Autowired public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) { RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(factory); template.setKeySerializer(new StringRedisSerializer()); return template; }} 最后在使用的时候自动装配就可以了。 12@Autowired private RedisTemplate redisTemplate; 单元测试编写单元测试： 1234567891011121314@ContextConfiguration(&quot;classpath:ApplicationContext.xml&quot;)@RunWith(SpringJUnit4ClassRunner.class)public class TestRedis extends AbstractJUnit4SpringContextTests { @Autowired private RedisTemplate redisTemplate; @Test public void redisTest() throws UnsupportedEncodingException { redisTemplate.opsForValue().set(&quot;zhangsan&quot;, &quot;book1&quot;); if (redisTemplate.hasKey(&quot;abc&quot;)){ System.out.println(&quot;abc已经存入redis&quot;); } }} 然后会发现数据已经添加到了这个redis服务器中了。 Mybatis的一些知识点：SqlSesssionFactoryBuilder:这是一个工厂，主要的作用是创建一些SqlSessioFactory，对于SqlSessionFactory来说，它的职责就是创建每一个SqlSession，然后这个SqlSession的作用则是连接数据库，类似于传统的JDBC的Connection， SqlSesssionFactoryBuilder这个类主要是创建SqlSessioFactory,在这个类里面可以看到许多方法返回的是一个SqlSessionFactory，其中有一个方法是解析配置文件的方法： 12345678910111213141516171819202122public SqlSessionFactory build(Reader reader, String environment, Properties properties) { SqlSessionFactory var5; try { XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); var5 = this.build(parser.parse()); } catch (Exception var14) { throw ExceptionFactory.wrapException(&quot;Error building SqlSession.&quot;, var14); } finally { ErrorContext.instance().reset(); try { reader.close(); } catch (IOException var13) { ; } } return var5; } 这里的第一个参数是一个流，这个流是将一个properties或者xml文件读取之后供mybatis使用的。而对于SqlSessionFactory来讲，这是一个接口，其主要的接口方法如下： 12345678910111213141516171819public interface SqlSessionFactory { SqlSession openSession(); SqlSession openSession(boolean var1); SqlSession openSession(Connection var1); SqlSession openSession(TransactionIsolationLevel var1); SqlSession openSession(ExecutorType var1); SqlSession openSession(ExecutorType var1, boolean var2); SqlSession openSession(ExecutorType var1, TransactionIsolationLevel var2); SqlSession openSession(ExecutorType var1, Connection var2); Configuration getConfiguration();} 也就是说这个Factory主要负责的是Session的开启来查询数据库 注意的坑：在mybatyis里面的话若使用对象传参的话需要注意#{XXX}=YYY，这里的YYY的话是需要和POJO相对应的，否则会提示 there is no getter for property named","link":"/2018/03/27/spring%E8%BF%9E%E6%8E%A5redis%E7%9A%84%E4%B8%80%E4%BA%9B%E5%81%9A%E6%B3%95%E4%BB%A5%E5%8F%8Amybatis%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/"},{"title":"Spring ContentNegotiation（内容协商）之使用篇（一）","text":"背景随着业务系统的成熟，如果你的项目正好是公司的中台战略之一，但是下游系统的接收方式不统一，这一种情况在一些老的公司系统架构总经常出现，如果下游系统不方便兼容，那么就需要中台系统对外提供各种不同格式返回报文 内容协商简单说就是服务提供方根据客户端所支持的格式来返回对应的报文，在 Spring 中，REST API 基本上都是以 json 格式进行返回，而如果需要一个接口即支持 json，又支持其他格式，开发和维护多套代码显然是不合理的，而 Spring 又恰好提供了该功能，那便是ContentNegotiation 在 Spring 中，决定一个数据是以 json、xml 还是 html 的方式返回有三种方式，分别如下： 1：favorPathExtension 后缀模式，例如：xxx.json，xxx.xml2：favorParameter format模式，例如：xxx?format=json,xxx?format=xml,3：通过请求的 Accept 来决定返回的值 在这三种模式中，前面两种模式都是关闭，如果需要打开，可以通过以下方式来开启1：重写 WebMvcConfigurer(Spring5.X以后推荐的实现类) 的 configureContentNegotiation 来设置为 true 即可2：设置 spring.mvc.contentnegotiation.favor-path-extension=true 或者 pring.mvc.contentnegotiation.favor-parameter=true tips:如果是使用 Spring2.X以上的版本，不要开启 @EnableWebMvc 注解，否则会导致你的配置无效，如果需要开启该注解，则只能使用方法一重写 WebMvcConfigurer 了并且还需要说明一点的是通过配置文件开启的话，需要设置 useSuffixPattern 为 true，重写 configureContentNegotiation 的已经默认为 true 了 三种模式1：favorPathExtension 后缀模式12345678server: port: 8081spring: mvc: contentnegotiation: favor-path-extension: true media-types: json: application/json favor-path-extension 表示是否开启后缀匹配，media-types 表示后缀以何种方式进行解析，在这里需要注意一下一定是需要有对应的 HttpMessageConvert 才能解析，否则是会提示 406 Could not find acceptable representation 在 Spring 中已经默认含有json解析的 HttpMessageConvert，所以是可以直接解析的，如果需要支持解析 xml，可以引入 xml 包 1234&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt; &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt;&lt;/dependency&gt; 当开启了后缀模式以后，返回的文本类型会根据你的入参做不同的处理，.json 会返回 json 格式的数据，.xml 会返回 xml 格式的数据，当然也可以自定义一个 HttpMessageConverter 来自定义的返回文本格式 123456789101112131415GET localhost:8081/controller/advice/decrypt.json{ &quot;name&quot;: &quot;a&quot;, &quot;age&quot;: 1, &quot;date&quot;: null}GET localhost:8081/controller/advice/decrypt.xml&lt;Advice&gt; &lt;name&gt;a&lt;/name&gt; &lt;age&gt;1&lt;/age&gt; &lt;date/&gt;&lt;/Advice&gt; 2：favorParameter这种模式下是通过在 url 中通过一个参数来区分如何解析的，spring中已经默认这个关键字是 format 修改配置文件如下： 1234567server: port: 8081spring: mvc: contentnegotiation: favor-parameter: true 123456789101112131415GET localhost:8081/controller/advice/decrypt?format=json{ &quot;name&quot;: &quot;a&quot;, &quot;age&quot;: 1, &quot;date&quot;: null}GET localhost:8081/controller/advice/decrypt?format=xml&lt;Advice&gt; &lt;name&gt;a&lt;/name&gt; &lt;age&gt;1&lt;/age&gt; &lt;date/&gt;&lt;/Advice&gt; 当然也可以自己修改 parameter 的关键字，只需要在配置文件中调整下即可 1parameter-name: meida_type 此时再次请求的时候 parameter 就需要调整为 meida_type，否则就会以默认的方式去解析返回的文本信息 Accept解析这种就是默认的一种解析方式，无需进行任何配置，Spring 就是默认以这种模式进行解析的GET请求 XML请求 总结本文只是简单的介绍了如何使用，后续会介绍原理篇","link":"/2021/04/07/talk-about-httpMessageConvert/"},{"title":"聊一聊 SpringBoot 中的一些被忽视的注解","text":"之前在老的项目中看到了一个比较有趣的现象 有一个需求是需要对返回的数据进行加解密的操作，部分老代码是直接硬编码在项目中，但是后来有人改了一版，称之为 1.0 版本 1.0 版本是通过切面配合注解进行处理，大致的处理流程是对返回的对象通过反射遍历字段，如果发现字段上有指定的注解，则进行加密操作，如果发现该字段是一个对象的话，则进行递归处理，直至结束 后来有一个需求是对返回的手机号、身份证信息只需要对中间的信息进行加密，两边不处理…其他的例如家庭地址、家庭成员全部加密为密文 拿到这个需求的时候，想改也挺简单，只需要增加一个新的注解，然后替换该切面扫描到的返回值进行替换注解就可以了 但是这样会导致切面里面的代码越来越臃肿，于是后来通过 RestControllerAdvice 进行了一个优化处理 今天就来聊一聊这两个被忽视的注解 @ControllerAdvice 通过上述的描述我们可以知道该注解是可以在多个 Controller 中共享一些操作，配合 ExceptionHandler、InitBinder 等，可以在请求数据进入 Controller 之前进行一个预处理，减少代码中的硬编码部分。 对于 ExceptionHandler 大家可能不会很陌生，一个通用的异常处理，如果项目不是纯 dubbo 对外提供接口的话，那么应该是会用到该注解的 ExceptionHandler这个注解可以定义一个全局的异常处理器，可以将指定的异常转换为约定的格式返回，例如： 1234@ExceptionHandler({BusinessException.class})public JsonResponseResult handleRuntimeException(final Exception re) { return JsonResponseResult.error(XXX);} 这样，一旦 Controller 里面抛出了 BusinessException，于是返回自动就变成了这种自定义的 XXX 不用每一次都在 Controller 中手动捕获异常然后转换成 code，从业务的角度来说只需要区分各种异常，然后统一地方进行收口处理，尽最大的努力避免对业务代码的入侵 123code:1-99：参数问题code:2-199：权限不足XXX InitBinder该注解可以在请求参数进入 Controller 之前进行预处理，但是这个不能作用于 @RequestBody（这个注解是通过RequestBodyAdvice来生效的，不是同一个流程），这样就可以做一些比较好玩的操作了 优势一：例如一个请求的 url 是 localhost:8081/controller/testAdvice?advice=1-2 在业务中如果要接收这个参数是需要将 advice 定义为 String，那么如果想直接用对象来接收，会直接抛出一个异常 1234@PostMapping(&quot;/testAdvice&quot;)public String testAdvice(@RequestParam (name = &quot;advice&quot;)Advice advice){ return advice.toString();} 如果非要用对象来接收，这个时候就可以通过 InitBinder 来实现了 GlobalAdvice 1234567891011@ControllerAdvicepublic class GlobalAdvice { Logger logger = LoggerFactory.getLogger(GlobalAdvice.class); @InitBinder public void registerProduct(WebDataBinder webDataBinder, String advice){ logger.info(&quot;origin:{}&quot;,advice); webDataBinder.registerCustomEditor(Advice.class,new ProductEditor()); }} ProductEditor 1234567891011121314public class ProductEditor extends PropertyEditorSupport { @Override public void setAsText(String text) throws IllegalArgumentException { if(StringUtils.isEmpty(text)){ return; } String[] strings = text.split(&quot;-&quot;); Advice advice = new Advice(); advice.setName(strings[0]); advice.setAge(Integer.parseInt(strings[1])); setValue(advice); }} 此时再次请求 localhost:8081/controller/testAdvice?advice=1-2 就会发现是正常展示了 优势二可以解析前端 form-data 提交过来信息，在对象处理之前进行格式化，避免出错 12345678910111213@PostMapping(&quot;/testAdvice2&quot;)public String testAdvice2(Advice advice){ return advice.toString();}@Overridepublic void setAsText(String text) throws IllegalArgumentException { if(StringUtils.isEmpty(text)){ return; } // 格式化，这里为了简单，直接new Date() setValue(new Date());} 此时 Date 就会在这里统一格式化，十分的方便 优势三配合 Validator 来使用 通过实现 Validator 这个类来实现一些复杂的检验规则，例如要求年龄大于1岁的，必须要有姓名，在这个场景下普通的校验规则就无法满足的，所以这个时候可以自定一个规则 1234567891011121314@ControllerAdvicepublic class GlobalAdvice { Logger logger = LoggerFactory.getLogger(GlobalAdvice.class); @InitBinder public void registerProduct(WebDataBinder webDataBinder, String advice){ logger.info(&quot;origin:{}&quot;,advice); webDataBinder.registerCustomEditor(Advice.class,new ProductEditor()); webDataBinder.registerCustomEditor(Date.class,new DateEditor()); webDataBinder.setValidator(new AdviceValidator()); }} 123456789101112131415public class AdviceValidator implements Validator { @Override public boolean supports(Class&lt;?&gt; clazz) { return Advice.class.equals(clazz); } @Override public void validate(Object target, Errors errors) { Advice advice = (Advice) target; if(advice.getAge() &gt;= 1 &amp;&amp; StringUtils.isEmpty(advice.getName())){ errors.rejectValue(&quot;name&quot;,&quot;advice.name&quot;,&quot;年龄大于1岁的，姓名不能为空&quot;); } }} 同时该注解也无需在实体类上面进行任何操作，很方便的进行扩展，通过配置中心，可以实现定制化的控制规则，方便运营及时的调整规则，实时生效 @RestControllerAdvice官网的介绍如下： convenience annotation that is itself annotated with @ControllerAdvice and @ResponseBody 表示这个注解包含了上面的 ControllerAdvice 以及 ResponseBody， 作用是可以对入参的 @RequestBody 和 @ResponseBody 进行处理，常见的如对返回数据加密等操作 例如刚才的 Advice 类，如果需要对其的 name 统计进行小写转大写（这里只是做展示，实际情况可能是对name进行加密操作），如果在业务代码中进行处理，那么会造成一种硬编码， ResponseBodyAdvice Allows customizing the response after the execution of an @ResponseBody or a esponseEntity controller method but before the body is written with an HttpMessageConverter 也就是说可以在 HttpMessageConverter 调用之前，对返回的对象进行处理，而 HttpMessageConverter 由于不在本文范畴，暂时忽略，但是需要记住这个类是将返回的对象处理成 json 的地方 1234567891011121314/** * @author somersames */@Target({ElementType.PARAMETER,ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface DecryptInfo { /** * 代表解密的类型 * @return */ String type() default &quot;idCard&quot;;} 1234567891011121314151617@RestControllerAdvicepublic class DecryptAdvice implements ResponseBodyAdvice { @Override public boolean supports(MethodParameter returnType, Class converterType) { return returnType.hasMethodAnnotation(DecryptInfo.class); } @Override public Object beforeBodyWrite(Object body, MethodParameter returnType, MediaType selectedContentType, Class selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) { if(body instanceof Advice){ Advice advice = (Advice) body; advice.setName(advice.getName().toUpperCase()); return advice; } return body; }} 当定义好上述两个类以后，则只需要在方法上加上该注解即可。 12345@PostMapping(&quot;/advice/decrypt&quot;)@DecryptInfopublic Advice testAdvice(@Validated @RequestBody Advice bodyAdvice){ return bodyAdvice;} 此时 name 就会被转为大写，如果需要对入参进行处理，实现 RequestBodyAdvice 即可，相同的道理 思考在业务代码中，如果需要对 controller 层返回的数据进行加解密操作，有两种选择，一种是切面配合反射来遍历对象的字段判断是否包含指定的注解，如果含有的话，则直接进行加解密操作 还有一种就是今天的 RestControllerAdvice 扫描指定的包然后进行操作 如果是通过切面来进行处理，那么每一个返回的对象都必须明确的指明加解密的类型以及字段，需要配合注解使用，但是如果用 RestControllerAdvice 是否会是一个更好的选择呢？ 显然，这两种方式个人认为切面比较不优雅，主要无法解耦，当需要扩展一个注解的功能时，会修改切面里面的代码，而且如果需要对返回的数据进行按照顺序处理，如果使用 RestControllerAdvice，那么直接使用 @Order 注解使用","link":"/2021/04/05/talk-about-springboot-anno/"},{"title":"Spring ContentNegotiation（内容协商）之原理篇（二）","text":"简介在了解这部分之前，你需要知道 Spring 都是通过 DispatcherServlet 来处理和分发请求的，如果不知道的话也不会影响到本文的阅读 在开启内容协商之后，URL 肯定是会变的，例如之前是 a/b，开启后则变成为 a/b.json 或者 a/b.xml 那么 Spring 首先第一步就需要解决如何将这个 url 映射到正确的 Controller 上的呢？ HandlerMapping在 Spring 5.X 中，目前只含有 5 种，分别如下如果你没有做任何操作，那么用于处理 Controller 的请求就是来自于 RequestMappingHandlerMapping，如果你想把/*/a，/a* 映射到指定的 Controller，那么可以了解下 SimpleUrlHandlerMapping，当然这些都是后话了，与本文无关 获取 HandlerMapping 最重要的一个原因是要拿到 HandlerExecutionChain，在 HandlerExecutionChain 里面有我们熟悉的拦截器以及处理请求的 Handle，获取 Handle 后通过 getHandlerAdapter 来获取最终的 HandlerAdapter，通过 HandlerAdapter.handle(HttpServletRequest request, HttpServletResponse response, Object handler) 来处理请求 具体的作用如图 HandlerExecutionChainHandlerExecutionChain 内部没有很多的属性，主要都是拦截器相关，既然拦截器执行是在获取到 HandlerExecutionChain 之后执行的，那么匹配 Url 肯定是在这之前，所以只需要看 getHandler 就可以了 getHandlergetHandler 是 HandlerMapping 接口的一个抽象方法，在 AbstractHandlerMapping 中被实现，主要功能是匹配该 Request 对应的 HandlerExecutionChain 这里 getHandlerInternal 返回的是一个 HandlerMethod，打印出来如下： 可以看到这里返回的其实就是对应的Controller 处理方法，那么 mapping 肯定就在这个方法内部，在这个方法里面有一个很重要的方法 lookupHandlerMethod，该方法就是最终进行匹配的地方 lookupHandlerMethod在进行下去会看到 AbstractHandlerMethodMapping 的 addMatchingMappings 方法，该方法就是会对所有该项目的所有的url进行依一一匹配最终会调用到 getMatchingCondition 方法，而我们仅仅需要对 getMatchingCondition 关注即可，因为前面的判断并不会跟内容协商有关 getMatchingCondition 最终会调用的 PatternsRequestCondition 的 getMatchingPattern 方法，这个方法也就是今天的核心了 12345678public PatternsRequestCondition getMatchingCondition(HttpServletRequest request) { if (this.patterns.isEmpty()) { return this; } String lookupPath = this.pathHelper.getLookupPathForRequest(request); List&lt;String&gt; matches = getMatchingPatterns(lookupPath); // 省略} matches 里面存储的事所有匹配到的 url，这个url 并不是原始的，而是匹配后的，例如 a/b/c.* 对于内容协商，则只需要关心 this.patternsCondition.getMatchingCondition(request) 即可，而该方法最终会进入 PatternsRequestCondition#getMatchingPattern 如果你开启的是后缀匹配模式，那么 this.useSuffixPatternMatch 就必须是true，这也是上面说的 tips，上图红框内代码会判断下，如果是后缀匹配，那么 url 里面肯定是会有一个 . 的，所以此时用 . 进行区分，也就是如果你的 url 是 a/b/c，请求的url 是 a/b/c.json，此时就会通过 a/b/c.* 和 a/b/c.json 进行匹配，可想而知，肯定可以匹配到的，所以此时就可以找到正确的处理方法了，也就是会返回 a/b/c.* 作为匹配到的 url 那么既然匹配到 url 是 a/b/c.*，那么在 controller 中的 url 又是如何映射过去的呢？ 答案是在这个 for 循环里面，在进行遍历的时候，那个 mapping 就是原始的 controller 中的 url 方法，所以 spring 才可以通过这个定位到是通过哪个方法来处理该请求 当获取到 HandlerMethod 之后，则 通过 getHandlerExecutionChain 来获取所有的拦截器，并且进行处理，如果拦截器返回 false，则直接 return，否则通过 HandlerAdapter 来进行处理请求 返回既然已经知道了调用的方法，那么最后就是通过 url 的后缀来匹配对应的 HandlerMethodReturnValueHandler HandlerMethodReturnValueHandler这是一个抽象接口，里面只有两个抽象放啊，分别如下 1234567public interface HandlerMethodReturnValueHandler { boolean supportsReturnType(MethodParameter returnType); void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception;} supportsReturnType 判断该 Handle 能否处理该请求，handleReturnValue 则是真正处理该请求返回值的方法 RequestResponseBodyMethodProcessor这个类是专门用来处理用 ResponseBody 注解修饰的方法，其 supportsReturnType 如下： 12345@Overridepublic boolean supportsReturnType(MethodParameter returnType) { return (AnnotatedElementUtils.hasAnnotation(returnType.getContainingClass(), ResponseBody.class) || returnType.hasMethodAnnotation(ResponseBody.class));} writeWithMessageConverters该方法位于 AbstractMessageConverterMethodProcessor，关键代码如图： getAcceptableMediaTypes 就是来判断该 url 适合用什么格式来解析的关键代码，具体代码如下： 1234private List&lt;MediaType&gt; getAcceptableMediaTypes(HttpServletRequest request) throws HttpMediaTypeNotAcceptableException { return this.contentNegotiationManager.resolveMediaTypes(new ServletWebRequest(request));} 继续往下跟，到了 getMediaTypeKey 也就最后不远了，getMediaTypeKey 是 PathExtensionContentNegotiationStrategy 下的一个方法，主要是获取 url 里面所支持的 MediaType， 通过 UriUtils.extractFileExtension(path) 是可以拿到 /a/b/c.json 后面的 json，具体代码如下： 123456789101112131415161718public static String extractFileExtension(String path) { int end = path.indexOf('?'); int fragmentIndex = path.indexOf('#'); if (fragmentIndex != -1 &amp;&amp; (end == -1 || fragmentIndex &lt; end)) { end = fragmentIndex; } if (end == -1) { end = path.length(); } int begin = path.lastIndexOf('/', end) + 1; int paramIndex = path.indexOf(';', begin); end = (paramIndex != -1 &amp;&amp; paramIndex &lt; end ? paramIndex : end); int extIndex = path.lastIndexOf('.', end); if (extIndex != -1 &amp;&amp; extIndex &gt; begin) { return path.substring(extIndex + 1, end); } return null;} favorParameter format模式这个模式在处理请求的部分大同小异，主要是在解析返回的 MediaTypeKey 上有区别直接解析指定的参数，拿到对应的格式，一切就都结束了","link":"/2021/04/10/talkabout-httpMessageConvert-2/"},{"title":"mysql中count(*)和count(1)的区别","text":"在进行旧的项目 review 的时候，我发现绝大多数的统计SQL都是基于 count(1) 来进行的，只有一少部分是基于 count(*)，那么这两种写法到底有什么区别。 mysql中，常用的存储引擎有myisam和innodb，但是由于myisam只支持表级别锁，而且还不支持事物，所以在mysql的5.5版本之后就将默认的存储引擎调整为innodb。 &nbsp;以下实验基于 Mysql8.0.1 来进行&nbsp; 首先这里准备了一个表. 在此之前先通过 show table status 查看表中的数据，如下： 1234567mysql&gt; select count(*) from t_book;+----------+| count(*) |+----------+| 54173 |+----------+1 row in set (0.00 sec) 可以看到此时查询出来的ROWS是53935，那么再查看下通过索引查询出来的数据行数有多少呢？ 可以看到show index from t_book 查询出来的数据如下： 其中有大概 238 条数据的统计差异，这其实是因为Mysql的show table status查询出来的只是一个估计值。 查看系统的show table status在mysql的系统库中可以看到 t_book 表的相关信息，可以使用如下方式：其中的n_rows就是之前查询出来的值，但是这一个值是一个粗略估计的值。而 clustered_index_size 和 sum_of_other_index_sizes 则分别表示的是聚簇索引和其它的索引所占有的页的数量 回到之前的主题上来，那么下面就来看下 count(*)和count(1) 的区别： 区别首先打开 Mysql 的 warnings，便于分析日志： 12mysql&gt; warnings;Show warnings enabled. 然后再分析下两个之前的差异： count(*)可以看到此时 count* 走的是一个二级索引，但是在 Note 里面可以看到最后 count(*) 还是被转换成了count(0)。 count(1)而 count(1) 可以发现也是走的一个二级索引 索引的选择在 Mysql 中，由于主键索引一般情况下是比二级索引大的，所以在Mysql中，如果有二级索引的话，那么Mysql一定会选择一个二级索引来作为 count 的字段。 但是当有多个二级索引的时候，Mysql又会如何选择索引呢？ 首先以 book_page 为例： 123mysql&gt; create index book_page on t_book(book_page);Query OK, 0 rows affected (0.12 sec)Records: 0 Duplicates: 0 Warnings: 0 然后通过 count 函数查询如下： 由于 count(1) 执行结果与这个类似，就不放截图了 这个时候可以看到 book_page 的索引 key_len 是5，那么如果第二个 二级索引 的 key_len 比这个短的话，那么 Mysql 会该如何选择呢？ 以 book_num 作为索引123mysql&gt; create index book_num on t_book(book_num);Query OK, 0 rows affected (0.12 sec)Records: 0 Duplicates: 0 Warnings: 0 此时查看 count(*)和 count(1)，可以发现 Mysql 选择的是 book_num 索引作为 count 的字段。 对比之前的索引的 key_len，你会发现 Mysql 在有多个二级索引的情况下，是会优先选择 key_len 较小的索引。 由于count(*)和 count(1)索引的选择是一样的，此时就不放 count(1) 截图 PS：为了验证索引的创建顺序对 Mysql count 的选择没有影响，因此在这之后，又测试了一遍，不过是先创建的 book_num，后创建的 book_page，此时发现 Mysql 还是选择 book_num 作为索引。 相同的Key_len&nbsp;如果此时两个二级索引的 key_len 相同，Mysql 又会怎样选择呢？此时修改 book_num ： 123mysql&gt; ALTER TABLE t_book MODIFY book_num int(11) NULL;Query OK, 0 rows affected, 1 warning (0.92 sec)Records: 0 Duplicates: 0 Warnings: 1 此时进行 explain： 再反复的测试之后，会发现 Mysql 在 相同的 key_len 下，会自动的选择最后一个创建的 最小 key_len 索引最为 count 索引。 索引的选择跟区分度有关系吗 &nbsp;将 book_page 的字段区分度弄的低一点。 123mysql&gt; update t_book set book_page = 10 limit 10000;Query OK, 9888 rows affected (0.10 sec)Rows matched: 10000 Changed: 9888 Warnings: 0 &nbsp;然后此时再查看 Mysql 是如何选择索引的。 此时还是选择的是 book_page 字段，如果再把 book_num 的区分度弄的更低呢？ 123mysql&gt; update t_book set book_num = 100 limit 30000;Query OK, 29970 rows affected (0.40 sec)Rows matched: 30000 Changed: 29970 Warnings: 0 最后对比会发现，执行的还是 book_page 作为索引，那么这是为什么呢？ optimizer_trace 分析首先打开optimizer_trace： 12mysql&gt; SET optimizer_trace=&quot;enabled=on&quot;;Query OK, 0 rows affected (0.00 sec) 然后执行 count(1) 语句。最后查看结果如下：只截取关键信息： 可以看到在使用 book_page 做索引的时候，Mysql认为其 cost 是 5808.1，那么对于 book_num 索引呢？ book_num1234567mysql&gt; select count(*) from t_book force index(book_num);+----------+| count(*) |+----------+| 54173 |+----------+1 row in set (0.01 sec) 执行完以后，查看 cost 如下： 可以看到他的 cost 是 59193，所以 Mysql 认为 book_page 更加划算，那么是不是因为 book_num 的区分度太低，导致 cost 变大了呢？，此时调整 book_page 字段的值，如下： 123mysql&gt; update t_book set book_page =1 limit 53000;Query OK, 52548 rows affected (0.68 sec)Rows matched: 53000 Changed: 52548 Warnings: 0 然后会发现，Mysql 还是会选择 book_page 作为索引来进行 count，于是后来测试了下创建索引的顺序，发现当 key_len 相同的时候，Mysql是会选择首次创建的索引来进行 count，除非有更小的 key_len 出现。 总结所以 Mysql 在选择 count 的时候，优先会选择 二级索引，当有多个 二级索引 的时候，会优先选择 key_len 小的，当有多个 key_len 相同的二级索引时，直接选择第一次创建该 key_len 的索引。","link":"/2020/06/05/the-difference-between-count-and-count-1/"},{"title":"vue里面冒号和非冒号区别","text":"vue里面冒号和非冒号的却别今天在使用Vue的时候，突然发现了一个问题，就是在后端传过来的值因为是一个boolean类型的，但是前端又需要进行展示，由于我们使用的是ElementUI的话，于是参照官网上就是这样写的： 1234&lt;el-select v-model=&quot;option_boolean&quot;&gt; &lt;el-option label=&quot;1&quot; value=&quot;true&quot;&gt;&lt;/el-option&gt; &lt;el-option label=&quot;2&quot; value=&quot;false&quot;&gt;&lt;/el-option&gt;&lt;/el-select&gt; 但是此时页面上展示并非是 1 和 2 ，而是 true 和 false。如下： 按照正常的逻辑，此时这个下拉框里面的值应该是1，而不是true。如果此时修改为如下写法： 1234&lt;el-select v-model=&quot;option_boolean&quot;&gt; &lt;el-option label=&quot;1&quot; :value=&quot;true&quot;&gt;&lt;/el-option&gt; &lt;el-option label=&quot;2&quot; :value=&quot;false&quot;&gt;&lt;/el-option&gt; &lt;/el-select&gt; 此时的页面就会显示正常了。 解释在 Vue 里面，冒号:代表的是一个双向绑定，其值要么是一个变量，要么是一个函数，而此 Demo 里面，第一个例子中，value仅仅是作为一个属性，所以它只能是接受字符串类型等 但是在第二个例子里面，由于使用了 Vue 的一个双向绑定模式，所以此时便可以正确的识别出 boolean 类型的值了","link":"/2019/06/27/vue%E9%87%8C%E9%9D%A2%E5%86%92%E5%8F%B7%E5%92%8C%E9%9D%9E%E5%86%92%E5%8F%B7%E5%8C%BA%E5%88%AB/"},{"title":"从业务的角度来说说Spring为什么不推荐字段注入","text":"在我们的日常开发中，会发现一些代码中充斥着 @Autowired，这种做法在我们的业务代码中非常的常见，但是如果仔细观察，你就会发现 IntelliJ 会出现一个 warning 不推荐使用字段进行注入？？？我用了这么长时间的注入方式，竟然被 Spring 不推荐了？ 然后我们的其他同学就放弃了 @Autowired 注解，转而使用 @Resource 注解，虽然说 IDEA 不再告警，但是从业务的角度上来说，还是不推荐这样处理。 看到这里，顺带提一句，Spring 有哪几种构造方式？这个是经典八股了，如果没有背熟的，各打一个大板，答案是： field setter constructor 既然 Field injection 已经被标记为不推荐，那么 setter 和 constructor 呢？ 可以看到非常的干净，没有一个告警，那么起码说明这种方式，Spring 没说不推荐，可以用，构造器注入也一样，在这里就不贴代码了，感兴趣可以自己去试试。 从官方文档中，可以看到，对于 setter 和 constructor 的注入方式，Spring 优先使用 constructor。 Spring5.3.23-core The Spring team generally advocates constructor injection, as it lets you implement application components as immutable objects and ensures that required dependencies are not null. 即通过构造器注入的 Bean，是可以确保一定不是为 null 的。 那么有同学可能会说，我一个类里面几十个 bean，那通过构造器的注入不会就导致几十个入参吗。Spring 也考虑过这个情况，而且给出的建议是让你重构这个类。 As a side note, a large number of constructor arguments is a bad code smell, implying that the class likely has too many responsibilities and should be refactored to better address proper separation of concerns. 正常的情况下，如果一个类依赖于太多的 bean，那么说明该类的职责太多，如果不重构，后续肯定会成为一个超级复杂的类，然后成为无人敢动的屎山。 下面从两个角度来说明下，为什么还是在代码中使用构造器注入 构造器注入的好处1）阻止业务类循环依赖虽然说通过字段注入的时候，Spring可以帮助我们解决循环依赖的问题，但是一旦业务中两个类产生了循环依赖，那就说明设计不合理，需要进行业务的拆分。 用构造器注入，如果产生了循环依赖，可以在启动的时候直接报错，让业务人员进行优化，从而避免坏味道的模块。 2）防止业务类大杂烩当习惯通过字段注入以后，很容易就形成一个超级复杂的单体类，在业务中也经常会发现一些偷懒的情况，类的职责远远大于命名。 例如一个类的名称是 OrderService，负责处理下单、取消订单、退货。 如果用的是字段注入，很容易就会把一些额外的功能引入进来，比如支付逻辑，保价逻辑等等，到最后就会越来越臃肿。 当然也不是说构造器注入可以避免这一种情况，但是最起码在加入另一个 Bean 的时候，可以考虑下是不是有必要放到这一个类。 3）可以写出优秀的单元测试代码在业务开发的过程中，一般核心业务都会对单元测试有要求，至少核心的方法必须要跑单元测试，整个项目的单元测试覆盖率必须要达到一定的指标。 单元测试的麻烦点在于数据的构造，如果被单测的类依赖于 DB 或者 NoSQL 中的数据，本地开发的时候还好，可以有 DEV 环境进行单测。 而如果测试环境，禁止访问容器外部的网络，比如我们的 UAT 环境，在跑单元测试的时候，是禁止访问 docker 容器外部的网络，以避免单元测试对于测试环境产生干扰。 这个时候，如果跑单元测试覆盖率就会失败。从而影响整体的代码覆盖率，间接的可能影响到你的工资。。 3.1）Mockito单元测试为了解决上述的问题，我们在单元测试的时候，是直接进行 Mock 返回结果进行处理，例如如下 Demo 这个类有两个方法，其中一个就是简单的进行加法计算，而另一个需要调用第三方服务进行支付，然后返回真实的支付价格。这个时候针对这个类写一个单元测试。 add 方法可以看到是没问题的，1 + 2 = 3，但是如果需要对 getPayFee 进行 Mock 测试，那么我们就需要 Mock 一个 payService。 这个类需要在不访问外部网络的情况下，返回我们所需要的数据。 果不其然，直接报错了，那么我们就需要 Mock 一个 payService。 这个时候，问题来了，因为 FeeService 是采用字段注入的，我们怎么才能将 PayService 注入进去呢？ 不优雅的方案 直接反射进行注入： 可以看到，我们在这边直接将 mock 出来的 payService 通过反射注入到 FeeService 以后，单测可以跑通了，但是非常的不优雅。 优雅的解决方法 构造器注入 可以看到单测已经没问题了。相比于字段注入，这个方式的优点是不需要额外的反射处理 总结如果是以前的旧项目，如果没必要可以不用进行重构，但是对于新的业务项目，如果有机会，可以尝试用构造器进行注入。 如果是核心项目或者对单元测试覆盖率有要求的话，建议直接要求必须使用构造器注入，方便单元测试的编写。","link":"/2022/10/22/why-spring-recommended-construction-inject/"},{"title":"线程的interrupt和stop区别，以及线程的中断机制","text":"interrupt在Java里面线程的中断是一个协作式的，也就是说线程会在自己合适的时候自己中断自己，一般来讲线程如果需要中断的话有如下两种方法： 捕获InterruptException 通过Thread的interrupted()或者isInterrupted()方法，但是需要注意的是interrupted会清除这个线程的状态 当一个线程调用另一个线程的interrupt的时候，另一个线程并不会马上结束，而是会设置一个中断的状态，如果一个线程处于阻塞的状态，那么此时该线程会马上抛出一个InterruptException，由上层的代码进行处理。若线程没有处于阻塞的话，此时线程还是会执行的。但是线程需要自己在合适的地方通过上述的两个方法来判断自己是否应该中断。如果自己 stopstop方法和interrupt有许多相似之处，具体就是在阻塞的时候都会抛出Interruptexception这个异常。但是在运行期间的话stop方法会直接强迫另一个线程终止，并且抛出一个ThreadDeath的Error，这个 中断机制：总结了下，有如下几种。当线程在阻塞的时候收到中断请求，但是线程会抛出一个Exception并且线程的中断状态会被清除掉。（常见的阻塞如sleep，wait，join）等 此时需要注意，如果无法继续向上抛出这个异常的话，那么就应该继续调用interrupt这个方法，因为每一个中断都应该被上层知道的。 在判断线程的中断状态的时候需要注意 12345678while(!Thread.currentThread().isInterrupted()){ try{ // IO操作或者其他的中断操作 }catch(InterruptException e){ break; } } 在这段代码中，IO操作一旦发生阻塞并且收到一个中断异常但是这个异常又没有进行处理，这个时候while会一直判断不到这个线程已经是需要被中断的了 总结关于线程中断就是捕获的异常一定需要抛出，不能抛出就需要设置中断中断状态","link":"/2018/05/22/x%E7%BA%BF%E7%A8%8B%E7%9A%84interrupt%E5%92%8Cstop%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%BB%A5%E5%8F%8A%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%B8%AD%E6%96%AD%E6%9C%BA%E5%88%B6/"},{"title":"一次SpringBoot初始化引起的思考","text":"在Spring中，经常会使用@Resource注解来自动装配一些Bean，但是在初始化的时候还是有一点小坑的，下面是一段代码，有三个类，分别是A，B，C。 类A： 1234567891011@Servicepublic class A { @Resource B b; public void someMethod(){ System.out.println(b.getC() == null); }} 类B： 12345678910111213@Servicepublic class B { @Resource C c ; public C cParam =getC(); public C getC(){ return c.getAnewC(); }} 类C： 1234567891011121314151617181920212223242526272829@Servicepublic class C { private int i; private String name ; public C getAnewC(){ C c =new C(); c.setI(1); c.setName(&quot;a&quot;); return c; } public int getI() { return i; } public void setI(int i) { this.i = i; } public String getName() { return name; } public void setName(String name) { this.name = name; }} 此时这三个类有一个地方会抛出一个空指针异常，如果你可以一眼看出来的话，不妨继续走下去看看是否正确。编写一个测试类，如下： 123456789101112131415@RunWith(SpringJUnit4ClassRunner.class)@SpringApplicationConfiguration(classes = DemoApplication.class)public class SpringTest { @Autowired A a ; @Test public void hello(){ a.someMethod(); }} 这样看下去，可能就看不出哪里有问题，但是运行之后会抛出一个NullException，错误日志如下: 123456789101112131415Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'b' defined in file [C:\\Users\\SZH\\IdeaProject\\firstcloud\\target\\classes\\szh\\demo\\test\\B.class]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [szh.demo.test.B]: Constructor threw exception; nested exception is java.lang.NullPointerException--------------------------------------------------Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [szh.demo.test.B]: Constructor threw exception; nested exception is java.lang.NullPointerException------------------------------------------------------Caused by: java.lang.NullPointerException at szh.demo.test.B.getC(B.java:21) at szh.demo.test.B.&lt;init&gt;(B.java:18) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:142) ... 55 more 此时的异常栈如下，可以看到在类B里面，第18行，也就是public C cParam =getC();抛出了一空指针异常，这个异常的原因就是在方法getC()里面，c是一个null，那么在这里可能就会有一个疑问了，Spring不是会自动装配有Resource注解的吗？那么此时的c为什么没有被初始化。 Spring的初始化大家都知道，无论Spring无论怎样初始化，都是需要生成一个对象的，这个对象不管是通过Class.loadClass或者Class.forName等，那么也是无论逃不过Java的初始化，在这里为了验证B中的异常是在初始化B的时候产生的，此时修改B位如下： 1234567891011121314151617@Servicepublic class B { @Autowired C c ; { System.out.println(&quot;public C cParam =getC()上面一行&quot;); } public C cParam =getC(); { System.out.println(&quot;public C cParam =getC()下面一行&quot;); } public C getC(){ return c.getAnewC(); }} 在此调用测试类，你会发现&quot;public C cParam =getC()上面一行&quot;打印出来之后马上就会出错，这也印证了前面的猜想，这个异常是在初始化B的时候产生的。 猜想那么在这里就基本上可以才出来Spring的一个加载流程了，首先Spring会扫描所有的带有注解得类，然后当初始化完毕之后放入Beanfactory，最后再进行一个变量的赋值，为了验证此猜想，修改B和A代码为如下：类B 1234567891011121314151617@Servicepublic class B { @Autowired C c ; { System.out.println(&quot;public C cParam =getC()上面一行&quot;); } public C cParam =null; { System.out.println(&quot;public C cParam =getC()下面一行&quot;); } public C getC(){ return c.getAnewC(); }} 类A： 12345678910public class A { @Resource B b; public void someMethod(){ System.out.println(b.getC() == null); }} 此时解决。 总结所以以后在初始化的时候，需要注意变量的初始化是否会涉及到一些对象","link":"/2018/09/25/%E4%B8%80%E6%AC%A1SpringBoot%E5%88%9D%E5%A7%8B%E5%8C%96%E5%BC%95%E8%B5%B7%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"一次nginx的排查经历","text":"现象配置nginx的https，但是修改配置文件之后一直无法访问。。。 排查步骤刚开始以为是防火墙的原因，由于是阿里云的主机，所以直接登录云主机查看安全配置。一切都是OK的。 然后又查看主机自己的防火墙，由于是centos7，所以在此花了点时间，最后还是将443端口添加到了防火墙规则中，然后重启防火墙。。。 https访问网站，发现还是没反应。 这个时候就开始怀疑nginx的配置文件是不是哪里配置错误了，主要都是修改的 443 的服务 其实这时候思路已经错误了 后来通过https还是一直不能访问到 继续排查当时想了下，索性直接修改 80 配置的server，然后发现修改之后还能正常访问，于是就推断可能是修改的nginx的配置文件并非是nginx所读取的。于时通过以下命令发现问题： 1234[root@iZ2lqpf5ei7560Z ~]# locate nginx.conf/usr/local/nginx/conf/nginx.conf/usr/local/nginx/conf/nginx.conf.default/usr/local/nginx/nginx-1.10.1/conf/nginx.conf 可以看到我的服务器中出现了两个nginx.conf，最下面那一个是nginx安装文件中的配置文件，我之前一直改的就是这个文件 然后再查看nginx的服务 12345[root@iZ2lqpf5ei7560Z ~]# ps -ef|grep nginxroot 4154 1 0 Dec16 ? 00:00:00 nginx: master process ./nginxroot 4155 4154 0 Dec16 ? 00:00:00 nginx: worker processroot 10071 10026 0 00:15 pts/2 00:00:00 grep --color=auto nginx[root@iZ2lqpf5ei7560Z ~]# 看到这里就大概知道原因了。之前一直改的是编译文件中的配置文件，自然是不会生效的。 思考这也为之后遇到问题的排查思路提供了一点警示，即遇到问题的时候不要慌张，首先一点点的梳理逻辑，然后再进行排查。","link":"/2018/12/21/%E4%B8%80%E6%AC%A1nginx%E7%9A%84%E6%8E%92%E6%9F%A5%E7%BB%8F%E5%8E%86/"},{"title":"为什么Spring官方推荐通过构造器注入","text":"一我们在使用Spring的时候，最方便的就是它的IOC（控制反转），也就是所有的Bean都交由 Spring进行管理，那么我们在看网上的文章或者自己在写代码的时候经常会像这样写： 123456789101112131415161718192021222324@Servicepublic class TestService { @Autowired TestDao TestDao; public void getInfo(){ }}@Servicepublic class TestService { private final TestDao TestDao; @Autowired public TestService(TestDao TestDao){ this.TestDao = TestDao; } public void getInfo(){ }} 但是貌似许多人都会使用第一种方式，因为这样简单方便，如果是采用第二种的话，每一次新增加一个bean，都需要在构造器的入参上面加一个参数，就会显得有点麻烦。 但是如果采用第一个写法，就会在IDEA里面出现一个提示： spring官方建议通过构造器的方式进行注入 这又是为什么呢？ Spring官方对于Setter注入和构造器注入的看法 二在springboot里面，最常用的注入方式有两种：一种是构造器注入，一种是field注入 在上一部分的代码里面，第一个是 Field 注入，第二个是构造器注入，既然这两种方式Spring都支持，那么到底这两种有什么区别呢？ Field注入这边新建两个测试类： 1234567891011121314151617@Servicepublic class A { public void sayA(){ System.out.println(&quot;A&quot;); }}@Servicepublic class B { @Autowired A a; public B(){ a.sayA(); }} 此时你启动Spring的话，就会出现一个空指针的异常，如果需要避免的话，则必须是类 A 先进行初始化，然后再初始化 B 。（当然Spring官方也提供了很多方式来控制 Bean 的初始化顺序，但是和本篇文章无关） 构造器注入1234567891011@Servicepublic class B { private final A a; @Autowired public B(A a){ a.sayA(); this.a = a; }} 此时Spring的项目就会正常的启动，那么为什么同样的代码，一个通过构造器注入，一个通过Field注入，两者的结果相差这么大呢？ 三官方之所以现在推荐使用构造器注入，是因为通过构造器注入是可以防止 空指针异常，同时可以确保的是被引用的 Bean ，它的引用是不可以变的，所以这可能是Spring官方团队的一些权衡点吧当然早期的时候，Spring曾推荐过使用Setter注入，不够现在可能Spring可能觉得构造器注入比较好。","link":"/2019/07/22/%E4%B8%BA%E4%BB%80%E4%B9%88Spring%E5%AE%98%E6%96%B9%E6%8E%A8%E8%8D%90%E9%80%9A%E8%BF%87%E6%9E%84%E9%80%A0%E5%99%A8%E6%B3%A8%E5%85%A5/"},{"title":"代理模式之JDK为什么需要实现接口(上篇)","text":"简介首先代理模式分为静态代理和动态代理，由于JDK采用的是动态代理，所以静态代理在这里不再介绍。 动态代理JDK动态代理首先JDK的动态代理要求真实的对象必须实现一个接口，而代理类则实现InvocationHandler接口来完成动态代理。如下代码： 接口 123public interface Car { void jdkProxy();} 被代理的类 123456public class Bus implements Car{ @Override public void jdkProxy() { System.out.println(&quot;测试&quot;); }} 代理类 123456789101112131415161718public class ProxyTest implements InvocationHandler { private Object target; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;before---------&quot;); Object object = method.invoke(target,args); System.out.println(&quot;after---------&quot;); return object; } public Object getInstance(Object target) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException { this.target = target; Class&lt;?&gt; clazz = target.getClass(); return Proxy.newProxyInstance(new SelfClassLoader(),clazz.getInterfaces(),this); }} 在代理模式中真实对象继承一个接口如果说是为了在代理类中统一管理，那么在 JDK动态代理 中如果被代理的对象还必须实现一个接口就有点繁琐了，但是对于 Java 语言来说其实也没办法。我们都认为很繁琐的一个实现，JDK 开发人员应该也会想到这一点。 下面就来解释下为什么动态代理必须实现一个接口。 为什么必须实现一个接口在回答这个问题之前，首先要说明的是 JDK动态代理 实现的原理是重新为对象类生成了一个子类，这个类由于是 JDK 自己生成的，于是会以$开头，在 JDK 文档中有如下说明： 12345A proxy class has the following properties:Proxy classes are public, final, and not abstract.The unqualified name of a proxy class is unspecified. The space of class names that begin with the string &quot;$Proxy&quot; should be, however, reserved for proxy classes.A proxy class extends java.lang.reflect.Proxy. 其实还有很多说明，但是涉及到本文的也就上面三条。 proxy类是非抽象类，且由public final修饰。 proxy类是以$Proxy开头的加类的名称。 proxy类是java.lang.reflect.Proxy的子类。 那么既然是$Proxy开头的，那么是否可以将生成的子类打印出来呢？，如下代码： 可以看到这个新生成的类其实是直接继承了 Proxy 类，由于Java的单继承方式，此时如果要获取对象类里面的方法那么就必须实现一个接口，所以也就对象类必须要实现一个接口。 1Car car = (Car) new ProxyTest().getInstance(new Bus()); 那么当我们执行这个代码的时候究竟在执行什么呢？在这里 debug 来跟踪源码解释 JDK 到底是如何生产 Proxy 代理类的. newProxyInstance方法 在这个方法里面，目前暂时需要注意红框中的方法，代理类是在该方法里面是生成的。 Class&lt;?&gt; cl = getProxyClass0(loader, intfs); 这一行代码，在这里返回的是一个 class，然后通过反射来初始化这个 class，最后这个 class 就是 JDK 生成的代理类。 getProxyClass0 而在 getProxyClass0 方法里面就是通过 proxyClassCache.get(loader, interfaces)来获取 Proxy，如果里面包含该 classLoader 加载的 interface 的话，则直接返回，否则就是再初始化一次，然后返回。 1234567891011private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) { if (interfaces.length &gt; 65535) { throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); } // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory return proxyClassCache.get(loader, interfaces);} proxyClassCache.get(loader, interfaces) 从这个方法之后就是真正开始生成 Proxy 类的过程，由于这里面包含一些 Java8 的 lambda表达式 用法，对于不太了解的人可能会有点看不懂。 上面的图给出了初始化 map 的一个方法，在这里需要注意的是，这个map的结构如下： 12private final ConcurrentMap&lt;Object, ConcurrentMap&lt;Object, Supplier&lt;V&gt;&gt;&gt; map = new ConcurrentHashMap&lt;&gt;(); 也就是一个两层的 ConcurrentMap，而之所以为为什么这样设计是跟实现的接口相关的，是由于第一层的 key 是由 classLoader 所生成的 key，而第二层测试keyX，与所实现的接口有关。剩下的将会在下一篇文章。","link":"/2020/04/15/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E4%B9%8BJDK%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%9E%E7%8E%B0%E6%8E%A5%E5%8F%A3-%E4%B8%8A%E7%AF%87/"},{"title":"代理模式之JDK为什么需要实现接口(下篇)","text":"前言在上篇主要介绍了 JDK 动态代理执行的一些流程，通过 Proxy 类来实现生成新的代理类，在这一篇主要讲的是 Proxy 是如何来生成以及缓存生成的代理类 二级缓存在上一篇的结尾提到过了一个map，其结构如下： ConcurrentMap&lt;Object, ConcurrentMap&lt;Object, Supplier&gt;&gt; 在这里可以看到是两个 ConcurrentMap 来实现的嵌套，那么 Java 为什么要这样设计呢？首先查看 CacheKey 的代码。如下： 可以看到 CacheKey 是继承自 WeakReference，以便于当该 CacheKey 没人使用的时候可以让 GC 直接回收。ReferenceQueue 是 WeakReference 所使用的。 那么对于 Java 来讲，其是用一个 classLoader 为 key 的 cacheKey 来生成一个 key，这个key就是二级缓存的第一个key。回到上述的map。 可以看到在首次初始化的时候，map 还是会以刚刚的 cacheKey 来获取二级缓存，但是首次一定是空的，所以此时直接 new 了一个 ConcurrentMap 作为 value 来放入二级缓存。再往下看代码： 1Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); 这是一个 Java8 的 lambda 表达式，查看这个表达式实现的地方： 上面的 KeyX 的作用是用被代理的人所实现的接口进行hash，同时将它们实现的接口放入 refs，这个refs是一个数组， 这个 refs 和 WeakReference 有关，主要是用于当 WeakReference 的对象被回收的时候作为一个监听 此时这个 subKey 已经生成了，那么接下来就是 Supplier supplier = valuesMap.get(subKey); 这个也是 Java8 的 lambda 函数，具体作用是通过调用其 get 方法，可以快读的执行函数，但是这里，这个 supplier 是由 Factory 所实现的。 这一段代码的具体逻辑是，当 supplier 不为空的时候就直接通过 get 方法获取其 value，然后返回，但是如果为 null 的话，那么首先就要初始化一个 Factory，然后在这期间，如果有现成将 supplier 赋值了，那么此时就会进行空判断，如果替换 valueMap 成功，则直接返回刚刚生成的 factory，否则返回 valueMap 里面的值。 在这里需要注意的是这个 value 就是一个代理类的的类名称。 代理类生成1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic synchronized V get() { // serialize access // re-check Supplier&lt;V&gt; supplier = valuesMap.get(subKey); if (supplier != this) { // something changed while we were waiting: // might be that we were replaced by a CacheValue // or were removed because of failure -&gt; // return null to signal WeakCache.get() to retry // the loop return null; } // else still us (supplier == this) // create new value V value = null; try { value = Objects.requireNonNull(valueFactory.apply(key, parameter)); } finally { if (value == null) { // remove us on failure valuesMap.remove(subKey, this); } } // the only path to reach here is with non-null value assert value != null; // wrap value with CacheValue (WeakReference) CacheValue&lt;V&gt; cacheValue = new CacheValue&lt;&gt;(value); // put into reverseMap reverseMap.put(cacheValue, Boolean.TRUE); // try replacing us with CacheValue (this should always succeed) if (!valuesMap.replace(subKey, this, cacheValue)) { throw new AssertionError(&quot;Should not reach here&quot;); } // successfully replaced us with new CacheValue -&gt; return the value // wrapped by it return value;} 这里还有一个重要的方法就是 Factory.get()，该方法由 synchronized 修饰，是用于生成代理类的 class 文件的。而生成代理类的方法在 ProxyClassFactory 的 apply 里面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Overridepublic Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) { Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); for (Class&lt;?&gt; intf : interfaces) { // ...... 省略 if (!interfaceClass.isInterface()) { throw new IllegalArgumentException( interfaceClass.getName() + &quot; is not an interface&quot;); } } // ...... 省略 for (Class&lt;?&gt; intf : interfaces) { int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) { accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf('.'); String pkg = ((n == -1) ? &quot;&quot; : name.substring(0, n + 1)); if (proxyPkg == null) { proxyPkg = pkg; } else if (!pkg.equals(proxyPkg)) { throw new IllegalArgumentException( &quot;non-public interfaces from different packages&quot;); } } } //...... 省略 /* * Choose a name for the proxy class to generate. */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * Generate the specified proxy class. */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try { return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); } catch (ClassFormatError e) { /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); }}} 在这里有几个重要的方法说下： 123456// 判断是否是接口，这也是为什么 JDK动态代理必须实现一个接口了，因为不是接口的话这个地方验证过不去if (!interfaceClass.isInterface()) { throw new IllegalArgumentException( interfaceClass.getName() + &quot; is not an interface&quot;);} 至于生成的类，可以通过生成文件来查看了。当生成完 class 文件之后，回到 Proxy.newProxyInstance 方法。 1234567891011final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams);final InvocationHandler ih = h;if (!Modifier.isPublic(cl.getModifiers())) { AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() { public Void run() { cons.setAccessible(true); return null; } });}return cons.newInstance(new Object[]{h}); 在这里是将参数上的 InvocationHandler 通过构造器将代理类中的 InvocationHandler 赋值，从而使用invoke 方法来实现代理。 生成的代理类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//import design_model.proxy.jdk.Car;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;public final class $Proxy0 extends Proxy implements Car { private static Method m1; private static Method m3; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler var1) throws { super(var1); } public final boolean equals(Object var1) throws { try { return (Boolean)super.h.invoke(this, m1, new Object[]{var1}); } catch (RuntimeException | Error var3) { throw var3; } catch (Throwable var4) { throw new UndeclaredThrowableException(var4); } } public final void run() throws { try { super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final String toString() throws { try { return (String)super.h.invoke(this, m2, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final int hashCode() throws { try { return (Integer)super.h.invoke(this, m0, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } static { try { m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m3 = Class.forName(&quot;design_model.proxy.jdk.Car&quot;).getMethod(&quot;run&quot;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); } catch (NoSuchMethodException var2) { throw new NoSuchMethodError(var2.getMessage()); } catch (ClassNotFoundException var3) { throw new NoClassDefFoundError(var3.getMessage()); } }}","link":"/2020/04/21/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E4%B9%8BJDK%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%9E%E7%8E%B0%E6%8E%A5%E5%8F%A3-%E4%B8%8B%E7%AF%87/"},{"title":"使用Mq实现延时队列","text":"在实际的应用开发中，下游系统并不需要立即处理上游系统的mq，但是又不可能将消息阻塞在上有系统中。且这两个系统之间又没有接口提供出来。这个时候就需要通过Mq的死信队列来实现一个延时效果 简介由于Mq的发送方不支持延迟发送(目前的新版本可以使用插件来支持，但是可能由于公司的其他限制，导致无法升级)，这时候就需要使用Mq的死信队列来实现延时队列 一般情况下，Mq的发送流程如下： Mq的死信队列在Mq的使用中，死信队列用于处理一下三种情况： The message is negatively acknowledged by a consumer using basic.reject or basic.nack with requeue parameter set to false The message expires due to TTL; or The message is dropped because its queue exceeded a length limit 上述三种情况分别是 消费者拒绝了该Mq，同时消费者也设置该消息不重新入队。 消息过期，即无人消费 队列设置了长度，同时队列已满 由于Mq不支持消息的延迟发送，即消息一经投递，就会马上入队，到达交换机。然后根据交换机的属性，进行投递。 这样就带来了一个问题，如果发送方需要消费者等待一定的时间才能进行消费。如果不经由死信队列，那么只能在发送方做等待。 例如使用Thread.sleep()来实现，由于Sleep方法是阻塞的，所以这样做又会影响到性能。又或者通过定时任务来实现，但是定时任务每一次又要去取出该发送的Mq，然后再发出去，这样就会非常的影响到效率。 死信队列实现延迟队列通过Rabbitmq的死信转发转发规则2，便可以实现一个延时队列。具体流程如下： 这里实现的关键是讲消费队列设置为DLK,TTL,DLX，具体如下： 在上图里面，可以看到Tutorials是一个消费队列，同时已经设置了它的死信转发规则。而dTutorials是一个死信队列，这个队列就是用于存放死信队列 代码如下：12345678910111213141516171819202122232425262728293031323334353637 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(HOST); factory.setUsername(USER); factory.setPassword(PASSWORD); factory.setPort(5672); factory.setVirtualHost(&quot;/&quot;); // 声明一个连接 connection = factory.newConnection(); channel = connection.createChannel(); // 在这里分别新建两个Exchange，一个是死信的Exchange，一个是消费者的Exchange channel.exchangeDeclare(exchangeName,&quot;direct&quot;,true,false,null); channel.exchangeDeclare(dExchangeName,&quot;direct&quot;,true,false,null); byte[] bytes =messgae.getBytes(); //声明一个队列 - 持久化，同时设置死信的转发Exchange和Queue。以及消息的过期时间 Map&lt;String,Object&gt; args = new HashMap&lt;String, Object&gt;(); args.put(&quot;x-dead-letter-exchange&quot;,dExchangeName); args.put(&quot;x-dead-letter-routing-key&quot;,routingKey); args.put(&quot;x-message-ttl&quot;,2000); channel.queueDeclare(queueName, true, false, false, args); channel.queueDeclare(dQueeueName, true, false, false, null); //设置通道预取计数 channel.basicQos(1); //将消息队列绑定到Exchange channel.queueBind(queueName, exchangeName, routingKey); channel.queueBind(dQueeueName, dExchangeName, routingKey); channel.basicPublish(exchangeName, routingKey, null, bytes);} 生产者代码： 1234567891011121314151617181920212223242526272829303132ConnectionFactory factory = new ConnectionFactory(); factory.setHost(HOST); factory.setUsername(USER); factory.setPassword(PASSWORD); factory.setPort(5672); factory.setVirtualHost(&quot;/&quot;); // 声明一个连接 connection = factory.newConnection(); // 声明消息通道 channel = connection.createChannel(); //在这里消费者直接消费死信队列即可 channel.queueBind(dQueueName, dExchangeName, routingKey); DefaultConsumer defaultConsumer =new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String str = new String(body); System.out.println(str); System.out.println(new Date()); if(&quot;测试&quot;.equals(str)){ channel.basicReject(envelope.getDeliveryTag(),false); }else{ channel.basicAck(envelope.getDeliveryTag(),false); } } }; channel.basicConsume(dQueueName,defaultConsumer); 下图中可以看到，消费者总是在2秒钟之后收到了发送方发送的消息，这时一个延时队列就实现了 详细代码连接具体的代码以上传至Github:https://github.com/Somersames/MqTutorials","link":"/2019/03/04/%E4%BD%BF%E7%94%A8Mq%E5%AE%9E%E7%8E%B0%E5%BB%B6%E6%97%B6%E9%98%9F%E5%88%97/"},{"title":"使用Mybatis遇到的there is no getter 异常","text":"在使用mybatis的时候有时候会遇到一个问题就是明明参数是正确的，但是还是会提示There is no getter XXX这个异常，但是一般的解决办法是在mapper里面添加@Param注解来完成是别的，那么为什么会遇到这个问题呢？ 以下为举例代码: Mapper层代码 12345public interface Pro1_Mapper { Pro1_Studnet insertStu(Pro1_Studnet pro1_studnet);} 实体类代码 123456789101112131415161718192021222324252627282930313233343536373839404142public class Pro1_Studnet { private String stuId; private String stuName; private String stuClass; private String stuTeacher; public String getStuId() { return stuId; } public void setStuId(String stuId) { this.stuId = stuId; } public String getStuName() { return stuName; } public void setStuName(String stuName) { this.stuName = stuName; } public String getStuClass() { return stuClass; } public void setStuClass(String stuClass) { this.stuClass = stuClass; } public String getStuTeacher() { return stuTeacher; } public void setStuTeacher(String stuTeacher) { this.stuTeacher = stuTeacher; }} Main方法 1234567891011121314151617public static void main(String[] args) { Logger logger = null; logger = Logger.getLogger(Pro1_Main.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); Pro1_Mapper pro1_Mapper = sqlSession.getMapper(Pro1_Mapper.class); Pro1_Studnet pro1_studnet =new Pro1_Studnet(); pro1_studnet.setStuName(&quot;张三&quot;); Pro1_Studnet pro1_studnet1 =pro1_Mapper.insertStu(pro1_studnet); System.out.println(pro1_studnet1.getStuClass()); sqlSession.commit(); } finally { sqlSession.close(); } } XML文件 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;study.szh.demo.project1.Pro1_Mapper&quot;&gt; &lt;resultMap type=&quot;study.szh.demo.project1.Pro1_Studnet&quot; id=&quot;pro1_stu&quot;&gt; &lt;result property=&quot;stuId&quot; column=&quot;stu_id&quot;/&gt; &lt;result property=&quot;stuName&quot; column=&quot;stu_name&quot;/&gt; &lt;result property=&quot;stuClass&quot; column=&quot;stu_class&quot;/&gt; &lt;result property=&quot;stuTeacher&quot; column=&quot;stu_teacher&quot;/&gt; &lt;/resultMap&gt; &lt;select id=&quot;insertStu&quot; parameterType=&quot;study.szh.demo.project1.Pro1_Studnet&quot; resultMap=&quot;pro1_stu&quot;&gt; SELECT * from pro_1stu where stu_name = #{pro1_studnet.stuName}; &lt;/select&gt;&lt;/mapper&gt; 如果执行上述的代码，你会发现mybatis会抛出一个异常：There is no getter for property named 'pro1_studnet' in 'class study.szh.demo.project1.Pro1_Studnet'很明显就是说pro1_studnet这个别名没有被mybatis正确的识别，那么将这个pro1_studnet去掉呢? 尝试将xml文件中的pro1_studnet去掉然后只保留stuName，执行代码： 1张三 这表明程序运行的十分正常，但是在实际的写法中，还有如果参数为String也会导致抛出getter异常，所以此次正好来分析下 分析mybatis是如何解析mapper参数的跟踪源码你会发现在MapperProxy的invoke处会进行入参: 1234567891011121314@Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { if (Object.class.equals(method.getDeclaringClass())) { return method.invoke(this, args); } else if (isDefaultMethod(method)) { return invokeDefaultMethod(proxy, method, args); } } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } final MapperMethod mapperMethod = cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); } 注意此处的args，这个参数就是mapper的入参。 那么mybatis在这里接收到这个参数之后，它会将参数再一次进行传递，此时会进入到MapperMethod的execute方法 1234567891011121314151617181920212223242526272829public Object execute(SqlSession sqlSession, Object[] args) { //省略无关代码 case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) { executeWithResultHandler(sqlSession, args); result = null; } else if (method.returnsMany()) { result = executeForMany(sqlSession, args); } else if (method.returnsMap()) { result = executeForMap(sqlSession, args); } else if (method.returnsCursor()) { result = executeForCursor(sqlSession, args); } else { Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); } break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(&quot;Unknown execution method for: &quot; + command.getName()); } if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) { throw new BindingException(&quot;Mapper method '&quot; + command.getName() + &quot; attempted to return null from a method with a primitive return type (&quot; + method.getReturnType() + &quot;).&quot;); } return result; } 因为在xml文件里面使用的是select标签，所以会进入case的select，然后此时会进入到Object param = method.convertArgsToSqlCommandParam(args); 在这里args还是Stu的实体类，并未发生变化 随后进入convertArgsToSqlCommandParam方法，然后经过一个方法的跳转，最后会进入到ParamNameResolver的getNamedParams方法， 12345678910111213141516171819202122public Object getNamedParams(Object[] args) { final int paramCount = names.size(); if (args == null || paramCount == 0) { return null; } else if (!hasParamAnnotation &amp;&amp; paramCount == 1) { return args[names.firstKey()]; } else { final Map&lt;String, Object&gt; param = new ParamMap&lt;Object&gt;(); int i = 0; for (Map.Entry&lt;Integer, String&gt; entry : names.entrySet()) { param.put(entry.getValue(), args[entry.getKey()]); // add generic param names (param1, param2, ...) final String genericParamName = GENERIC_NAME_PREFIX + String.valueOf(i + 1); // ensure not to overwrite parameter named with @Param if (!names.containsValue(genericParamName)) { param.put(genericParamName, args[entry.getKey()]); } i++; } return param; } } 此时注意hasParamAnnotation这个判断，这个判断表示该参数是否含有标签，有的话在这里会在Map里面添加一个参数，其键就是GENERIC_NAME_PREFIX(param) + i 的值。像在本次的测试代码的话，会直接在return args[names.firstKey()];返回，不过这不是重点，继续往下走，会返回到MapperMethod的execute方法的这一行result = sqlSession.selectOne(command.getName(), param); 此时的param就是一个Stu对象了。 继续走下去…由于mybatis的调用链太多，此处只会写出需要注意的点，可以在自己debug的时候稍微注意下。 BaseExecutor的createCacheKey的方法123456789101112131415161718192021222324252627282930313233343536@Override public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) { if (closed) { throw new ExecutorException(&quot;Executor was closed.&quot;); } CacheKey cacheKey = new CacheKey(); cacheKey.update(ms.getId()); cacheKey.update(rowBounds.getOffset()); cacheKey.update(rowBounds.getLimit()); cacheKey.update(boundSql.getSql()); List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry(); // mimic DefaultParameterHandler logic for (ParameterMapping parameterMapping : parameterMappings) { if (parameterMapping.getMode() != ParameterMode.OUT) { Object value; String propertyName = parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) { value = boundSql.getAdditionalParameter(propertyName); } else if (parameterObject == null) { value = null; } else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) { value = parameterObject; } else { MetaObject metaObject = configuration.newMetaObject(parameterObject); value = metaObject.getValue(propertyName); } cacheKey.update(value); } } if (configuration.getEnvironment() != null) { // issue #176 cacheKey.update(configuration.getEnvironment().getId()); } return cacheKey; } 当进行到这一步的时候，由于mybatis的类太多了，所以这里选择性的跳过，当然重要的代码还是会介绍的。 DefaultReflectorFactory的findForClass方法1234567891011121314@Override public Reflector findForClass(Class&lt;?&gt; type) { if (classCacheEnabled) { // synchronized (type) removed see issue #461 Reflector cached = reflectorMap.get(type); if (cached == null) { cached = new Reflector(type); reflectorMap.put(type, cached); } return cached; } else { return new Reflector(type); } } 注意MetaObject的getValue方法： 12345678910111213public Object getValue(String name) { PropertyTokenizer prop = new PropertyTokenizer(name); if (prop.hasNext()) { MetaObject metaValue = metaObjectForProperty(prop.getIndexedName()); if (metaValue == SystemMetaObject.NULL_META_OBJECT) { return null; } else { return metaValue.getValue(prop.getChildren()); } } else { return objectWrapper.get(prop); } } 这里的name的值是pro1_stu.stuName,而prop的属性是这样的:这里的hasNext函数会判断这个prop的children是不是为空，如果不是空的话就会进入 get 方法，然后进入到如下的方法通过返回获取get方法。所以当遍历到stuName的时候会直接return， 然后就需要注意Reflector的getGetInvoker方法， 1234567public Invoker getGetInvoker(String propertyName) { Invoker method = getMethods.get(propertyName); if (method == null) { throw new ReflectionException(&quot;There is no getter for property named '&quot; + propertyName + &quot;' in '&quot; + type + &quot;'&quot;); } return method; } 这个propertyName就是pro1_studnet，而getMethods.get(propertyName);就是要通过反射获取pro1_studnet方法，但是很明显，这里是获取不到的，所以此时就会抛出这个异常。 那么为什么加了@param注解之后就不会抛出异常呢此时就需要注意MapWrapper类的get方法。 123456789@Override public Object get(PropertyTokenizer prop) { if (prop.getIndex() != null) { Object collection = resolveCollection(prop, map); return getCollectionValue(prop, collection); } else { return map.get(prop.getName()); } } 在之前就说过，如果加了注解的话，map的结构是{“param1”,”pro1_studnet”,”pro1_studnet”,XXX对象}，此时由于prop的index是null，所以会直接返回map的键值为pro1_studnet的对象。而在DefaultReflectorFactory的findForClass里面，由于所加载的实体类已经包含了Pro1_Student，随后在metaValue.getValue(prop.getChildren());的将stu_name传入过去，就可以了获取到了属性的值了。","link":"/2018/09/10/%E4%BD%BF%E7%94%A8Mybatis%E9%81%87%E5%88%B0%E7%9A%84there-is-no-getter-%E5%BC%82%E5%B8%B8/"},{"title":"使用lambda来优化责任链模式","text":"责任链模式是设计模式的一种，可以为调用的对象进行一个链式处理，这种模式在 Java 的一些第三方库中经常见到。 而在业务开发中，这种需求也是很常见的，如果用好这个设计模式，对于代码的扩展性和可维护性都是非常有帮助的，例如常见的下单流程，就可以用责任链模式处理。 而在第三方库中，像 tomcat 的过滤器就是使用责任链模式进行处理 Tomcat 中的使用在 tomcat 中，过滤器的实现就完全是责任链模式的使用了 首先 tomcat 定义了一个 Filter 类，这个类有三个方法，其中 init 和 destroy 为 default 方法，因此不要求子类必须实现，而 doFilter 则是一个必须实现的方法，正是这个方法实现了链式调用。 如果之前有使用过 servlet 的同学应该知道，如果需要使用过滤器，则需要在要在 web.xml 里面配置所需要加载的 Filter，tomcat 会将所配置的过滤器加载到一个数组里面去，最后在调用 doFilter 的时候，选择 chain 进行传递 Java 中常见的用法Java 中的最常见用法有两种 一种是用一个 List 将所有的实现类全部添加到一个集合里面 一种就是通过 List 的 iterator 进行迭代调用 Lambda表达式JDK8 中一个大升级就是提供了 lambda 表达式，而这个特性又为责任链模式的使用提供了一个新的实现方式 Function函数式接口是 JDK8 推出的一个升级点，不同于之前的接口，函数式接口有且只能有一个抽象方法，并且接口被 FunctionalInterface修饰，配合 Stream 类，可以方便快速的实现各种操作 该接口可以将一个范型为 T 的入参，转化为 R 类型的返回值，如果单独来看仅仅就是一个普通的接口，在 JDK8 之前的版本也可以实现类似的功能。 但是 JDK8 中配合 Stream 就可以实现非常大的提升。 StreamStream 是 JDK8 提供的一个非常强大的工具类，能将集合转为为流，通过一系列的操作，将流转化为某种结果 reducereduce 是 Stream 类中的一个方法，可以将流进行合并，并且产生一个新的值，这个值可以是一个 函数式接口，也可以是一个具体的某个值，下面是其三个方法 其中参数个数依次变多，不过今天主要是来研究前两个，利用这两个函数可以实现一个非常优雅的责任链吊用 数组求和Demo 这是一个数组求和的demo，分别使用了 reduce 的前两种用法，不过第一个返回的是 Optional，所以需要 get 操作。 而第二个方法由于第一个参数已经指定了返回的类型，因此无需再操作get，并且第二个方法的第一个参数其实也是一个初始值，可以将其与最后的结果累加。 在这里如果继续思考一下，如果 reduce 里面是一个函数，并且第一个函数的值作为第二个函数的入参，那么是不是就可以实现责任链了。 也就是说如果返回值是 Function，然后在 Function 中实现一个默认的方法，这个方法的返回值也是一个 Function，那么是不是就可以达到这个目标了。 定义接口因为 reduce 的入参需要是 Function 函数，因此在这里定义一个函数 其中 andThen 的思想就是将上一个 Function 的结果作为下一个 Function 的入参，然后再递归进行调用 1after.apply(apply(t)) 如果对算法比较了解的同学，可以看到这个方法就是一个递归调用，直至调用到达最后一个 Function。 那么接下来就定义一些 Processor 实现首先定一个查询商品的 Processor，该 Procrssor 通过商品的 ID 来获取商品的详细信息，包括库存数以及价格等等。 然后将商品的详细信息带入到优惠卷查询 Processor 里面去，最后判断该商品有哪些优惠卷 然后返回的 Goods 可以作为优惠卷查询 Processor 的入参，获取用户所包含的优惠券 链式调用首先将这两个 Processor 加入到一个集合里面去，方便转成 Stream，然后通过 reduce 调用 andThen 函数 最终的结果是，如果 apply 的入参是 d1，那么返回的结果如下： 12OrderInfo{goods=Goods{id='d1', remain=10, price=1}, coupon=Coupon{idList=null, discount=null}} 而如果入参是 “1”，那么得到的结果就是： 12OrderInfo{goods=Goods{id='1', remain=10, price=1}, coupon=Coupon{idList=[1], discount=10}} 于是通过 JDK8 的 lambda 就可以实现一个简单的责任链了 Context 传递也许有人说，这样的话，如果需要修改顺序，那入参的地方都要改，非常的不友好，别慌，还有 BiFunction 如果需要在一个链式调用中，通过 Context 来保存调用的上下文，那么可以这样实现 每一个实现 Processor 的类第一个参数都会是 OrderContext，而在调用的地方其实也没什么区别，就是多加了一个入参，所以非常的方便。 配合 Spring 使用如果配合 Spring 来使用，那么还有一个更加便捷的方法，每一个 Processor 都实现 Ordered 接口，然后通过 @Resouces 直接注入进来，非常的方便 最后其实在做业务的时候，如果可以抽空用 JDK 的新特性优化下之前的老代码，是一个非常不错的提升机会，既可以增加自己对业务的理解程度，又可以将所学用于业务开发，可以在日常的开发中尝试一下 本文的demo地址","link":"/2021/12/12/%E4%BD%BF%E7%94%A8lambda%E6%9D%A5%E4%BC%98%E5%8C%96%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/"},{"title":"关于ArrayList的所见所想","text":"关于ArrrayList扩容： 今天在面试的时候面试官提到过ArrayList扩容是原来的1.5倍加一，但是我看jdk1.8的时候是显示为1.5倍 查看源码： 123456789101112//Java1.8的扩容方法private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } 那么ArrayList会是扩充1.5倍之后加一呢？ 于是翻看以前的jdk源码，发现在jdk1.6的时候其代码确实是+1了 12345678910111213// java1.6版本的扩容方法public void ensureCapacity(int minCapacity) { modCount++; int oldCapacity = elementData.length; if (minCapacity &gt; oldCapacity) { Object oldData[] = elementData; int newCapacity = (oldCapacity * 3)/2 + 1; if (newCapacity &lt; minCapacity) newCapacity = minCapacity; // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } } 关于其扩容具体做法：jdk1.8 其次再jdk1.8中的ArrayList的一些分析： 首先需要向List中添加元素的时候： 调用 boolean add(E e); 这个方法首先会调用 ensureCapacityInternal(int)方法 1234567private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity);} 当第一次调用的时候因为在构造方法里面已经将设置为’elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA’ 所以第一次调用的时候是直接接着调用’private void ensureExplicitCapacity(int minCapacity)’方法。 但是这里会发现一个问题就是jdk1.8并未提供初始的默认值， 在jdk1.6中ArrayList的构造方法： 12345678910public ArrayList() { this(10); } public ArrayList(int initialCapacity) { super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; } 但是在jdk1.8中却并未有默认的初始值： 123public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;} 而是提供了一个修改容量的方法： 123456789101112public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) { ensureExplicitCapacity(minCapacity); } }","link":"/2017/11/07/%E5%85%B3%E4%BA%8EArrayLIst%E7%9A%84%E6%89%80%E8%A7%81%E6%89%80%E6%83%B3/"},{"title":"关于IO多路复用以及其他的整理","text":"记得去年去年10月份的时候一个电话面试，当时面试官由多线程那一部分知识转到了IO部分，先提出同步IO和异步IO，最后面试官提出要一个IO的多路复用需要怎么去实现。但是由于对多路复用 的一些模式还是不太清楚，而现在正好在研究 netty 的时候想起来了，所以做一个总结算了。 同步IO所谓的同步IO ，在刚学Java的Socket通信时就是一个阻塞IO，具体来讲就是在服务端的一个 while 中会调用 Inputstream inputStream =scoket.getInputStream() 在这里如果客户端没有发送数据的话，服务端就会一直卡在这里。或者说在接收数据的过程中会一直卡在这里，对于服务端来讲的话这里就是一个同步的，因为在这里没有接收到数据的话代码一直不能往下走。 异步阻塞IO Reactor IO多路复用异步代表的是被调用方需要主动的反馈通知，就比如在本例中，读取Socket的流，如果在读取流的时候必须等待调用方自己去判断是否读取完毕，则这个就是一个同步IO。但是如果在调取的时候，每一次调用方都可以得到一个反馈，无论客户端是否准备就绪。则称之为异步的：如下： 12345while(true){InputStream in =read(); // 假设此时每次调用都会返回一个结果Reader reader =new InputStreamReader(in);} 此时就可以称之为一个异步IO，主要就是因为这个读取操作不再会被阻塞，每一次的调用，被调用方都会反馈一个信息. 在 Java 的 Netty 中每次循环都会去检查所有的key，看下是不是有的key可以都去了或者可以写入了之类的，这个就是一个典型的IO多路复用 同步非阻塞继续以Socket举例：在同步IO的时候，没有收到数据是不会进行下一步操作的，但是同步非阻塞IO就是在这里直接判断数据准备好了没有，不管是否有数据，直接进行下一步操作。 异步非阻塞异步IO指的是调用方不必在这里一直询问是否准备好数据没，而是直接执行代码，最后等待被调用方的一个通知即可。在这里还没有找到很好的举例 扩展在这里其实在Java的多线程中 CallBack 和 Future 类似，当CallBack执行完毕之后再执行 Future 便可以得到一个结果，但是这个是一个同步阻塞的，异步","link":"/2018/04/28/%E5%85%B3%E4%BA%8EIO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E7%9A%84%E6%95%B4%E7%90%86/"},{"title":"关于Java的HashMap","text":"Map接口： 在map接口中的存在一个主接口：public interface Map 和一个内部接口：interface Entry 其中Map接口主要功能是提供map的一些基本的操作，例如put,inEmpty,get等，而Entry接口则主要是负责遍历操作时的一些方法，例如getKey(),getValue()，setValue等 #HashMap实现类： 在HaspMap这个类里面其实包含了很多的内部类：如下图：1234567891011121314NodeHashMapKeySetKeySpliteratorEntrySetValuesHashIteratorKeyIteratorValueIteratorEntryIteratorHashMapSpliteratorValueSpliteratorEntrySpliteratorTreeNode TreeNode类： TreeNode类是一个红黑树：里面包含的是一些红黑树的操作；EntrySet类： EntrySet类是提供一个HashMap的遍历方式的一个类，EntrySet类里面有iterator方法，其作用在于map的一种遍历方式：12345Iterator iterator =map.entrySet().iterator();while (iterator.hasNext()){ Map.Entry&lt;Object,Object&gt; map1 = (Map.Entry&lt;Object, Object&gt;) iterator.next(); System.out.println(map1.getKey());} 这里涉及的知识是EntrySet是继承了AbstractSet间接实现了Set接口，而map.entrySet()方法返回的是一个Set&lt;Map.Entry&lt;K,V&gt;&gt;这个实例。其他的暂时还不知道有什么用 关于HashMap的其他细节每一扩容都是2的n次方：首先为什么每一次都是2的n次方这是由于hash函数导致的，假设一个值是1001，那么另一个值可以是1101,1011,1111，这三种情况做且运算的话结果都是1001，发生了三次碰撞，那么假设固定值是2的n次方的话，减一之后是1111，折三个数与其做且运算的话都不会相同，所以会较小碰撞次数。 允许null作为键/值那么直接看代码： 123456789101112131415161718192021222324252627public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 很明显当我们传入的参数是null得时候hash的值是0的时候first = tab[(n - 1) &amp; hash]这个first永远是tab[0];但是又由于tab的长度是2的n次方,所以tab[0]肯定是null，所以当键是null的时候得到的也是null。","link":"/2017/11/09/%E5%85%B3%E4%BA%8EJava%E7%9A%84HashMap/"},{"title":"关于Java的链式代理","text":"关于Java的代理思路： 了解Enhancer的意义 查看生成的字节码（和原来的对比） 分析代理的区别 完成代理链 首先需要新建一个代理管理类，这个管理类会存储目标代理类，目标代理方法，以及一个List，List中存储的是一些代理类，最后是需要一个操作就是","link":"/2018/03/20/%E5%85%B3%E4%BA%8EJava%E7%9A%84%E9%93%BE%E5%BC%8F%E4%BB%A3%E7%90%86/"},{"title":"关于Java线程的一些思考","text":"消费者和生产者的模型：今天在看《现代操作系统》P73页的一个关于多线程的竞态条件的时候书中说到了唤醒等待位方法，这个方法使我突然想起联想到以前在Java多线程中sleep()方法会清除中断状态的一些类似之处：树上的代码： 1234567891011121314151617181920212223#definde N 100int count =0;void producer(void){int item;while(True){ item=producer_item(); if (count == N) sleep(); insert_item(item); count=count+1; if (count == 1 )wakeup(consumer);}}void consumer(void){ int item; while(true){ if (count == 0)sleep(); item =remove_item(); count=count -1; if (count == N-1) wakeup(producer); consumer_item() }} 在书中提到过一个情况就是：当消费者判断count==0的时候是会睡眠的，但是此时由于某种情况(线程的sleep()方法还没被执行完)，而恰巧生产者又生产了一个item，导致此时count加一为1，而当count为1的时候生产者是会发出一个wakeup信号给消费者的，此时，由于消费者并没有睡眠因此会忽略掉该信号，而当消费者真正睡眠之后又由于生产者再不会进行通知，导致队列被生产者塞满，从而该模型阻塞。那么为了解决该方法，书中引入了唤醒等待位方法，该方法就是在生产者发出信号给消费者的时候添加一个中断状态，而当该线程需要进行睡眠的时候会先判断状态，若是wakeup则不进行睡眠 Java多线程中的引用：在Java中调用interrupt()方法的原理也是类似的，设置一个中断状态，但是这样做的原因是为了安全起见，因为通过其他的线程来中断另一个线程是及其不安全的，在其他线程发出中断信号的时候，它并不会知道另一个线程目前正在做什么事情，所以安全的做法是设置一个中断状态","link":"/2018/02/07/%E5%85%B3%E4%BA%8EJava%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"title":"关于Leetcode上判断位数的解法","text":"在Leetcode上有一道算法题目判断最后一位是不是一位的，题目的意思是当在一个数组中如果存在10或者00，那么这个就是一个连续的。这个数组的最后一位永远都是0。 We have two special characters. The first character can be represented by one bit 0. The second character can be represented by two bits (10 or 11).Now given a string represented by several bits. Return whether the last character must be a one-bit character or not. The given string will always end with a zero. 思路：刚开始错误的理解题目的意思了，导致一直在纠结数组的最后两位和三位，后来看了答案之后觉得自己的思路没有想到电子上，所以在此记录一下。 解法一：这一题有一个明显的规律，也就是当 bits[i] 为 1 的时候，那么他就可以不管后面的一位，可以直接后移两位来判断。当 bits[i] 为 0 的时候，那么他就只能是向后移动一位了。所以算法如下： 1234567int len=0;while(int len =0 &amp;&amp; len&lt; bits.length){ len += bits[len] +1;}if(len == bits.length){ return true;} 解法二：从后遍历，找出第一个为 0 的，如果第一个为0的是数组的倒数第二位的话，那么就是 true 1234567int len =bits.length-2;while(len &gt;0 &amp;&amp; bits[len] &gt;0 ){ len --;}if(len == bits.length -2){ return true} 以上便是这两种解法了，只不过当时还未想到。","link":"/2018/08/17/%E5%85%B3%E4%BA%8ELeetcode%E4%B8%8A%E5%88%A4%E6%96%AD%E4%BD%8D%E6%95%B0%E7%9A%84%E8%A7%A3%E6%B3%95/"},{"title":"关于Unicode和其他的字符集以及Spring乱码处理","text":"Spring出现乱码的解决办法：若需要快速的解决乱码问题可以直接看配置文件: 后台逻辑：在项目中的web.xml中添加Spring的字符过滤器，配置如下： 1234567891011121314151617&lt;filter&gt; &lt;filter-name&gt;SpringEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter &lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;SpringEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 同时也可以在spring-mvc.xml中配置各个请求头过来的处理方式： 前端处理： 在普通的html页面中可以加入&lt;meta charset=&quot;UTF-8&gt;&quot; 在Jsp中需要加入&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; 另外对于ajax的异步请求的话若在url中包含汉字最好采用encodeURI()进行一个字符集处理 123456789101112131415&lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter&quot;&gt; &lt;property name=&quot;messageConverters&quot;&gt; &lt;list&gt; &lt;bean class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt; &lt;property name=&quot;supportedMediaTypes&quot;&gt; &lt;list&gt; &lt;value&gt;text/plain;charset=UTF-8&lt;/value&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;value&gt;applicaiton/javascript;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 另外在出现乱码之后还可以设置request.setCharset()或者response.setCharset()的方式来解决 编码的历史Unicode编码的出现由于在计算机中只能存储的是1和0，那么在早期美国那边正好以2的8次方，也就是一个字节来表示所有的英文字母和一些符号，所谓的ASCII码，但是当越来越多的人进入到互联网之后却发现最初的ACSCII码已经不够使用了。因为仅中国的汉字就多达几万种，那么可想而知世界上其他的国家和汉字。于是为了统一编码的愿望就出现了，Unicode的目的皆在为了将世界上的所有字符全部统计进来。在Unicode中，通常是用2个字节，也就是16为来表示一个字符。但是在Unicode中又分了17个位面，每一个位面都可以代表的不通过的字符。 UTF-8UTF-8和unicode的区别在于UTF-8是unicode的一种实现方式。在UTF-8的编码之中，可以含有三个字节或者更多的一个字节。读取规则： 最高为为0 ，类似于00000100,则以ASCII码的方式读取该字符。 若读取的字节最高为是以1开头的，检测含有几个1，含有几个1就代表的是读取后面的几个字节，例如：11100000 1000000 10000000表示该字符是由三个字节组成的。","link":"/2018/03/26/%E5%85%B3%E4%BA%8EUnicode%E5%92%8C%E5%85%B6%E4%BB%96%E7%9A%84%E5%AD%97%E7%AC%A6%E9%9B%86%E4%BB%A5%E5%8F%8ASpring%E4%B9%B1%E7%A0%81%E5%A4%84%E7%90%86/"},{"title":"关于两道动态规划的思考","text":"说到动态规划，离不开一个爬楼梯的问题和一个铺砖快的问题。爬楼梯的问题： 一个N层的楼梯，一次可以走一步或者两步，求走到楼梯顶部的所有步数 铺砖快的问题： 一个2*n的地方，需要铺上瓷砖，但是瓷砖的规格只有 2x1 的，求多少种铺法。 计算到顶层的最小问题： On a staircase, the i-th step has some non-negative cost cost[i] assigned (0 indexed). Once you pay the cost, you can either climb one or two steps. You need to find minimum cost to reach the top of the floor, and you can either start from the step with index 0, or the step with index 1. 也就是说，可以从0或者1开始选择起点，而且每一步都可以选择走一步还是两步。 走楼梯首先这两道题目都可以使用递归来实现，关于爬楼梯的问题，一般是采用递归实现，如下： 123456789101112 /** * * @param step 表示从第几步开始走 * @param end 表示有多少层楼梯 * @return */private static int upLouti(int step ,int end){ if(step == end || step == end -1 ){ return 1; } return upLouti(step+1,end) + upLouti(step+2,end); } 那么此题的递归解法就是从0一直求到end，直到结束。 铺瓷砖铺瓷砖也是类似的一个解法，也就是如果第一次是横着铺，那么下面一个肯定也是只能横着铺。如果第一个瓷砖是竖着铺的，那么可以直接进行长度减1，然后再计算。 第一次铺： 可以看到，如果第一次铺的砖块是竖着铺的话，那么剩余的计算就是n-1。 如果第一次是横着铺的话，那么其下面的一块砖肯定也是只能横着铺的，所以直接长度减2，计算就是n-2 代码如下： 123456789101112private static int puCiZhuan(int step ,int end){ if(step &gt;= end){ return 0; } if(step == end -1){ return 1; } if(step == end -3){ return 2; } return puCiZhuan(step+1,end) + puCiZhuan(step + 3 ,end); } 最小路径到顶点当在解决这个问题的时候，不能和走楼梯有一样的思路了，因为该题不仅仅需要考虑下一步的位置，还需要考虑下下一步的位置。例如0,2,2,1，如果只考虑单步的话，那么它的最优解就是1+2 =3 ，但是其实着一道题目的最优解是2+0=2 ,所以在这里需要考虑的是如何保存上一次的结果然后进行比较。 1234567891011class Solution { public int minCostClimbingStairs(int[] cost) { int f1 = 0, f2 = 0; for (int i = cost.length - 1; i &gt;= 0; --i) { int f0 = cost[i] + Math.min(f1, f2); f2 = f1; f1 = f0; } return Math.min(f1, f2); }} 所以可以看到在该算法中，其中f2一直保存着上一次的计算结果","link":"/2018/09/15/%E5%85%B3%E4%BA%8E%E4%B8%A4%E9%81%93%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"关于字符串回文的Manacher算法","text":"对于Manacher算法自己研究了一会，总算是理解了其中的含义，乘着有时间正好可以过来记录下： 思路： 将字符串变成奇数，通过加非字符串里面的符号表示，不过一般都是加的# 找出以当前索引为中点的最长回文数长度，并且记录，例如：#a#b#a,这个字符串所对应的长度便是[1,2,1,2,1,1],因为在#的时候组成不了回文，所以#这里是1，而到了a这里，因为#a#组成了回文，所以a的最长回文字符串长度是2,以此类推 for循环开始比较，在这里一般来讲是首先需要定义一个字符串的边界变量，以防止数组越界，另外一个就是最长回文字符串的中点坐标，以此类推就可以 代码：将填充符号计入到需要匹配的的字符串中 12345678//最大回文串的数组 StringBuilder sb =new StringBuilder(); sb.append(&quot;#&quot;); for(int i=0 ;i&lt;str.length() ;i++){ sb.append(str.charAt(i)); sb.append(&quot;#&quot;); } String s1 =sb.toString(); 然后再定义一个最大的字符串长度和一个最长回文字符串的中点位置的坐标 123456789101112131415161718192021222324252627282930313233343536373839404142434445int array=[s1.length()];int maxRight =0;int mid =0;for(int i =0 ;i&lt;s1.length() ;i++){ int r=1; //定义最小的半径 if(i&gt; maxRight){ r=Math.min(array[2*mid -i ],maxRight-i); } //以此为中心点继续扩张 while(i - r &gt; &gt;0 &amp;&amp; i+r &lt;s1.length() &amp;&amp; s1.charAt(i-r) == s1.charAt(i+r) ){ r++; } //在这里需要减一的原因是因为需要将当前的多加一去除掉，因为i算了一遍当前的索引,r又算了一遍当前的索引 if(i+r -1 &lt;right){ right =i +r -1; mid =i; } array[i]=r;}//或者可以将for循环修改为如下：for (int i = 0; i &lt; s1.length(); i++) { int r = 0; if (array[mid] + mid &gt; i) { r = Math.min(array[2 * mid - i], array[mid] + mid - i); } else { r = 1; } while (i - r &gt; 0 &amp;&amp; i + r &lt; s1.length() &amp;&amp; s1.charAt(i - r) == s1.charAt(r + i)) { ++r; } if (mid + array[mid] &lt; array[i] + r) { mid = i; } if (r &gt; maxLength) { maxLength = r; }// if(i+r -1 &gt;maxLength){// maxLength =i+r-1;// mid=i;// }// array[i] = r; } 简单一点的代码如下： 123456789for(int i =0 ;i&lt;s1.length() ;i++){ r=Math.min(array[2*mid -i],right -i):1; //这种情况考虑的是当前索引位置再最长回文字符串之间，而且这个索引说不定还有更长的回文字符串，所以在这里需要一个while判断 while(array[i-array[i]] == array[i+array[i]]) ++r; if( i +array[i] &gt;right){ right =i+array[i]; array[i]=r; }} 在上面的代码中为什么是非要array[i-array[i]] == array[i+array[i]]相等才会进行半径的增加呢？这里其实有几种情况需要考虑的","link":"/2018/04/04/%E5%85%B3%E4%BA%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%9B%9E%E6%96%87%E7%9A%84%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/"},{"title":"关于流和多态的一些记录","text":"流的读取问题read()方法read()方法经常用于读取一个byte数组或者char数组，其内部的方法如下： 123public final int read(byte b[]) throws IOException { return in.read(b, 0, b.length);} 首先在读取字节流的时候是所有的字节流的顶级类的是InputStream抽象类，而继承自Inputstream的类都是带有read方法，每一个类几乎丢重写了自己的read方法，而这就是多态的一种体现。 关于socket.getInputStream()方法获取的InputStream这个获取的输入流是SocketInputStream,在这个类里面有一个变量就是private boolean eof;这个变量在read方法中可以用于判断文件是否已经到尾了 123456789int read(byte b[], int off, int length, int timeout) throws IOException { int n; // EOF already encountered if (eof) { return -1; } //省略后面方法 } 多态：其实在java的字节流处理类中就很好的利用了多态，以前只是了解但是实际使用的不多。而且最重要的是父类的静态变量子类是可以直接继承过来的","link":"/2017/11/10/%E5%85%B3%E4%BA%8E%E6%B5%81%E5%92%8C%E5%A4%9A%E6%80%81%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/"},{"title":"关于自己代码风格的一点思考","text":"随着第一个项目正式上线，是时候总结点什么了。以供自己日后反思。顺便也提醒下自己是时候看下《代码整洁之道》了 关于枚举和静态常量常量在代码里面最好不要有魔法值，这个虽然在阿里巴巴开发手册中提到过，但是自己刚开始的时候一直觉得，有时候这个变量就使用一次，为啥还需要定义一个变量，那样到时候去寻找这个变量的时候不是更加麻烦么。 后来才发现这个想法有点愚蠢，因为你根本不知道后来会不会使用这个变量。例如以下代码： 123if(&quot;A&quot;.equals(A)){ XXX} 假设代码中就一处使用这个”A”，没问题。但是当代码中有多处有类似的判断时，直接用上述代码虽然也可以完成，若随后需求一变动，那么就需要修改多个代码块的判断。这时候就会显得很繁琐，而且还说不准会漏掉几个。所以这个时候一个静态变量是不错的选择。例如： 1234public static final String XX_TYYPE =&quot;A&quot;;if(XX_TYPE.equals(A)){ XXX} 枚举枚举其实和常量在意义上是一样的。区别就是枚举可以表达更多的信息。例如最常用的接口返回值。 1234567891011121314public enum ResponseEnum{ SUCCESS(&quot;200&quot;,&quot;success&quot;), FAIL(&quot;100&quot;,&quot;fail&quot;) ; private String code; private String message; public ResponseEnum(String code,String message){ this.code = code; this.message = messgae; }} 这个时候就可以表达更多的信息了。同样的，利用枚举也可以来定义一些Exception。 配置文件除了上述的枚举和常量之外，其实还可以在配置文件中来定义一些通用的变量，在配置文件中定义一些变量的有点是无需修改代码(前提是使用了微服务，有自己的配置中心)。然后在代码中使用@Value注解来获取该变量。从而无需改动代码。 关于Lambda表达式Java8中使用的Lambda表达式确实可以简化不少代码的编写，但是需要注意的是。在使用的时候不要写过长的表达式。这样虽然可以方便开发者省去不少的时候，但是有时候却不利于后来维护者阅读代码。 关于Optional泛型这个泛型其实目前我使用的还是不较少，所以对于这个泛型的使用目前还不清楚是否可以简化代码。 代码的简化代码的简化是一门艺术，不仅仅是可以简化代码的 函数和变量命名对于函数和变量的命名，最好是越明确越规范越好。不然自己日后读起来…自己都不清楚这个函数是干啥的 代码行数太多，最好抽离出来一个方法里面如果行数太多，最好将其抽离出来的，作为一些独立的方法。这样不仅之后阅读起来比较方便，而且还方便以后替换逻辑，如果方法里面逻辑太多，那么在以后如果出现一个需求，功能和之前的某一个方法类似，仅仅是一点不同。 如果是优化后的代码，则仅仅是替换掉那部分不同的方法即可。若一个代码的方法逻辑太多，则必须重写一个类似的方法。这无形之间又加大了开发的工作量，而且也不便于日后的维护。","link":"/2018/12/09/%E5%85%B3%E4%BA%8E%E8%87%AA%E5%B7%B1%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/"},{"title":"初识7层模型和5层模型","text":"今天突然想知道在访问一个网站的时候那些请求头是在何时被加上去的。例如访问百度的时候，host和refer或者其他的字段是在七层模型的哪一层被加入的。 Wireahark抓包在这里尝试了下百度的抓包，但是百度的是是HTTPS的连接，导致一直找不到那个HTTP连接，所以还是放弃了，但是随后又找了一个非HTTPS的网站，进行抓包，然后测试了下，发现可以 解析抓包这是用wireshark抓取的结果：却发现这里多了一个Hypertext Transfer Protocol，在这里的话因为是GET请求，所以准备用POST请求再次尝试：发现其实内容都差不多，那也就证明了这些请求头都是在应用层被添加进去的。那么在这些抓包中可以看到他的层次是5层，而并非是7层模型。在这里的的五层模型分别是物理层,数据连续层,网络层,传输层,应用层。相较于7层的OSI模型少了会话层和表示层。 应用层其实与代码打交道的那一层是应用层。可以发现在Java代码中可以通过Response.setXXX来设置请求头或者回复头等。这也就从旁边来证明了我们所编写的程序其实都是在应用层。","link":"/2018/04/06/%E5%88%9D%E8%AF%867%E5%B1%82%E6%A8%A1%E5%9E%8B/"},{"title":"利用Jquery和Spring异步上传文件","text":"异步上传文件：用Jquery的异步上传文件的时候需要引入一个js文件jquery.form.min.js，用这个文件里面的$.ajaxSubmit()方法来实现一个异步的文件上传功能。 12345678910111213141516171819$(this).ajaxSubmit({ type:'POST', url: &quot;/uploadfile&quot;, dataType: 'json', data: serializeData, contentType: false, cache: false, processData:false, beforeSubmit: function() { }, uploadProgress: function (event, position, total, percentComplete){ }, success:function(){ }, error:function(data){ alert('上传图片出错'); } }); 在这里的话$(&quot;&quot;)函数需要是form的id，而且beforeSubmit可以在上传文件之前可以做一些检查，例如文件后缀或者文件大小之类的检查。 在后端的话接受上传的文件其实跟Servlet差不多，主要是从request中获取请求流，参数的话需要标记为这个 @RequestParam(&quot;file&quot;) MultipartFile file,最后在SpringMvc中有一个方法可以将上传的文件通过移动或者复制然后转移到我们指定的文件夹中： 1234567891011121314151617void transferTo(java.io.File dest) throws java.io.IOException, java.lang.IllegalStateExceptionTransfer the received file to the given destination file.This may either move the file in the filesystem, copy the file in the filesystem, or save memory-held contents to the destination file. If the destination file already exists, it will be deleted first.If the target file has been moved in the filesystem, this operation cannot be invoked again afterwards. Therefore, call this method just once in order to work with any storage mechanism.NOTE: Depending on the underlying provider, temporary storage may be container-dependent, including the base directory for relative destinations specified here (e.g. with Servlet 3.0 multipart handling). For absolute destinations, the target file may get renamed/moved from its temporary location or newly copied, even if a temporary copy already exists.Parameters:dest - the destination file (typically absolute)Throws:java.io.IOException - in case of reading or writing errorsjava.lang.IllegalStateException - if the file has already been moved in the filesystem and is not available anymore for another transferSee Also:FileItem.write(File), Part.write(String) 也就是说这个方法transferTo会将接收到的文件移动或者copy到指定的位置。那么在Controller中的主要方法体是： 12345678910111213141516171819@RequestMapping(value = &quot;/uploadfile&quot; ,method = RequestMethod.POST) @ResponseBody public String uploadfile( @RequestParam(&quot;file&quot;) MultipartFile file) throws IOException { Map&lt;String ,Object&gt; map =new HashMap&lt;&gt;(); /* 上传的文件为空 */ if(file == null || file.isEmpty()){ map.put(&quot;code&quot;,&quot;0&quot;); map.put(&quot;msg&quot;,&quot;fail&quot;); return JSON.toJSONString(map); }else{ String rootPath = &quot;D:\\\\disk&quot;; File dir = new File(rootPath + File.separator + &quot;tmpFiles&quot;); File serverFile = new File(dir.getAbsolutePath() + File.separator + file.getOriginalFilename()); file.transferTo(serverFile); map.put(&quot;code&quot;,&quot;1&quot;); map.put(&quot;msg&quot;,&quot;success&quot;); return JSON.toJSONString(map); } } 这个方法的话如果文件上传成功会返回一个json格式的信息，从而提示前端文件已经上传成功了。 生成验证码：在这里的话顺带在说下关于验证码的事情，在Spring里面使用验证码的话可以使用google的kaptcha这个jar包，因为在maven仓库中的话是没有这个版本的，所以这个需要自己下载到本地然后通过命令行导入，最后才可以在pom文件中引用这个包；导入方法如下：mvn install:install-file -Dfile=jar包位置 -DgroupId=com.google.code -DartifactId=kaptcha -Dversion=2.3.2 -Dpackaging=jar，这样在pom文件中便可以引用了。如下： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.code&lt;/groupId&gt; &lt;artifactId&gt;kaptcha&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;/dependency&gt; 同时在ApplicationContext.xml中添加一个bean. 123456789101112131415161718192021222324252627282930313233343536&lt;bean id=&quot;captchaProducer&quot; class=&quot;com.google.code.kaptcha.impl.DefaultKaptcha&quot;&gt; &lt;property name=&quot;config&quot;&gt; &lt;bean class=&quot;com.google.code.kaptcha.util.Config&quot;&gt; &lt;constructor-arg&gt; &lt;props&gt; &lt;!-- 图片边框 --&gt; &lt;prop key=&quot;kaptcha.border&quot;&gt;no&lt;/prop&gt; &lt;!-- 图片宽度 --&gt; &lt;prop key=&quot;kaptcha.image.width&quot;&gt;95&lt;/prop&gt; &lt;!-- 图片高度 --&gt; &lt;prop key=&quot;kaptcha.image.height&quot;&gt;45&lt;/prop&gt; &lt;!-- 验证码背景颜色渐变，开始颜色 --&gt; &lt;prop key=&quot;kaptcha.background.clear.from&quot;&gt;248,248,248&lt;/prop&gt; &lt;!-- 验证码背景颜色渐变，结束颜色 --&gt; &lt;prop key=&quot;kaptcha.background.clear.to&quot;&gt;248,248,248&lt;/prop&gt; &lt;!-- 验证码的字符 --&gt; &lt;prop key=&quot;kaptcha.textproducer.char.string&quot;&gt;0123456789abcdefghijklmnopqrstuvwxyz这是一个测试验证码的例子&lt;/prop&gt; &lt;!-- 验证码字体颜色 --&gt; &lt;prop key=&quot;kaptcha.textproducer.font.color&quot;&gt;0,0,255&lt;/prop&gt; &lt;!-- 验证码的效果，水纹 --&gt; &lt;prop key=&quot;kaptcha.obscurificator.impl&quot;&gt;com.google.code.kaptcha.impl.WaterRipple&lt;/prop&gt; &lt;!-- 验证码字体大小 --&gt; &lt;prop key=&quot;kaptcha.textproducer.font.size&quot;&gt;35&lt;/prop&gt; &lt;!-- 验证码字数 --&gt; &lt;prop key=&quot;kaptcha.textproducer.char.length&quot;&gt;4&lt;/prop&gt; &lt;!-- 验证码文字间距 --&gt; &lt;prop key=&quot;kaptcha.textproducer.char.space&quot;&gt;2&lt;/prop&gt; &lt;!-- 验证码字体 --&gt; &lt;prop key=&quot;kaptcha.textproducer.font.names&quot;&gt;new Font(&quot;Arial&quot;, 1, fontSize), new Font(&quot;Courier&quot;, 1, fontSize)&lt;/prop&gt; &lt;!-- 不加噪声 --&gt; &lt;prop key=&quot;kaptcha.noise.impl&quot;&gt;com.google.code.kaptcha.impl.NoNoise&lt;/prop&gt; &lt;/props&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; 最后在Controller添加如下方法： 1234567891011121314151617181920212223242526@RequestMapping(value = &quot;verificationcode&quot;, method = RequestMethod.GET) public ModelAndView Verificationcode(ModelAndView modelAndView, HttpServletRequest request, HttpServletResponse response, @RequestParam(value = &quot;timestamp&quot;, required = false) String timestamp) throws IOException { /* 添加时间戳，以设置验证码的过期时间*/ if (timestamp == null || timestamp.length() == 0) { modelAndView.addObject(&quot;timestamp&quot;, System.currentTimeMillis()); } else { modelAndView.addObject(&quot;timestamp&quot;, timestamp); } response.setDateHeader(&quot;Expires&quot;, 0); response.setHeader(&quot;Cache-Control&quot;, &quot;no-store, no-cache, must-revalidate&quot;); response.addHeader(&quot;Cache-Control&quot;, &quot;post-check=0, pre-check=0&quot;); response.setHeader(&quot;Pragma&quot;, &quot;no-cache&quot;); response.setContentType(&quot;image/jpeg&quot;); String capText = captchaProducer.createText(); request.getSession().setAttribute(Constants.KAPTCHA_SESSION_KEY, capText); BufferedImage bi = captchaProducer.createImage(capText); ServletOutputStream out = response.getOutputStream(); ImageIO.write(bi, &quot;jpg&quot;, out); try { out.flush(); } finally { out.close(); } return null; } 最后在前端页面引用 1&lt;img src=&quot;verificationcode&quot;&gt; //在这里随意设置宽度和高度即可。","link":"/2018/03/29/%E5%88%A9%E7%94%A8Jquery%E5%92%8CSpring%E5%BC%82%E6%AD%A5%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6/"},{"title":"单例模式引起的一些思考","text":"单例模式通常有饿汉式和懒汉式，懒汉式 饿汉式无线程安全性问题 123456789public class SingleHungryStyle { private static SingleHungryStyle instince = new SingleHungryStyle(); public static SingleHungryStyle getInstince(){ return instince; } } 懒汉式单线程下 demo1 1234567891011public class SingleLazyStyle { private static SingleLazyStyle instance = null; public static SingleLazyStyle getInstance(){ if(instance == null){ instance = new SingleLazyStyle(); } return instance; }} 上述代码在多线程的情况下会出现多个实例，所以需要进行一个加锁判断。 多线程下 demo2 123456789101112public class SingleLazy1Style { private static SingleLazy1Style instance = null; public static SingleLazy1Style getInstance(){ synchronized (SingleLazy1Style.class){ if(instance == null){ instance = new SingleLazy1Style(); } return instance; } }} 上述代码在多线程的情况下运行多次偶尔会出现一个问题，就是CPU的重排序会导致instance还未完全初始化就被使用了。 例如: 此时线程二就有可能报错，因为JVM在进行一个类的初始化的时候是分为三步的。 Java SE 8的JVM规范里面对一个类的加载进行了详细的描述：Java SE 8的JVM规范。具体来说就是分为三步： Loading Linking Initializing Creation and Loading在这一步，JVM需要判断需要初始化的类是数组还是一个普通类，如果是一个普通类的话，就再进行判断是需要使用bootstrap class loader来进行加载还是说用user-defined class loader进行加载。 Linking在这一步主要是验证和准备 Verification Preparation Resolution 而将未初始化的引用绑定到实例上就是Resolution。具体可以参考: Java SE 8的JVM规范 5.4.3 Initializing初始化，即将字段进行一个默认值初始化。 但是这里因为Linking和Initializing之间并无任何的关联性，所以可能会导致先进行了一个初始化，但是并未将该引用绑定到堆的一个实例上，而此时由轮到另一个线程执行。所以就会导致另一个线程获取的是空对象。 问题但是在这个初始化的过程中，Linking和Initializing之间由于互相不依赖，所以CPU可能会先进行初始化，但是并未进行关联，即将引用关联到JVM里面的一个实例。而直接返回了。此时由于happens-before原则并不能跨线程，所以会出现两种情况： 如果线程一在线程二之前使用了instance，此时线程二使用instance不会出现任何问题 如果线程一在初始化完毕之后释放了锁资源，然后线程二执行，因为线程二判断instance已经被初始化了（但此时实际上并未Linking），所以在使用的时候会报错。 但是这个时候，由于CPU的重排序，导致线程二获取的instance可能出现空指针异常; 所以一般为了避免这种情况，会加一个volatile关键字来禁止内存重排序。 demo3 123456789101112131415public class SingleLazy2Style { private volatile static SingleLazy2Style instance = null; public static SingleLazy2Style getInstance(){ if(instance == null) { synchronized (SingleLazy2Style.class) { if (instance == null) { instance = new SingleLazy2Style(); } return instance; } } return instance; }} volatile在这里之所以使用volatile的特性之一：防止内存进行重排序（包含写屏障和读屏障） happens-before原则Java SE 8中Happens-before原则 1234567891011Two actions can be ordered by a happens-before relationship. If one action happens-before another, then the first is visible to and ordered before the second.If we have two actions x and y, we write hb(x, y) to indicate that x happens-before y.If x and y are actions of the same thread and x comes before y in program order, then hb(x, y).There is a happens-before edge from the end of a constructor of an object to the start of a finalizer (§12.6) for that object.If an action x synchronizes-with a following action y, then we also have hb(x, y).If hb(x, y) and hb(y, z), then hb(x, z) happens-before原则要求在一个线程内，重排序后执行的结果与未重排序之前的执行结果必须一致。所以在demo1中单线程里面，是不会出现任何问题的，因为即使发生重排序，最后在使用instance的时候，instance也一定会完成初始化，否则就是编译器bug了。 但是在多线程的情况下，happens-before原则无法生效，所以就会导致其他线程在获取实例的时候会出现异常。 所以在多线程的情况下需要使用volatile关键字进行修饰，主要是因为需要确保首个初始化的线程必须完成整个类的初始化的操作。","link":"/2019/05/04/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E5%BC%95%E8%B5%B7%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"title":"在Angular中使用Bootstrap","text":"在去年的时候短暂的接触了大概一个星期的 Angular 之后就再也没碰过了，今天突然想重新捡起 Angular 的相关知识，并且想将 Angular 结合 Bootstrap 一起使用。所以正好记录下一起结合使用的步骤。 初始化一个 Angular 的项目。初始化之后打开命令行，输入： 12npm install jquery --save-devnpm install bootstrap --save-dev 输入以上两条命令之后，在 package.json 中可以看到已经多出了 jquery 和 bootstrap 这两个库了。如下： 当初始化完成之后会在 node_modules 中出现 bootstrap 和 jquery 这两个文件夹。 然后再将需要引入的 css 文件和 js 文件加入到 .angular-cli.json中，如下： 最后再执行如下命令，将 JS 文件转化为 TypeScript 文件。 123npm install @types/jquery --save -devnpm install @types/bootstrap --save -dev 出错解决如果安装完成之后提示的错误如下： 1ERROR in ./node_modules/css-loader?{&quot;sourceMap&quot;:false,&quot;importLoaders&quot;:1}!./node_modules/postcss-loader/lib?{&quot;ident&quot;:&quot;postcss&quot;,&quot;sourceMap&quot;:false}!./node_modules/bootstrap/dist/css/bootstrap.min.css Module build failed: BrowserslistError: Unknown browser major at error 此时需要将 node_modules 里面的 bootstrap 文件夹中的 package.json 中的 &quot;last 1 major version&quot;,&quot;&gt;= 1%&quot; 删除。然后 输入 npm start就可以了。","link":"/2018/05/29/%E5%9C%A8Angular%E4%B8%AD%E4%BD%BF%E7%94%A8Bootstrap/"},{"title":"在Mybatis中使用bind进行枚举和模糊查询","text":"首先在网上查询了下关于bind得用法，网上大多数都是bind和模糊查询绑定在一起，但是在这里的话其实bind和枚举一起结合起来使用会有很大的便利，比如一个班级的名称和班级的ID，需要根据班级的ID查询出班级的姓名的话，一般在mybatis中的sql语句是select * from table where class_id =#{class_id} 但是这样就有一个问题，假设学校现在系统升级，每一个班级的ID都变了，这时候需要到处修改mybatis的参数，将其修改成为正确的ID,那么这是一个浩大的工程，同时如果以后再需要改的话，会比较麻烦。处理这个问题，这个时候有如下方法： 枚举和typehandle组合解决问题； 枚举和bind一起组合解决； 暂时没想到 ##枚举和bind组合解决：新建一个实体类： 1234567891011121314151617181920212223242526public class clazzEntity { private String clazz_name; private int clazz_id; public clazzEntity(String clazz_name, int clazz_id) { this.clazz_name = clazz_name; this.clazz_id = clazz_id; } public String getClazz_name() { return clazz_name; } public void setClazz_name(String clazz_name) { this.clazz_name = clazz_name; } public int getClazz_id() { return clazz_id; } public void setClazz_id(int clazz_id) { this.clazz_id = clazz_id; }} 新建一个枚举类： 123456789101112131415161718192021222324252627public enum clazz { CLASS_ONE(&quot;一年级&quot;,2018001),CLASS_TWO(&quot;一年级&quot;,2018002),CLASS_THREE(&quot;一年级&quot;,2018003),CLASS_FOUR(&quot;一年级&quot;,2018004),CLASS_FIVE(&quot;一年级&quot;,2018005); private String className; private int classId; clazz(String className, int classId) { this.className = className; this.classId = classId; } public String getClassName() { return className; } public void setClassName(String className) { this.className = className; } public int getClassId() { return classId; } public void setClassId(int classId) { this.classId = classId; }} 新建一个mapper文件： 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;study.dao.clazzEntityDao&quot;&gt; &lt;resultMap id=&quot;clazzmap&quot; type=&quot;study.entity.clazzEntity&quot;&gt; &lt;result property=&quot;clazz_name&quot; column=&quot;clazz_name&quot;&gt;&lt;/result&gt; &lt;result property=&quot;clazz_id&quot; column=&quot;clazz_id&quot;&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;select id=&quot;findAllCLass&quot; resultMap=&quot;clazzmap&quot;&gt; &lt;bind name=&quot;_CLASS&quot; value=&quot;@study.entity.clazz@CLASS_ONE.className&quot;/&gt; select * from clazzentity where clazz_name = #{_CLASS} &lt;/select&gt;&lt;/mapper&gt; 在这里的的那个&lt;bind name=&quot;_CLASS&quot; value=&quot;@study.entity.clazz@CLASS_ONE.className&quot;/&gt;代表的是已经绑定了这个枚举一年级了，而且我们在参数里面无论输入什么都不会再修改到这个sql的值了，可以做如下测试： 1234567891011121314151617public static void main(String args[]) { Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); clazzEntityDao clazzEntityDao = sqlSession.getMapper(clazzEntityDao.class); List&lt;clazzEntity&gt; list =clazzEntityDao.findAllCLass(clazz.CLASS_TWO.getClassName()); for( clazzEntity clazzEntity :list){ System.out.println(clazzEntity.getClazz_id()); } sqlSession.commit(); } finally { sqlSession.close(); } } 可以发现我这里已经将参数修改为了二年级的了，但是查询结果还是会显示的是一年级的. 123456DEBUG [main] - ==&gt; Preparing: select * from clazzentity where clazz_name = ? DEBUG [main] - ==&gt; Parameters: 一年级(String)TRACE [main] - &lt;== Columns: clazz_name, clazz_idTRACE [main] - &lt;== Row: 一年级, 2018001DEBUG [main] - &lt;== Total: 12018001 在这里其实已经可以不需要任何参数了，但是为了测试用法还是加上去了，这样的写法在以后的作用就是省的去改动大量的sql，若以后一年级的ID变了的话，那么我们只需要修改枚举的值而不需要对sql做很多的改变，这就是第一种方法，即枚举加上bind一起使用以减少sql语句的改动。 枚举加上typehandle明天研究下补充","link":"/2018/04/01/%E5%9C%A8Mybatis%E4%B8%AD%E4%BD%BF%E7%94%A8bind%E8%BF%9B%E8%A1%8C%E6%9E%9A%E4%B8%BE%E5%92%8C%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2/"},{"title":"在Mybatis中使用association进行一对一查询","text":"在今天主要是测试了下在mybatis中使用两种方式来进行一对一查询。在mybatis中进行普通查询的话肯定是一个JavaBean对应一个Sql语句，但是当需要进行两表或者多表之间一对一的查询的时候就需要使用mybatis中的association进行一对一查询，而association的设置一般有两种方式： ##基础类：员工类： 1234567891011121314151617181920212223242526272829public class People implements Serializable { private int people_id; private String people_card; private Role role; public int getPeople_id() { return people_id; } public void setPeople_id(int people_id) { this.people_id = people_id; } public String getPeople_card() { return people_card; } public void setPeople_card(String people_card) { this.people_card = people_card; } public Role getRole() { return role; } public void setRole(Role role) { this.role = role; }} 权限类： 1234567891011121314151617181920212223242526272829public class Role implements Serializable { private int myrole_id; private String role_name; private RoleDetail roleDetail; public RoleDetail getRoleDetail() { return roleDetail; } public void setRoleDetail(RoleDetail roleDetail) { this.roleDetail = roleDetail; } public int getMyrole_id() { return myrole_id; } public void setMyrole_id(int myrole_id) { this.myrole_id = myrole_id; } public String getRole_name() { return role_name; } public void setRole_name(String role_name) { this.role_name = role_name; }} 权限描述类： 1234567891011121314151617181920public class RoleDetail implements Serializable{ private int rd_id; private String rd_detail; public int getRd_id() { return rd_id; } public void setRd_id(int rd_id) { this.rd_id = rd_id; } public String getRd_detail() { return rd_detail; } public void setRd_detail(String rd_detail) { this.rd_detail = rd_detail; }} 在这里面的话，员工和权限默认是一对一的关系(这里仅仅是为了测试，现实中肯定是一对多的关系)，权限和权限描述是一对一的关系： 使用进行配置：假设现在有一个需求就是需要查询出一个人的信息以及其对应的权限，这时在数据库中就是一个左连接和右连接的事情，但是在Mybatis中因为是一个JavaBean和数据库相映射，所以，此时就需要一个一对一查询：新建一个RoleDao的xml文件 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;study.dao.RoleDao&quot;&gt; &lt;/mapper&gt; 由于是需要在查询员工的时候顺带将其权限也查出来，所以这个时候需要在people的xml文件中使用association: 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;study.dao.PeopleDao&quot;&gt; &lt;resultMap type=&quot;study.entity.People&quot; id=&quot;people&quot;&gt; &lt;id property=&quot;people_id&quot; column=&quot;people_id&quot;/&gt; &lt;result property=&quot;people_card&quot; column=&quot;people_card&quot;/&gt; &lt;association property=&quot;role&quot; javaType=&quot;study.entity.Role&quot; &gt; &lt;id property=&quot;myrole_id&quot; column=&quot;myrole_id&quot;&gt;&lt;/id&gt; &lt;result property=&quot;role_name&quot; column=&quot;role_name&quot;&gt;&lt;/result&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;select id=&quot;getPeopleCard&quot; parameterType=&quot;int&quot; resultMap=&quot;people&quot;&gt; select p.people_id,p.people_card,m.* from people p, mybatisrole m where people_id=#{role_id} and p.role_id =m.myrole_id &lt;/select&gt; &lt;/mapper&gt; &lt;!-- select p.people_id,p.people_card,m.* from people p, mybatisrole m where people_id=#{role_id} -- &gt; 注意上面association中的property，这是people中的一个属性，而javaType则代表的是这个是一个什么类型；测试查看结果: 1234567891011121314151617181920public class MybatisExample { public static void main(String args[]) { Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); PeopleDao peopleDao = sqlSession.getMapper(PeopleDao.class); List&lt;People&gt; list = peopleDao.getPeopleCard(1); for(People p : list) { System.out.println(p.getRole().getRole_name()); } sqlSession.commit(); } finally { sqlSession.close(); } }} 打印结果: DEBUG [main] - ==&gt; Preparing: select p.people_id,p.people_card,m.* from people p, mybatisrole m where people_id=?DEBUG [main] - ==&gt; Parameters: 1(Integer)TRACE [main] - &lt;== Columns: people_id, people_card, myrole_id, role_nameTRACE [main] - &lt;== Row: 1, qw, 1, aDEBUG [main] - &lt;== Total: 1a 当使用如上这种配置的时候会执行2次不同的sql：RoleDetailmapper.xml配置文件： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;study.dao.RoleDetailDao&quot;&gt; &lt;resultMap id=&quot;getRd_detail&quot; type=&quot;study.entity.RoleDetail&quot;&gt; &lt;id property=&quot;rd_id&quot; column=&quot;rd_id&quot;&gt;&lt;/id&gt; &lt;result property=&quot;rd_detail&quot; column=&quot;rd_detail&quot;&gt;&lt;/result&gt; &lt;/resultMap&gt;&lt;select id=&quot;getDetailById&quot; parameterType=&quot;int&quot; resultType=&quot;study.entity.RoleDetail&quot;&gt; SELECT * from roledetail where rd_id=#{id}&lt;/select&gt;&lt;/mapper&gt; 然后再Rolemapper.xml中调用它： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;study.dao.RoleDao&quot;&gt; &lt;resultMap type=&quot;study.entity.Role&quot; id=&quot;Role&quot;&gt; &lt;result property=&quot;myrole_id&quot; column=&quot;myrole_id&quot;/&gt; &lt;result property=&quot;role_name&quot; column=&quot;role_name&quot;/&gt; &lt;association property=&quot;roleDetail&quot; column=&quot;rd_id&quot; select=&quot;study.dao.RoleDetailDao.getDetailById&quot;&gt; &lt;id property=&quot;myrole_id&quot; column=&quot;rd_id&quot;&gt;&lt;/id&gt; &lt;result property=&quot;rd_detail&quot; column=&quot;rd_detail&quot;&gt;&lt;/result&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;!--select * from mybatisrole m ,roledetail r where m.myrole_id=#{id} and m.myrole_id=r.rd_id--&gt; &lt;select id=&quot;getRoleById&quot; parameterType=&quot;int&quot; resultMap=&quot;Role&quot;&gt; select * from mybatisrole m ,roledetail r where m.myrole_id=#{id} &lt;/select&gt; &lt;/mapper&gt; 在这个地方的话association后跟的是column和select属性，所以在执行的时候先执行getRoleById,再执行getDetailById: 123456789101112131415161718public class MybtisRoleExample { public static void main(String args[]) { Logger logger = null; logger = Logger.getLogger(MybatisExample.class.getName()); logger.setLevel(Level.DEBUG); SqlSession sqlSession = null; try { sqlSession = study.mybatis.MybatisUtil.getSqlSessionFActory().openSession(); RoleDao roleDao= sqlSession.getMapper(RoleDao.class); List&lt;Role&gt; list= roleDao.getRoleById(1); for(Role r : list) { System.out.println(r.getRoleDetail().getRd_detail()); } } finally { sqlSession.close(); } }} 打印日志如下： DEBUG [main] - ==&gt; Preparing: select * from mybatisrole m ,roledetail r where m.myrole_id=?DEBUG [main] - ==&gt; Parameters: 1(Integer)TRACE [main] - &lt;== Columns: myrole_id, role_name, rd_id, rd_detailTRACE [main] - &lt;== Row: 1, a, 1, 主要权限负责人DEBUG [main] - ====&gt; Preparing: SELECT * from roledetail where rd_id=?DEBUG [main] - ====&gt; Parameters: 1(Integer)TRACE [main] - &lt;==== Columns: rd_id, rd_detailTRACE [main] - &lt;==== Row: 1, 主要权限负责人DEBUG [main] - &lt;==== Total: 1DEBUG [main] - &lt;== Total: 1主要权限负责人 可以看到执行了2条sql； 总结：虽然可以进行一对一查询了，但是不知道问什么两种配置会导致执行不同次数的sql，官方文档的解释是一个 Java 类的完全限定名,或一个类型别名(参考上面内建类型别名的列表)。 如果你映射到一个 JavaBean,MyBatis 通常可以断定类型。然而,如 果你映射到的是 HashMap,那么你应该明确地指定 javaType 来保证期望的 行为。但是却没说为啥或者怎样映射得。","link":"/2018/03/31/%E5%9C%A8Mybatis%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%B7%A6%E8%BF%9E%E6%8E%A5%E6%88%96%E8%80%85%E5%8F%B3%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/"},{"title":"在Mysql中使用变量进行复杂的查询(一)","text":"注意，此文章使用的Mysql变量均是用户变量 简介首先看一个需求，有如下数据表： 12345678910111213mysql&gt; select * from t1 order by area_id;+---------+-----------+| area_id | order_num |+---------+-----------+| 1 | 22 || 1 | 10 || 1 | 10 || 2 | 10 || 2 | 10 || 2 | 22 || 3 | 10 |+---------+-----------+7 rows in set (0.14 sec) 可以看到，这是一个很非常普通的数据表。 需求一假设某一次运营需要统计每一个地区的销量量，让你出一个报表给他，那么方法如下： 方法一统计每一个地区的总销量：常用SQL如下： 12345678910mysql&gt; select t1.area_id, sum(t1.order_num) from t1 group by area_id;+---------+-------------------+| area_id | sum(t1.order_num) |+---------+-------------------+| 1 | 42 || 2 | 42 || 3 | 10 |+---------+-------------------+3 rows in set (0.14 sec) 从图中可以看到，该SQL已经可以把每个地区的销售总量全部统计完毕了。但是有一天，需求又变了。 需求二：现在其他的运营也过来了，说需要按照区域的Id做为优先级，依次统计每一个优先级地区的总销量。即如下： 地区1的销量为42，由于没有地区Id为0的，所以地区1的销量是42。 地区2的销量为42，但是由于地区2的优先级高于1，所以地区2的销量需要包含地区1的销量 地区3的销量为42，但是由于地区3的优先级高于1和2，所以地区3的销量需要包含地区1的销量和地区2的销量 此时你会发现，虽然可以进行groupby，但是却无法统计之前的累加和，此时Mysql变量就可以使用了 方法一:1234567891011121314mysql&gt; SELECT r1.area_id, @t_total := @t_total + r1.total AS 't_total' FROM ( SELECT @t_total := 0 ) r, ( SELECT t1.area_id, sum( t1.order_num ) AS 'total' FROM t1 GROUP BY area_id ) r1 order by r1.area_id;+---------+---------+| area_id | t_total |+---------+---------+| 1 | 42 || 2 | 84 || 3 | 94 |+---------+---------+3 rows in set (0.18 sec) 由于需求是需要高一级的地区需要统计所以低一级地区的销量，所以此时这个需求已经完成了。 此时查看SQL的效果，会发现，地区2的销量已经包含了地区1的，而地区3的销量已经包含了地区1和地区2的，所以该条SQL已经符合运营的需求了。 Mysql变量的初步使用在上面的这条SQL中，可以看到出现了@t_total这个变量，同时也出现了:=操作符，这就是Mysql的变量。 需求三现在运营发现，这样统计的话有点问题，需要进行调整下： 地区1的销量为42，不能包含自身的销量，于是只能统计比1小的地区，所以地区1的销量是0。 地区2的销量为42，不能包含自身的销量，于是只能统计比2小的地区，所以地区1的销量是42。 地区2的销量为10，不能包含自身的销量，于是只能统计比3小的地区，所以地区3的销量是84。 方法一12345678910111213141516mysql&gt; SELECT r1.area_id, @t_total := @t_total + r1.total AS 't_total', @before_total := @t_total - r1.total AS 'before_total' FROM ( SELECT @t_total := 0 ) r, ( SELECT @before_total := 0 ) rr, ( SELECT t1.area_id, sum( t1.order_num ) AS 'total' FROM t1 GROUP BY area_id ) r1;+---------+---------+--------------+| area_id | t_total | before_total |+---------+---------+--------------+| 1 | 42 | 0 || 2 | 84 | 42 || 3 | 94 | 84 |+---------+---------+--------------+3 rows in set (0.16 sec) 此时会看到before_total字段完美的符合和需求三。 Mysql变量通过需求二和需求三可以看到Mysql的变量使用技巧，在Mysql中，使用变量可以实现很多复杂的需求。但是在使用Mysql变量的时候需要注意，用户变量需要配合Select一起来使用。","link":"/2019/02/27/%E5%9C%A8Mysql%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%8F%98%E9%87%8F%E8%BF%9B%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%9A%84%E6%9F%A5%E8%AF%A2(%E4%B8%80)/"},{"title":"在SpringMvc中使用shiro进行安全配置","text":"前言在去年一年之间用过shiro的一些内容，但是最近又有点忘却了。现在正好有一个机会，所以正好搭建起来了然后自己做些记录 搭建过程引入jar包：需要使用shiro的话先在maven中引入以下Jar包： 123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.shiro/shiro-spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.shiro/shiro-web --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.shiro/shiro-ehcache --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; 编写方法体：在此之前需要加入以上jar包；当引入之后便可以进行spring和shiro的构建了。在进行搭建之前需要建立两一个Realm，一个shiro的核心控制器。需要继承AuthorizingRealm然后实现两个方法： 在doGetAuthenticationInfo(AuthenticationToken authenticationToken)方法中：authenticationToken参数是从前端页面接受的一个参数，里面封装的是一个token，该token是从前端获取的，具体的获取方法是 12Subject subject = SecurityUtils.getSubject();UsernamePasswordToken token = new UsernamePasswordToken(username, password); 在这段代码中的Controller获取到前端的用户名和密码之后在new UsernamePasswordToken(username, password)这里会生成一个token，然后这个token会被传到realm中，最后在realm中的doGetAuthenticationInfo接收到，然后在这里便可以进行数据库查询，要么返回异常，要么则是返回一个SimpleAuthenticationInfo。这个代码如下： 1234567891011String username = (String) authenticationToken.getPrincipal(); // 获取用户名String password = new String((char[])authenticationToken.getCredentials()); //得到密码Map&lt;String ,String &gt; map =new HashMap&lt;String, String&gt;();map.put(&quot;username&quot; ,username);map.put(&quot;password&quot;,password);User user =loginservice.loginCheck(map);if (user == null || user.getId() == null){ throw new UnknownAccountException(); //如果用户名错误}else{ return new SimpleAuthenticationInfo(username, password, &quot;myRealm&quot;);} 上面的代码只是简单的进行了数据库查询然后封装为一个token。然后返回即可，那么对用户的权限进行操作的就是如下这个代码： doGetAuthorizationInfo这个是进行权限分配的方法：具体的方法体如下: 1234567891011 protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { String username = (String) principalCollection.getPrimaryPrincipal(); //通过principalCollection查询到用户的姓名， List&lt;Resources&gt; resources =loginservice.getRoleById(username); 通过mybatis配置sql List&lt;String&gt; roles =new ArrayList&lt;String&gt;(); // 获取url for (Resources r: resources){ roles.add(r.getRole()); } SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addRoles(roles); //存放权限 return info;} 这里应该是还可以放一个权限，后面几篇日志在加上 ApplicationContext.xml的配置：123456789101112131415161718192021222324252627282930313233343536373839&lt;!-- 配置Shiro --&gt; &lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt; &lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot; depends-on=&quot;lifecycleBeanPostProcessor&quot;/&gt; &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; &lt;!-- securityManager 核心控制器 --&gt; &lt;property name=&quot;realm&quot; ref=&quot;myRealm&quot;/&gt; &lt;!-- 配置realm --&gt; &lt;/bean&gt; &lt;!-- 过滤器配置, 同时在web.xml中配置filter --&gt; &lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;property name=&quot;loginUrl&quot; value=&quot;/&quot;/&gt; &lt;property name=&quot;successUrl&quot; value=&quot;/login/main.jsp&quot;/&gt; &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/login/TestPage&quot;/&gt; &lt;property name=&quot;filters&quot;&gt; &lt;map&gt; &lt;entry key=&quot;authc&quot;&gt; &lt;bean class=&quot;org.apache.shiro.web.filter.authc.PassThruAuthenticationFilter&quot; /&gt; &lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=&quot;filterChainDefinitions&quot;&gt; &lt;value&gt; /user/** = authc &lt;!-- 需要认证通过, 即登录成功 --&gt; /user/insert = authc,roles[ADMIN] /user/getUser = authc,roles[MANAGER] /user/toinsert = authc,roles[ADMIN] /role/admin = authc,roles[ADMIN] /role/manager = authc,roles[SUPERMANAG] /role/manager = authc,roles[MANAGER] &lt;!--/blog/**.do = authc,perms[blog] &amp;lt;!&amp;ndash; 需要名称为blog的权限permission&amp;ndash;&amp;gt;--&gt; &lt;!--/admin/*.do = authc,roles[admin] &amp;lt;!&amp;ndash; 需要名称为admin的角色role&amp;ndash;&amp;gt;--&gt; &lt;!-- 说明: /*匹配的的是/abc; /** 匹配的是多个/*, 比如/abc/def --&gt; &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt;","link":"/2018/03/23/%E5%9C%A8SpringMvc%E4%B8%AD%E4%BD%BF%E7%94%A8shiro%E8%BF%9B%E8%A1%8C%E5%AE%89%E5%85%A8%E9%85%8D%E7%BD%AE/"},{"title":"在Spring中使用JWT生成token来验证用户","text":"首先JWT全程是 JSON WEB TOKEN 与Spring进行一个整合：获取JWT首先需要在pom中引入几个需要的jar包： 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; 另外这里也需要shiro的几个核心jar，这里就不写出来了。 jar包添加完毕之后便可以编写加密的方法了： 1234567byte[] apiKeySecretBytes = DatatypeConverter.parseBase64Binary(&quot;somersames&quot;);SignatureAlgorithm sigAlg = SignatureAlgorithm.HS256;Key signingKey = new SecretKeySpec(apiKeySecretBytes, sigAlg.getJcaName());JwtBuilder builder = Jwts.builder() .setSubject(username) .signWith(sigAlg, signingKey);String result =builder.compact(); 在这里需要解释下的就是对输入的Key进行一个Base64加密，然后将获取到的密匙和SignatureAlgorithm指定的一个加密算法进行加密，文章里面是HS256加密算法，最后生成一个Key。当获取到这个key之后通过JwtBuilder来进行生成一个JwtBuilder，最后在调用compact()从而可以获得一个JWT JWT验证用户：这一步就是需要和spring来打交道了，关于生成的token返回给前端之后到底储存在哪里，下午查了下目前有两种解决办法，一种是存在session里面，然后设置过期时间，另外一种则是存储在localstorage, 对比这两种存储方式，发现localstorage在客户端的话任何人都可以获取，所以最后还是考虑了通过sesison来存储TOKEN。 新建Spring的控制器： 1234567891011121314151617181920212223242526272829303132333435363738@RequestMapping(value = &quot;/provicer&quot;) @ResponseBody public String jwtTest(@RequestParam(value = &quot;username&quot;,required = false)String username, @RequestParam(value = &quot;password&quot;,required = false)String password, HttpServletRequest request,HttpServletResponse response ){ if (username == null || password == null) { username = &quot;zhangsan&quot;; Cookie[] cookie = request.getCookies(); if (cookie != null) { for (Cookie c : cookie) { if (c.getName().equals(&quot;token&quot;)) { if (decodeCookies(&quot;zheshijiakey&quot;, c.getValue()).equals(&quot;zhangsan&quot;)) { return &quot;OK&quot;; } } } } } byte[] apiKeySecretBytes = DatatypeConverter.parseBase64Binary(&quot;zheshijiakey&quot;); SignatureAlgorithm sigAlg = SignatureAlgorithm.HS256; Key signingKey = new SecretKeySpec(apiKeySecretBytes, sigAlg.getJcaName()); System.out.println(signingKey); System.out.println(username); JwtBuilder builder = Jwts.builder() .setSubject(username) .signWith(sigAlg, signingKey); String result =builder.compact(); Cookie cookie =new Cookie(&quot;token&quot;,result); response.addCookie(cookie); return result; } public static String decodeCookies(String Key , String jwt){ Jws&lt;Claims&gt; jws = Jwts.parser().setSigningKey(Key).parseClaimsJws(jwt); return jws.getBody().getSubject(); } 在这里仅仅是作为一个测试，所以并没有真正的查询数据库，在这里默认一个用户名叫zhangsan ,代码就是判断用户是否登陆，若未登录就生成一个包含usename为’zhangsan’的JWTtoken，然后通过response返回给前端，若在前端请求的HTTP连接中通过cookies获取到了这个token，那么就会进行一个解析。这里进行解析的时候若发现是非法token的话，最好进行一个try-catch然后返回给前端某些约定的错误码然后跳转会登陆界面。测试结果:首次登陆如下： 然后当登陆一次之后便会生成一个token，最后在第二次请求的时候便会返回OK","link":"/2018/03/30/%E5%9C%A8Spring%E4%B8%AD%E4%BD%BF%E7%94%A8JWT%E7%94%9F%E6%88%90token%E6%9D%A5%E9%AA%8C%E8%AF%81%E7%94%A8%E6%88%B7/"},{"title":"在Spring中全局处理异常","text":"随着业务的发展，在每一个业务模块里面都可能会出现一些自定义的通用异常，例如·验证失败，权限不足等等 ，这些自定义的异常很可能会被所有模块公用，在代码里面，最常见的一种写法是在每一个方法里面进行捕获，然后在Controller里面进行catch，最后进行相应处理 常见写法第一种写法： 这是一个常规的写法，每一个方法都处理自己的特定异常Controller层 1234567891011public void login(){ try{ //逻辑处理 }catch(AuthException e){ XXX }catch(CrsfException e){ XXX }catch(Exception e){ XXX }} 这样的代码如果分布在不同的Controller里面，将会是一种隐患。例如，有一天，需要对所有的AuthException异常都添加一个字段，用于前端的页面展示，那么此时我们就需要在代码里面找出所有的AuthException，然后再添加一些特殊的字段，如果漏掉了几个，就会引起一些bug。 第二种写法： 第二种写法几乎和第一种一样，不过不同之处在于第二种写法是编写了一个公共的处理方法. Controller层 12345678910111213@RequestMapping(value=&quot;login&quot;,methods=Request.POST)public void login(){ try{ //逻辑处理 }catch(AuthException e){ ExceptionHandle.handleAuthException(); }catch(CrsfException e){ XXX }catch(Exception e){ XXX }} 共用方法 12345public class ExceptionHandle(){ public static Object handleAuthException(){ XXX//逻辑处理 }} 这种方法虽然比第一种更加具有共用性，但是代码一点都不整洁和便于维护。例如，现在需要再次加一个异常，那么就只能是在Controller里面再次Catch，如果是增加还好，但是一旦需要里面既包含增加又包含删除，对于维护人员，这是极易出错的。 Spring的全局处理异常其实在Spring里面有更加优雅的处理方式，那就是全局的异常处理，对于一些常用的异常，直接在Controller里面抛出，而对于某一些方法的特定异常，则只需要自己进行捕获，然后自己进行处理 介绍在Spring里面可以使用@RestControllerAdvice或@ControllerAdvice，然后配合@ExceptionHandler进行处理，这样处理可以使的项目在整个异常处理这块十分的通用和优雅 异常处理类 1234567891011121314public class AuthException extends RuntimeException { public AuthException() { super(); } public AuthException(String message) { super(message); } public AuthException(String message, Exception e) { super(message, e); }} 全局的异常处理类 12345678910111213@RestControllerAdvicepublic class ExceptionHandle { @ExceptionHandler(AuthException.class) public ResponseEntity&lt;String&gt; handleAuthException(){ ResponseEntity&lt;String&gt; resp = new ResponseEntity&lt;&gt;(); resp.setCode(201); resp.setMessage(&quot;验证失败&quot;); resp.setData(&quot;全局异常所抛出的异常&quot;); return resp; }} Controller 123456789101112131415161718@RestController@RequestMapping(value = &quot;api&quot;)public class ExceptionController { @Autowired ExceptionService exceptionService; @RequestMapping(value = &quot;auth&quot;,method = RequestMethod.POST) public ResponseEntity&lt;String&gt; testAuth(@RequestBody String param) throws AuthenticationException { ResponseEntity&lt;String&gt; resp = new ResponseEntity&lt;String&gt;(); exceptionService.auth(param); resp.setCode(200); resp.setMessage(&quot;OK&quot;); resp.setData(&quot;Controller消息&quot;); return resp; }} Service 123456789@Servicepublic class ExceptionService { public void auth(String param) throws AuthenticationException { if(&quot;1&quot;.equalsIgnoreCase(param)){ throw new AuthException(&quot;非法访问&quot;); } }} 测试如下：请求api/auth，并且携带参数1 12345{ &quot;data&quot;: &quot;全局异常所抛出的异常&quot;, &quot;code&quot;: 201, &quot;message&quot;: &quot;验证失败&quot;} 请求api/auth，并且携带参数2 12345{ &quot;data&quot;: &quot;Controller消息&quot;, &quot;code&quot;: 200, &quot;message&quot;: &quot;OK&quot;} 增加特殊处理这样一来，所有的AuthException都可以被统一的进行处理，而且根据业务的需要们可以在Controller增加一些特定的异常 此处以NullPointerException代替 123456789101112131415161718192021222324@RestController@RequestMapping(value = &quot;api&quot;)public class ExceptionController { @Autowired ExceptionService exceptionService; @RequestMapping(value = &quot;auth&quot;,method = RequestMethod.POST) public ResponseEntity&lt;String&gt; testAuth(@RequestBody String param) throws AuthenticationException { ResponseEntity&lt;String&gt; resp = new ResponseEntity&lt;String&gt;(); try { exceptionService.auth(param); }catch (NullPointerException e){ resp.setCode(400); resp.setMessage(&quot;NUll&quot;); resp.setData(&quot;Null&quot;); return resp; } resp.setCode(200); resp.setMessage(&quot;OK&quot;); resp.setData(&quot;Controller消息&quot;); return resp; }} 12345678910111213@Servicepublic class ExceptionService { public void auth(String param) throws AuthenticationException { if(&quot;1&quot;.equalsIgnoreCase(param)){ throw new AuthException(&quot;非法访问&quot;); } if(&quot;3&quot;.equals(param)){ throw new NullPointerException(); } }} 再次测试请求api/auth，并且携带参数3 12345{ &quot;data&quot;: &quot;Null&quot;, &quot;code&quot;: 400, &quot;message&quot;: &quot;NUll&quot;} 总结此方法虽然可以统一的处理项目里面的异常，但是对项目内的开发人员要求还是比较高的，需要一起遵守统一的开发规范","link":"/2019/01/10/%E5%9C%A8Spring%E4%B8%AD%E5%85%A8%E5%B1%80%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8/"},{"title":"在Vue中使用filters来进行字典值的转换","text":"在Vue里面，经常会遇到一些字典值的转换，而这些字典值由于和后端进行了约定的，一般不会轻易的改变，所以在前后端开发的项目中，这种字典值最好的做法是前端独立的保存一份，自己在前端自行进行处理。 我们的做法是使用Vuex的store配合filters来进行前端的字典值转化，首先是在store里面将字典值进行固定，然后通过filters在页面中进行一个转换。 使用Store在Vue里面使用store首先需要安装vuex，安装完毕之后就可以直接在main.js里面直接引用了，但是为了统一管理还是决定新建一个store文件夹，然后将store相关的文件全部统一存放，新建完毕之后项目结构如下： 1234---App.vue---main.js---store-----index.js 在新建的index.js里面将Vuex实例注入到Vue中，如下: 12345678910111213141516171819import Vue from 'vue';import Vuex from 'vuex';Vue.use(Vuex);const store = new Vuex.Store({ state: { score: [ 60, 80, 100 ], enum: { 60: '及格', 80: '良好', 100: '优秀' } }});export default store; 在这里我定义了两个变量，一个是score，一个是enum，score主要是为了展示一些固定的值在前端的展示，而enum则是准备介绍filter的使用 新建一个Vue页面在这个页面里面，主要是介绍store的直接使用 12345678910111213141516171819202122&lt;template&gt;&lt;div&gt; &lt;div v-for=&quot;(item, index) of score&quot; :key=&quot;index&quot;&gt; {{item}} &lt;/div&gt;&lt;/div&gt;&lt;/template&gt;&lt;script&gt;import {mapState} from 'vuex';import Vue from 'vue';export default { computed: { ...mapState(['score']) }, }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 然后查看页面，就会发现页面上已经出现了三个分数，分别是在store里面定义的60，80，100。这种方式是通过...mapState来获取的state里面定义的一些值。 通过这种方式有几种好处，第一就是当需要改变前端某一个字段的值的时候，则可以直接通过store从而减少对项目的改动，其二就是可以让前端项目更加规范、可扩展。 使用filter来进行一些值的处理为了大大提高前端的可扩展性，通过会对一些固定的值进行转换。例如性别，后端可能会返回 0 或者 1，若前端在某些页面上需要显示为男|女，而在某一些页面上需要显示先生|女士，此时通过filter来进行处理，则是一个不错的选择。 新建一个文件夹和js文件123456---App.vue---main.js---store-----index.js---utils-----filter.js filter.js 123456import store from '../store/index';export function enumConvert (val) { return store.state.enum[val];} main.js 12345import * as filters from './utils/filter';Object.keys(filters).forEach((key) =&gt; { Vue.filter(key, filters[key]);}); 在这里通过Vue.filter将filter方法进行全局注册，然后在Vue页面进行使用 1234567891011121314151617181920212223242526&lt;template&gt;&lt;div&gt; &lt;div v-for=&quot;(item, index) of score&quot; :key=&quot;index&quot;&gt; {{item | convert}} &lt;/div&gt;&lt;/div&gt;&lt;/template&gt;&lt;script&gt;import {mapState} from 'vuex';import Vue from 'vue';export default { computed: { ...mapState(['score']) }, filters: { convert (val) { return Vue.filter('enumConvert')(val); } }};&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 此时页面上就会展示及格，良好，优秀了","link":"/2019/05/27/%E5%9C%A8Vue%E4%B8%AD%E4%BD%BF%E7%94%A8filters%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%AD%97%E5%85%B8%E5%80%BC%E7%9A%84%E8%BD%AC%E6%8D%A2/"},{"title":"在mysql5.7版本中使用groupby所需要注意的","text":"什么是ONLY_FULL_GROUP_BY 模式先看在mysql 5.7版本中的一个的group by，以下是这个数据库表： 12345678910111213mysql&gt; select * from testgroupby;+---------+-----------+------------+--------------+| user_id | user_name | user_score | user_subject |+---------+-----------+------------+--------------+| 1 | 张三 | 99 | 语文 || 2 | 张三 | 90 | 数学 || 3 | 张三 | 80 | 英语 || 4 | 李四 | 99 | 语文 || 5 | 王五 | 85 | 语文 || 6 | 李四 | 91 | 数学 || 7 | 王五 | 100 | 英语 |+---------+-----------+------------+--------------+7 rows in set (0.00 sec) 这是一个学生成绩数据库表。那么在MySQL5.7版本中执行它的话是会出现一个error的。如下所示： 123mysql&gt; select * from testgroupby group by user_name;ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'login.testgroupby.user_id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 注意查看报错：nonaggregated column 'login.testgroupby.user_id' which is not functionally dependent on columns。这段话是什么意思呢？大意表示的是对于groupby的username，userid并没有函数依赖于它。在这张表中，可以由user_id推导出user_name。因为每一个user_id都是唯一的，若对user_id进行groupby 则它可以推导出任何一个学生姓名，分数以及学科。即每一个学生Id都是可以确定一行值得，所以MySQL可以返回数据。 那么反过来是不是每一个学生姓名都可以推导出唯一的user_id呢？ 在这张表中，每一个user_name都是不可以推导出唯一的user_id-也就是说对user_name分组之后会有多余的user_id-, 所以也可以认为user_id并没有函数依赖于user_name。其实在这里还可以这样想，因为对user_name分组之后，在每一个组里面都会有几个值，那么随之而来的user_id ,user_score,user_subject都不是一个确定的值，也就是说在一个分组里面，在这几列中会有多个值，那么mysql这是就会不知道到底该返回哪一行得，所以这个时候开启了而这个模式的mysql就会拒绝查询。 解决方法：一：那么如何才让它不报这个错误呢，在mysql的官方文档上面说的是: The query is valid if name is a primary key of t or is a unique NOT NULL column. In such cases, MySQL recognizes that the selected column is functionally dependent on a grouping column. For example, if name is a primary key, its value determines the value of address because each group has only one value of the primary key and thus only one row. As a result, there is no randomness in the choice of address value in a group and no need to reject the query 也就是说如果groupby的列是一个主键的话，mysql会识别出他的一个函数依赖。在这个表中，由于对user_id进行groupby，在分组之后mysql是可以进行查询的。重新修改查询语句： 12345678910111213mysql&gt; select * from testgroupby group by user_id;+---------+-----------+------------+--------------+| user_id | user_name | user_score | user_subject |+---------+-----------+------------+--------------+| 1 | 张三 | 99 | 语文 || 2 | 张三 | 90 | 数学 || 3 | 张三 | 80 | 英语 || 4 | 李四 | 99 | 语文 || 5 | 王五 | 85 | 语文 || 6 | 李四 | 91 | 数学 || 7 | 王五 | 100 | 英语 |+---------+-----------+------------+--------------+7 rows in set (0.00 sec) 也就是说如果当groupby后面的字段是是一个非空主键的时候，由于主键是一个表中的唯一标识符，不可以重复，所以MySQL可以正确的推断出每一个分组。 那如果还是需要在user_name 这一列进行groupby怎么办？ 如果确实需要这样做的话，那么需要对groupby的字段进行一个处理，以确保就是这个集合是可以在分组之后都是唯一的(可以理解为只有一行) 二：除了上面的方法，还可以对分组查询出来的数据进行一个聚合操作。 12345678910mysql&gt; SELECT user_name ,COUNT(*) AS 'subject_num' FROM testgroupby GROUP BY user_name;+-----------+-------------+| user_name | subject_num |+-----------+-------------+| 张三 | 3 || 李四 | 2 || 王五 | 2 |+-----------+-------------+3 rows in set (0.00 sec) 对于聚合之后的操作，MySQL是接受查询的 三：ANY_VALUE()函数：12345678910mysql&gt; select ANY_VALUE(user_id),user_name from testgroupby group by user_name;+--------------------+-----------+| ANY_VALUE(user_id) | user_name |+--------------------+-----------+| 1 | 张三 || 4 | 李四 || 5 | 王五 |+--------------------+-----------+3 rows in set (0.00 sec) 当然这样使用的话mysql只会取所分组得第一行。 GROUP_CONCAT()函数：这个函数会将一个查询得结果集进行合并，从而可以使对user_name进行groupby之后返回得是一行 12345678910mysql&gt; SELECT GROUP_CONCAT(user_id) AS 'user_id',user_name FROM testgroupby GROUP BY user_name;+---------+-----------+| user_id | user_name |+---------+-----------+| 1,2,3 | 张三 || 4,6 | 李四 || 5,7 | 王五 |+---------+-----------+3 rows in set (0.00 sec) 总结：关于这个模式：下面这个图大致的解释了下为什么会报出这个错误。因为最后mysql会疑惑，你分组之后那么多得数据，我知道选则分组之后得哪一行？？？","link":"/2018/03/25/%E5%9C%A8mysql5.7%E7%89%88%E6%9C%AC%E4%B8%AD%E4%BD%BF%E7%94%A8groupby%E6%89%80%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84/"},{"title":"在nginx中编写rewrite","text":"在使用Nginx做一个反向代理的时候难免会碰到一些特殊的URL，例如获取图片的URL是http://dsda/XXX.jpg，后来由于需要加一个时间戳来获取另外一张图片的话，此时的URL就为http://dsda/XXX.jpg?time=YYYY。当遇到这个情况的时候是有两种选择的，分别如下： 配置location也就是在nginx中的server里面再加入一个匹配 ，但是这样加入的话若以后不再更改还好，一旦需求再次变更，就会导致配置许多的location。所以这种做法的话如果只是一些固定的URL还是可行的，但是若匹配一些动态的URL则不推荐。官网的说明如下： 12345678910111213server { listen 80; server_name example.org www.example.org; root /data/www; location / { index index.html index.php; } location ~* \\.(gif|jpg|png)$ { expires 30d; }} 配置rewrite规则针对上面的请求，可以编写如下规则： 12345678location / { # root /usr/share/nginx/html; # index index.html index.htm; if ( $request_uri ~* &quot;time=(.+)$&quot; ) { rewrite .*?(?=\\?) break; proxy_pass http://localhost:3000; } } 在这里面的一个if判断语句会判断URL是否是以time结尾，如果是的话则将?之后的URL截取然后转发至3000端口，最后便可以不用写一个location来实现转发了","link":"/2018/05/20/%E5%9C%A8nginx%E4%B8%AD%E7%BC%96%E5%86%99rewrite/"},{"title":"在springmvc中使用shiro注解","text":"前言：在之前写了一篇spring和shiro的一个整合，但是在那个项目中并没有使用注解，而且没有加入权限，只是加入了角色，所以在这篇日志中将这个项目添加注解并且加入权限。 开启Shiro的注解：刚开始开启这个注解的时候，添加了但是一直无效。 12345678910111213141516171819202122232425protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) { String username = (String) principalCollection.getPrimaryPrincipal(); List&lt;Resources&gt; resources =loginservice.getRoleById(username); List&lt;String&gt; roles =new ArrayList&lt;String&gt;(); for (Resources r: resources){ roles.add(r.getRole()); } SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addRoles(roles); if( !username.equals(&quot;MANAGER&quot;) ){ return info ; }else { List&lt;String&gt; pre = new ArrayList&lt;String&gt;(); pre.add(&quot;user:insert&quot;); info.addStringPermissions(pre); return info; } @RequestMapping(&quot;/toinsert&quot;) @RequiresPermissions(&quot;user:insert&quot;) public String toinsert(){ return &quot;getuser/userINsert&quot;; } 在这里本来想设计的是访问toinsert这个URL的时候检查权限，若没有权限则会禁止访问，可以看到在doGetAuthorizationInfo()方法中已经将权限添加到除了MANAGER以外的任意角色。但是在测试的时候却一直不能进行这个权限检测，也就是任何人都可以访问这个URL。后来在网上查了下发现是需要添加一些配置文件到spring-mvc.xml这个配置文件中。 1234567&lt;!-- shiro开启注解 --&gt; &lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot; depends-on=&quot;lifecycleBeanPostProcessor&quot;&gt; &lt;property name=&quot;proxyTargetClass&quot; value=&quot;true&quot; /&gt; &lt;/bean&gt; &lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;/bean&gt; 加了注解之后便可以依据这个设置的权限进行URL拦截，也就是除了MANAGER之外的任何人都可以进行插入用户操作。而没有该权限的用户访问这个页面的时候便会抛出一个异常。在后台可以捕获这个异常从而进行处理或者在后台使用.isPermitted(&quot;权限&quot;)来进行判断用户的权限 联想：关于Servlet的拦截器和Spring的拦截器之间的顺序在用shiro的时候顺便的也把Servlet拦截器的顺序和Spring拦截器的顺序都学习了下。在这里也顺便做了一个小的测试: 1234567891011121314151617181920212223242526272829303132public class ServletFilter implements Filter{ public void init(FilterConfig filterConfig) throws ServletException { System.out.println(&quot;Servlet得拦截器init()方法&quot;); } public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(&quot;Servlet得拦截器doFilter()方法&quot;); filterChain.doFilter(servletRequest,servletResponse); return; } public void destroy() { System.out.println(&quot;Servlet得拦截器destroy()方法&quot;); }}public class SpringHandle implements HandlerInterceptor { public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(request.getHeader(&quot;Accept&quot;).toString()); System.out.println(&quot;This is a intercept to test the web&quot;);// handleError(request,response); return true; } private void handleError(HttpServletRequest request ,HttpServletResponse response) throws ServletException, IOException { RequestDispatcher rd = request.getRequestDispatcher(&quot;/&quot;); System.out.println(&quot;处理错误，需要跳转&quot;); rd.forward(request, response); }} 编写好了之后启动项目，发现Servlet的的这个拦截器最先运行init()方法在最初项目启动的时候便会于运行，而且一个请求过来之后也是doFilter()先运行，然后再是Spring的拦截器","link":"/2018/03/26/%E5%9C%A8springmvc%E4%B8%AD%E4%BD%BF%E7%94%A8shiro%E6%B3%A8%E8%A7%A3/"},{"title":"字符串匹配相关算法","text":"今天无意间碰到了需要写子字符串匹配得算法，由于以前一直对字符串得KMP算法不太了解，所以今天趁着这个机会正好记录下这三种算法的差异： JavaApi实现 朴素字符串匹配算法 KMP算法 Java的API实现子字符串查找：在Java的String类里面有一个方法是indexof(str,index), 这个类的话是可以从一个字符串的指定位置查出是否包含子字符串，若不包含则返回的是-1，若包含的话则返回的是该子字符串在字符串中的索引位置。 那么这个算法是什么原理呢，这个算法的原理就是indexof会返回匹配到的字符串的索引，那么当这个算法匹配到了一个之后获取返回的索引，然后再加上字符串的长度，最后再在剩下的字符串里面匹配，便会得出所有的字符串个数。 1234567891011public void test(){ String findText= &quot;abcd&quot;; String srcText =&quot;mabcdfafasabcdfa&quot;; int count = 0; int index = 0; while ((index = srcText.indexOf(findText, index)) != -1) { index = index + findText.length(); count++; } System.out.println(count); } 朴素字符串匹配算法：这个算法就是两层循环，首先第一层循环便利需要字符串，获取第一个字符，然后进行第二层的循环，第二层的循环主要是进行子字符串，当子字符串的第一个字符和外面的字符串当前的字符相同的话，然后进行子字符串的第二个字符与外层的当前位置的第二个字符比较： 12345678910111213141516171819202122public void test1(){ String a =&quot;abcd&quot;; String b=&quot;qweabcdaefgadfabcdojfojaofoafjabcdafsaf&quot;; int count=0; for(int i =0 ;i&lt;b.length() ;i++){ int index=i; boolean flag =false; for(int j=0;j&lt;a.length() ;j++){ char c =b.charAt(index); if(a.charAt(j) == c){ index++; }else{ flag=true; break; } } if(!flag) { count++; } } System.out.println(count); } 在这个里面需要注意的就是外层的i需要保存，同时里面的循环j是不需要的，因为子字符串是每一次都会遍历的。 KMP算法：kmp算法所解决的问题是朴素字符串匹配算法每次匹配失败之后都要从头再来进行匹配的一个难点。在KMP算法中的： 匹配失败的话首先会检查匹配失败的头一个字符的的前缀和后缀，然后用以匹配的字符数减去其前缀和后缀的那个值，最后得出需要后退的数字，最后再在退出的步数上进同样的操作，最后再得出结论，当子字符串的第一个字符都不相匹配的时候最后子字符串整体后移一位。 关于Next数组的求法：今天看了下资料总算了解了写KMP的Next数组的求法。现在一般来讲是有两种求法，第一种是求出匹配失败后的跳转索引的数组。第二种则是求出数组中已经匹配的前后缀的数组。这两种数组的求法都会导致再以后的计算出现不同的结果，但是最后还是需要用到的一个原理就是匹配失败：回退步数就是已经匹配的字符数减去匹配的前后缀字符数。最后得出回退步数。 目前来讲有两种next数组的求法： 将前缀后缀匹配数放置到数组中 将失败后回退的位置放置在数组中 对于第一种next数组来说，网上的资料多是将其后移一位，例如字符串abcab他的前缀和后缀数组是[0,0,0,1,2]，那么网上其他人的写法多是将这个数组再右移一位,最后为[-1,0,0,0,1]。在今天，也自己尝试了写KMP算法，但是自己的写法没有做右移操作，而是直接求解，如下： 12345678910111213141516171819202122public static void generateNextArray(String str ,int[] array) { if (null == str || str.length() == 0) { return; } int i = 0; int j = 1; array[0] = 0; for (; j &lt; str.length(); ) { if (str.charAt(i) == str.charAt(j)) { i++; j++; array[j - 1] = i; } else if(i!= 0 &amp;&amp; j!=0) { i = array[i-1]; //这里需要i-1是因为最后需要数组右移一位，但是我这里没有移位，所以直接在这里减一 }else{ array[j]=i; j++; } } } 上面这个求next数组的方法其实也挺简单的，但是这里需要注意的是第二个if和第三个的else，为什么这里会出现两个是因为这里需要注意的情况会有两种，第二个if捕获的是在后续出现了断层需要重新从0开始匹配的情况。而最后一个else则是捕获的从头开始的一种情况，具体以abcdeabcdabcdeabcdfg做测试便会了解。而网上的其他算法不需要这三个判断是因为网上的匹配失败之后K会变成-1，然后j是不变的。如下： 1234567891011121314151617void getNext(String pattern, int next[]) { int j = 0; int k = -1; int len = pattern.length(); next[0] = -1; while (j &lt; len - 1) { if (k == -1 || pattern.charAt(k) == pattern.charAt(j)) { j++; k++; next[j] = k; } else { k = next[k]; } } } 也就是说网上的一旦匹配失败，k就会变成next[next[k]]的值。 JDK中的indexOf方法：再Java中的indexOf方法则是使用的朴素字符串匹配算法。如下： 12345678910111213141516171819202122232425262728293031323334353637static int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) { if (fromIndex &gt;= sourceCount) { return (targetCount == 0 ? sourceCount : -1); } if (fromIndex &lt; 0) { fromIndex = 0; } if (targetCount == 0) { return fromIndex; } char first = target[targetOffset]; int max = sourceOffset + (sourceCount - targetCount); for (int i = sourceOffset + fromIndex; i &lt;= max; i++) { /* Look for first character. */ if (source[i] != first) { while (++i &lt;= max &amp;&amp; source[i] != first); } /* Found first character, now look at the rest of v2 */ if (i &lt;= max) { int j = i + 1; int end = j + targetCount - 1; for (int k = targetOffset + 1; j &lt; end &amp;&amp; source[j] == target[k]; j++, k++); if (j == end) { /* Found whole string. */ return i - sourceOffset; } } } return -1; }","link":"/2018/04/02/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/"},{"title":"字符串数组组成最小的数字","text":"字符串数组拼接出一个最小的数字记得在之前的一个面试中遇到了这个算法题， 但是当时没怎么想好如何判断两个字符串之间的大小，比如 23 和 223 之间，其组合起来绝对是 23 大于 223，所以 223是需要放在前面的。 思路其实可以将两个字符串相加，例如 22323 &lt; 23223 ，所以 223 是需要放在 23 前面的，下面就是代码. 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344private void getNum(String[] str){ sortString(str,0,str.length-1); } private void sortString(String[] strArray ,int start ,int end){ if(end &gt; strArray.length -1 || end &lt;0){ return; } if(start &lt;0 || start &gt; strArray.length -1){ return; } if(start == end ){ return; } int mid =(start + end) / 2; while (start &lt; end &amp;&amp; start &lt;mid &amp;&amp; end &gt;mid){ String s1 =strArray[start] +strArray[mid]; String s2 =strArray[mid] +strArray[start]; String s3 =strArray[mid] +strArray[end]; String s4 =strArray[end] +strArray[mid]; if(Integer.parseInt(s1) &lt; Integer.parseInt(s2)){ start++; } if(Integer.parseInt(s3) &lt; Integer.parseInt(s4)){ end--; } } String temp =strArray[start]; strArray[start] = strArray[end]; strArray[end] =temp; sortString(strArray,start,mid ); sortString(strArray,mid-1,end ); } public static void main(String[] args) { String[] s =new String[]{&quot;32&quot;,&quot;321&quot;,&quot;3&quot;}; new GetSmallSum().getNum(s); StringBuffer stringBuffer =new StringBuffer(); for(String s1 :s){ stringBuffer.append(s1); } System.out.println(Integer.parseInt(stringBuffer.toString())); }","link":"/2018/08/25/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%95%B0%E7%BB%84%E7%BB%84%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0%E5%AD%97/"},{"title":"安装hexo的一些注意事项","text":"安装步骤： 首先安装npm — 然后安装hexo npm install hexo-cli -g 初始化一个hexo项目 hexo init blog 添加配置文件 pm install --save hexo-renderer-jade hexo-renderer-scss hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive 安装 npm install 添加主题文件: 在github上找出自己喜欢的一个主题 用git命令clone下来 将文件放到themes文件夹，然后修改hexo文件的_config.yml 将themes文件夹里面的那个主题名称添加到theme这个标签之后 可以在theme文件夹里面修改_config.yml这个文件来获得想要的效果","link":"/2017/11/06/%E5%AE%89%E8%A3%85hexo%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"},{"title":"对Java的List转Array的源码一点思考","text":"在Java里面，List转为Array是调用的Java的一个Arrays.copyOf()这个方法，查看了下源代码： 1234567891011121314151617181920212223public Object[] toArray() { return Arrays.copyOf(this.elementData, this.size); } public static &lt;T&gt; T[] copyOf(T[] original, int newLength) { return (T[]) copyOf(original, newLength, original.getClass()); } public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) { @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } public static Object newInstance(Class&lt;?&gt; componentType, int length) throws NegativeArraySizeException { return newArray(componentType, length); } 在这里需要注意的是在copyOf()方法中的那个三元表达式，也就是说在这里无论执行的true还是false，都会返回一个新的数组对象。而在这里有一行代码就是((Object)newType == (Object)Object[]).class)，这一句看起来没什么，其实可以看参数Class&lt;? extends T[]&gt; newType和Object会发现这两个数组对象都被强转成了Object，而==`比较符是不能比较不同类型的。例如： 1234public void te(){ System.out.println(&quot;ad&quot; == 2); System.out.println((String.class == Object.class)); } 上面的代码在代码的编译期就会被提示==不可以适用于上述的两种情况.如下： 那么回到正题，这里其实仔细看就会发现无论是否相等，在这里三元表达式中，都会返回一个新的数组对象，那么在这里加入了三元表达式的想法是newInstance()是通过反射进行创建的，而new Object[]则是非反射创建的. Because reflection involves types that are dynamically resolved, certain Java virtual machine optimizations can not be performed. Consequently, reflective operations have slower performance than their non-reflective counterparts, and should be avoided in sections of code which are called frequently in performance-sensitive applications. 所以在这里加一个判断使代码尽量通过非反射的方式进行创建。最后调用System.arraycopy(),但是这里的都已经强转成了Object，应该不会有失败的情况","link":"/2018/04/07/%E5%AF%B9Java%E7%9A%84List%E8%BD%ACArray%E7%9A%84%E6%BA%90%E7%A0%81%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/"},{"title":"对数据库事物的理解","text":"什么是事物事物通俗的来讲就是就是一组操作事件，可以类比于Java里面的原子操作。在一个事物中，要么全部成功，要么就是全部失败。 mysql中的事物在Mysql的innodb中，事物的默认级别是 可重复读，在该级别下，事物可能出现幻读。出现幻读的情况是该引擎为行级锁，导致mysql在进行一个事物的时候只会锁定与该事物有关的几行。 示例如下：新建一个表，其中的数据如下： 123456789mysql&gt; select * from curd;+----+------+| id | name |+----+------+| 3 | 4 || 4 | 3 || 5 | 2 || 6 | 1 |+----+------+ 开启一个事物： 123456START TRANSACTION;SELECT * FROM curd;UPDATE curd SET curd.name='1000';SELECT * FROM curd;-- SELECT * FROM curd;-- COMMIT; 此时两次查询的结果为： 12345678910111213141516171819202122232425262728mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM curd;+----+------+| id | name |+----+------+| 3 | 4 || 4 | 3 || 5 | 2 || 6 | 1 |+----+------+4 rows in set (0.00 sec)mysql&gt; UPDATE curd SET curd.name='1000';Query OK, 4 rows affected (0.00 sec)Rows matched: 4 Changed: 4 Warnings: 0mysql&gt; SELECT * FROM curd;+----+------+| id | name |+----+------+| 3 | 1000 || 4 | 1000 || 5 | 1000 || 6 | 1000 |+----+------+4 rows in set (0.00 sec) 此时可以看到，在事物一未提交之前，事物一做了一个全表的更新，将该表的数值全部更新为1000了。然后开启 事物二 1234567891011121314151617mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM curd;+----+------+| id | name |+----+------+| 3 | 4 || 4 | 3 || 5 | 2 || 6 | 1 |+----+------+4 rows in set (0.00 sec)mysql&gt; INSERT INTO curd(curd.id,curd.name) VALUES(7,'7');ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction-- 注意使用win10的管理员cmd会卡在这一行，但是使用sqlyog就不会，mysql会卡在这一行 当事物二执行完这一句之后，回到事物一，然后查询表数据。 123456789101112-- 使用Sqlyog新增之后看到的数据mysql&gt; SELECT * FROM curd;+----+------+| id | name |+----+------+| 3 | 1000 || 4 | 1000 || 5 | 1000 || 6 | 1000 || 7 | 7 |+----+------+4 rows in set (0.00 sec) 此时就是幻读，事物一明明就更新了6条数据，为什么会出现7条数据。。然后事物一提交： 12mysql&gt; commit;Query OK, 0 rows affected (0.01 sec) 此时数据库中的数据已经全部更新成1000，但是在事物二中： 1234567891011mysql&gt; SELECT * FROM curd;+----+------+| id | name |+----+------+| 3 | 4 || 4 | 3 || 5 | 2 || 6 | 1 || 7 | 7 |+----+------+5 rows in set (0.00 sec) 说明两个事物之间不能读取其他事物未提交的更改，并且在mysql的该级别下，事物开始和结束的时候所读取的数据是一样的。 此时提交事物二： 1234567891011121314mysql&gt; commit;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM curd;+----+------+| id | name |+----+------+| 3 | 1000 || 4 | 1000 || 5 | 1000 || 6 | 1000 || 7 | 7 |+----+------+5 rows in set (0.00 sec) 可以看到此时数据库中的数据已经如成为了上面所示了。 原理具体是因为mysql的innodb使用的是行级锁，而且在mysql的每一个表里面都会有两个隐藏的列，分别是创建的时候版本号和删除的时候的版本号。所以每一个事物执行的时候的版本号都会被该事物号所更新，因此会导致有的事物在执行期间会导致看到不同的数据。 注意现在mysql5.7貌似已经解决了幻读。。用命令行进行测试的。","link":"/2018/08/10/%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E7%89%A9%E7%9A%84%E7%90%86%E8%A7%A3/"},{"title":"将Jackson替换成Fastjosn","text":"记得有一次的面试是。如何在Spring中将JackSon 替换为 FastJson，emmmm…当时的回答是只需要替换 pom.xml，然后在使用的时候引入FastJosn就行了，但是在当时显然没有理解到面试官的意图，既然面试官强调的是如何替换，那么修改pom.xml很显然不是面试官所想要的答案，那还有什么答案呢？ 有一个方法可能是面试官想要的，那就是重写Spring的HttpMesageConverter方法，在这个方法里面引入FastJson的配置，然后替换掉Spring默认的Jackson。 替换方式有几种，一种是返回一个HttpMesageConverter，另一种是继承WebMvcConfigurerAdapter 来实现 configureMessageConverters 代码如下：12345678910111213141516171819202122232425@Configurationpublic class WebConfig extends WebMvcConfigurerAdapter { @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { FastJsonConfig fastJsonConfig = new FastJsonConfig(); fastJsonConfig.setSerializerFeatures( // 输出空置字段 SerializerFeature.WriteMapNullValue, // list字段如果为null，输出为[]，而不是null SerializerFeature.WriteNullListAsEmpty, // 数值字段如果为null，输出为0，而不是null SerializerFeature.WriteNullNumberAsZero, // Boolean字段如果为null，输出为false，而不是null SerializerFeature.WriteNullBooleanAsFalse, // 字符类型字段如果为null，输出为&quot;&quot;，而不是null SerializerFeature.WriteNullStringAsEmpty); fastJsonConfig.setCharset(Charset.forName(&quot;UTF-8&quot;)); fastJsonConfig.setDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); FastJsonHttpMessageConverter Converter = new FastJsonHttpMessageConverter(); Converter.setFastJsonConfig(fastJsonConfig); HttpMessageConverter&lt;?&gt; converter = Converter; converters.add(0,converter); super.configureMessageConverters(converters); } Controller类123456789@RequestMapping(&quot;/testjson&quot;) @ResponseBody public TestJson forword(@RequestBody TestJson testJson) { System.out.println(testJson.getId()); System.out.println(testJson.getName()); System.out.println(testJson.isFlag()); return testJson; } 打印结果如下： 1234567891011//发送请求{ &quot;id&quot;:1, &quot;name&quot;:null}//控制台输出{ &quot;flag&quot;: false, &quot;id&quot;: 1, &quot;name&quot;: &quot;&quot;} 所以可以看到默认的Jackson已经被替换为Fastjson了 为了区别与Jackson的差异，在这里注释掉Jackson的config，然后再次请求，结果如下： 12345{ &quot;id&quot;: 1, &quot;name&quot;: null, &quot;flag&quot;: false} 可以看到String为null的话，并没有被替换为””","link":"/2018/09/04/%E5%B0%86Jackson%E6%9B%BF%E6%8D%A2%E6%88%90Fastjosn/"},{"title":"快速写一个程序将JVM的堆或者栈打满","text":"堆和栈数据结构首先「堆」或者「栈」在本质上其实是一个数据结构，简介如下：「栈」既可以用链表来实现，又可以用数组来实现，用链表来实现的话，它是一个含有头指针和尾指针的一种数据结构，根据含有指针的不同，分为单链表和双向链表。「堆」是一种类似于「完全二叉树」的数据结构 JVM中的堆和栈在 JVM 中由于编译后的 class 文件都是一行行的指令，因此天然适合用「栈」这种数据结构， 12345678public class JvmTest { public static void main(String[] args) { int i = 0; i += 1; System.out.println(i); }} 反编译后如下： 123456789101112131415161718Compiled from &quot;JvmTest.java&quot;public class JvmTest { public JvmTest(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: iconst_0 1: istore_1 2: iinc 1, 1 5: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 8: iload_1 9: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 12: return} 那么对于这种指令来讲，以一种先进先出的方式存入肯定是最好的，所以在 JVM 中，栈的组成是一个一个的栈帧，那么每一个栈帧都包含如下几个部分： 1234局部变量表操作数栈动态链接方法出口 以 HotSpot VM 为例，由于其采用了固定栈大小的实现，也就指定了每一个线程的所能分配的栈内存的大小，那么依据此思路，可以有如下两种方法造成栈溢出： 无限递归，导致无法为该线程分配内存 无限创建线程，导致无法为该线程分配足够的内存 -Xss 可以指定栈的大小 demo1:注意加上启动参数：-Xss16m 123456789public class StackOverFlowTest { public static void main(String[] args) { new StackOverFlowTest().over(1); } private void over(int deepth){ System.out.println(deepth); over(++deepth); }} 不一会就可以看到 java.lang.StackOverflowError demo2:1234567891011121314151617181920212223242526272829public class StackOverFlowTest { public static void main(String[] args) { new StackOverFlowTest().threadOver(); } private void threadOver(){ ThreadTest threadTest = new ThreadTest(1); while (true){ new Thread(threadTest).start(); } } class ThreadTest implements Runnable{ public ThreadTest(int depth) { this.depth = depth; } private int depth; @Override public void run() { System.out.println(&quot;thread start&quot; + depth); try { Thread.sleep(10000000); } catch (InterruptedException e) { e.printStackTrace(); } } }} 艹，运行到一半的时候，把 mac 搞挂了。正常情况下是会出现 1java.lang.OutOfMemoryError: Unable to create new native thread 打满堆：在 JVM8 中，堆一般存放的是 对象实例，含有 S0、S1、Eden、Old这几个区域，具体的可以查看 JVM 规范。 1234567891011121314// -Xmx32mpublic class JvmTest { public static void main(String[] args){ List&lt;VM&gt; list = new ArrayList&lt;&gt;(); while (true){ VM vm = new VM(); list.add(vm); } } static class VM{ private byte[][] b = new byte[1024][1024]; }} 运行一段时间就会出现： 123Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at jvm.over_test.JvmTest$VM.&lt;init&gt;(JvmTest.java:30) at jvm.over_test.JvmTest.main(JvmTest.java:24) 元数据区：首先 元数据区 保存的是 类信息、运行时常量池、以及即时编译器编译后的代码 等。那么根据存储的数据的特性，直接选择动态生成代理类的加载类信息来打满这个区域，需要注意的是这个区域使用的是机器的内存，所以在测试的时候最好指定下 元数据区 的大小。 在这里也可以用自定的 ClassLoader，但是自定义 ClassLoader 需要重写 loadClass 方法，比较麻烦，所以直接选择使用代理类 12345678910111213// -XX:MaxMetaspaceSize=16M -XX:MetaspaceSize=16M public class JvmTest { public static void main(String[] args) throws MalformedURLException{ List&lt;InterfaceA&gt; list = new ArrayList&lt;&gt;(); String classFilePath = &quot;file:/Users/sunzhaohui/Desktop/person/java/MyCsNote/jvm/src/main/java/jvm/StringTest&quot;; URL[] classFileUrl = new URL[]{new URL(classFilePath)}; URLClassLoader newClassLoader = new URLClassLoader(classFileUrl); while (true) { InterfaceA t = (InterfaceA) Proxy.newProxyInstance(newClassLoader, new Class&lt;?&gt;[]{InterfaceA.class}, new MyInvocationHandler(new InterfaceAImpl())); list.add(t); } }}","link":"/2020/07/11/%E5%BF%AB%E9%80%9F%E5%86%99%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F%E5%B0%86JVM%E7%9A%84%E5%A0%86%E6%88%96%E8%80%85%E6%A0%88%E6%89%93%E6%BB%A1/"},{"title":"慢查询排查以及优化","text":"排查方式SHOW PROCESSLIST SHOW PROCESSLIST 只能查看到当前用户正在运行的线程，如果你是ROOT用户，那么是可以看到所有用户正在运行的线程如果想看到其他用户的线程，则必须为此用户赋予 PROCESS 权限。 危害： 造成机器的 CPU 以及 IO 过高，影响到数据库实例。 阻塞 DDL 查询语句，会造成其他正常业务出现耗时过高。 会导致 mysql 出现 MDL 锁，影响到该机器上正常 SQL 的执行。 对正在运行的业务锁产生的影响如果是 Java 应用，还有可能会导致 GC 耗时增大，线程 block 增多，最终引起整个服务的波动 如何监控慢查询：开启 mysql 的慢查询日志，再通过 FileBeat 将数据同步至 ELK 即可 慢SQL 优化：EXPLAIN SQL 查看语句是否走了索引，以及 extra 信息。 extra Using temporary 代表采用的是临时表 Using filesort采用文件索引 Using index 采用覆盖索引 Using join buffer (Block Nested Loop) BNL 优化，出现此项则代表多表 JOIN 连接没有走索引 索引失效： 索引字段不能使用函数 避免查询的字段发生隐式转换 如果索引字段使用 like key% 可以走索引，其他方式均不走 联合作引有最左原则，因此需要 where 条件中的索引出现符合最左原则 order by 的字段必须走索引，否则会出现 filesort 如果通过 explain 发现走全表扫描的成本还低于走索引的，那么证明索引不合理，需要重新区分 表链接一定要小表驱动大表 系统优化：1、数据冷热处理大数据量的查询走 hive，生产环境中不应该存在mysql 大数据量的查询，定期将一些冷门数据同步至冷库，而不应该占用生产核心数据库 如果生产环境部分客户查询到了冷库数据，需要进行限流，被限流的查询可以发送异步消息，将冷库数据取出，然后同步至缓存 冷库数据建议直接同步至 hive 或者 ES 中，并且将数据通过日期拆分，例如 每周、每月等，提高查询效率 2、分库分表目前业界采用的是 Sharding-JDBC+LVS+Keepalived","link":"/2021/08/31/%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%8E%92%E6%9F%A5%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"title":"数据库最左原则以及理解","text":"本次的实验是基于Mysql8版本。首先在数据库中有一个表，其结构如下： 12345678910111213mysql&gt; show create table org;+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| org | CREATE TABLE `org` ( `org_id` int(5) NOT NULL, `org_name` varchar(255) DEFAULT NULL, `org_parent_id` int(5) DEFAULT NULL, PRIMARY KEY (`org_id`), KEY `index_name` (`org_name`,`org_parent_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 可以看到在这个表中，有一个主键org_id以及一个联合索引index_name。其他的并无特别之处。 最左原则在Mysql里面，有一个最左原则，官网的介绍如下： The name index is an index over the last_name and first_name columns. The index can be used for lookups in queries that specify values in a known range for combinations of last_name and first_name values. It can also be used for queries that specify just a last_name value because that column is a leftmost prefix of the index (as described later in this section). Therefore, the name index is used for lookups in the following queries。 既然是最左原则，那么尝试着写一条SQL如下： 12345678mysql&gt; explain SELECT * FROM org WHERE org_name='1' and org_parent_id=1;+----+-------------+-------+------------+------+---------------+------------+---------+-------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------------+---------+-------------+------+----------+-------------+| 1 | SIMPLE | org | NULL | ref | index_name | index_name | 773 | const,const | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+------------+---------+-------------+------+----------+-------------+1 row in set, 1 warning (0.01 sec) 可以看到其type是 ref 类型的，于是去mysql官网寻找ref类型的索引是什么： All rows with matching index values are read from this table for each combination of rows from the previous tables. ref is used if the join uses only a leftmost prefix of the key or if the key is not a PRIMARY KEY or UNIQUE index (in other words, if the join cannot select a single row based on the key value). If the key that is used matches only a few rows, this is a good join type. 啥意思呢，就是当使用的索引符合最左原则的时候，且索引即不是主键也不是唯一索引。那么它所使用的类型就是ref。 那如果将org_name和org_parant_id反着呢? 1234567mysql&gt; explain SELECT * FROM org WHERE org_parent_id =1 and org_name='1';+----+-------------+-------+------------+------+---------------+------------+---------+-------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------------+---------+-------------+------+----------+-------------+| 1 | SIMPLE | org | NULL | ref | index_name | index_name | 773 | const,const | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+------------+---------+-------------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 说好的最左原则呢。。。索引的org_parent_id不是在右边的嘛。 其实在这种情况下，无论org_parent_id在左边还是在右边，对于查询结果来说，都是一样的。既然都是一样的，那么Mysql的优化器就直接把该条语句给优化掉了，所以你会发现无论是org_name在左还是在右边都是会使用到索引。 那再换一条SQL呢？ 12345678mysql&gt; explain SELECT * FROM org WHERE org_name&gt;'1' and org_parent_id=1;+----+-------------+-------+------------+-------+---------------+------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | org | NULL | index | index_name | index_name | 773 | NULL | 9 | 11.11 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec) 这就尴尬了…由于在最左原则里面，若前一个索引使用了&gt;、&lt;等比较符的时候，后面一个是不会进行索引查询的，这个跟Mysql的索引结构有关系(文章后面会介绍)，但是为啥这里又会走索引呢? 这个时候由于在Extra里面看到了Using index，猜测难道是发生了索引覆盖? 于是我又建立了一个表。于是立即又新建了一张表 123456789101112131415mysql&gt; show create table org_copy;+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| org_copy | CREATE TABLE `org_copy` ( `org_id` int(5) NOT NULL, `org_name` varchar(255) DEFAULT NULL, `org_parent_id` int(5) DEFAULT NULL, `org_copy` varchar(255) DEFAULT NULL, PRIMARY KEY (`org_id`), KEY `index_name` (`org_name`,`org_parent_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 这时，我再次执行刚才的sql。 12345678mysql&gt; explain SELECT * FROM org_copy WHERE org_name&gt;'一级部门' and org_parent_id=1;+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | org_copy | NULL | range | index_name | index_name | 768 | NULL | 1 | 11.11 | Using index condition |+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) 看起来的确是由于发生了索引覆盖，导致了虽然不符合最左原则，但是还是走了一个索引。 索引覆盖什么是索引覆盖呢?在org这个表里面，有三个字段，但是这三个字段分别是一个主键索引和一个联合索引，由于我查询的字段就包含在索引里面，那么而恰好这个表的所有字段都在索引里面，这就导致了mysql可以直接从索引里面获取到所需要的数据，那么此时就不必要再去通过磁盘IO去查询额外的字段数据了。也就不需要进行 继续回到最左原则，由于最左原则要求在聚合索引里面，每一个列是要为一个等值连接。即，如果org_name使用了非等值连接，那么就会导致org_parent_id无法使用最左原则。例子如下：若org_parent_id使用非等值连接，为了避免索引覆盖对SQL的影响，所以此次的SQL在 copy表里面执行。 12345678mysql&gt; explain SELECT * FROM org_copy WHERE org_name='1' and org_parent_id&gt;1;+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | org_copy | NULL | range | index_name | index_name | 773 | NULL | 1 | 100.00 | Using index condition |+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) 可以看到确实是使用了聚合索引，同时最后一列的 Using index condition也表明，这条查询语句会先通过索引来过滤出符合的数据，然后从过滤出来的数据里面在使用我们的where条件进行二次过滤，最终找出符合的数据。 解释在Mysql里面，索引是以B+树的形式实现的，而索引又分为一级索引(主键)和二级索引(该文章中的index_name)，其实在InnoDB里面，二级索引的叶子节点所存放的数据就是主键索引所对应的地址，这也称之为回表。回到文章中来，在这篇文章中介绍的index_name索引里面，mysql是根据索引列的顺序，一个一个在索引里面进行查找过滤，如果该索引在某一列断开了，例如索引A的列是(A、B、C)，但是在SQL里面却是where A ='a' and C='c'，此时就会只使用该索引A的A列，这个跟B+树的特点有关。 12345678910111213141516171819202122232425mysql&gt; show create table index_test;+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| index_test | CREATE TABLE `index_test` ( `a` int(5) NOT NULL, `b` varchar(255) DEFAULT NULL, `c` int(5) DEFAULT NULL, `d` varchar(255) DEFAULT NULL, `f` varchar(255) DEFAULT NULL, PRIMARY KEY (`a`), KEY `index_name` (`b`,`c`,`d`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)mysql&gt; EXPLAIN SELECT * FROM `index_test` WHERE b='b' and d='d';+----+-------------+------------+------------+------+---------------+------------+---------+-------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+------------+------+---------------+------------+---------+-------+------+----------+-----------------------+| 1 | SIMPLE | index_test | NULL | ref | index_name | index_name | 768 | const | 1 | 50.00 | Using index condition |+----+-------------+------------+------------+------+---------------+------------+---------+-------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) 可以看到ref就一个const，所以基本可以推测目前只用了b列，那么换成WHERE b='b' and c='c'呢? 12345678mysql&gt; EXPLAIN SELECT * FROM `index_test` WHERE b='b' and c='c';+----+-------------+------------+------------+------+---------------+------------+---------+-------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+------------+------+---------------+------------+---------+-------------+------+----------+-------+| 1 | SIMPLE | index_test | NULL | ref | index_name | index_name | 773 | const,const | 1 | 100.00 | NULL |+----+-------------+------------+------------+------+---------------+------------+---------+-------------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 所有对于使用最左原则来进行调优的话，首先应该注意的是索引列最好都是等值连接，并且中途最好不要有任何的断裂。这样才能发挥出联合索引的优势","link":"/2019/06/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%80%E5%B7%A6%E5%8E%9F%E5%88%99%E4%BB%A5%E5%8F%8A%E7%90%86%E8%A7%A3/"},{"title":"数据库调优(一)","text":"开篇在上一篇文章中，我们有一个表，里面的内容如下： 12345678910111213141516mysql&gt; select * from org_copy;+--------+-----------------+---------------+----------+| org_id | org_name | org_parent_id | org_copy |+--------+-----------------+---------------+----------+| 1 | 一级部门 | 0 | 1 || 2 | 一一级部门 | 0 | 2 || 3 | 1.1级部门 | 1 | 3 || 4 | 1.2级部门 | 1 | 4 || 5 | 1.1.1部门 | 3 | 5 || 6 | 1.1.2部门 | 3 | 6 || 7 | 1.1.1.1部门 | 5 | 7 || 8 | 1.3部门 | 1 | 8 || 9 | 1.2.1部门 | 4 | 9 |+--------+-----------------+---------------+----------+9 rows in set (0.00 sec) 这应该是一个很基本的一个mysql表，同时我们在上一篇文章中，也执行了如下SQL。 12345678mysql&gt; explain SELECT * FROM org_copy WHERE org_name&gt;'一级部门' and org_parent_id=1;+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | org_copy | NULL | range | index_name | index_name | 768 | NULL | 1 | 11.11 | Using index condition |+----+-------------+----------+------------+-------+---------------+------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) 那么如果此时我们换一个SQL来进行查询呢? 1234567mysql&gt; explain SELECT * FROM org_copy WHERE org_name&gt;'1' and org_parent_id=1;+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | org_copy | NULL | ALL | index_name | NULL | NULL | NULL | 9 | 11.11 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 此时是不是会一脸疑惑…为啥我的这个SQL，明明一样呀，但是为啥第一个就可以走索引呢???因为在mysql里面，会判断当前where条件查询的数据量，由于索引是由一个B+树的形式存在，所以当通过org_name来进行比较筛选的时候，是可以很快的定位出大致需要查询的数据量。 而当数据量大于30%的时候，mysql就会采用一种全表扫描的方式来进行查询，这也就是为什么不建议在区分度低的字段上建立索引了，假设一个字段只有三个值或者两个值，那么极有可能mysql会直接通过全表扫描的方式进行查询。 索引覆盖这个是mysql调优中经常忽略的一点，由于mysql在建立索引的时候会一次性将索引字段存入到索引树中，所以如果我们的where条件中所包含的字段均可以在索引中找到的话，那么mysql就会直接从索引中去取数据，而不会进行回表。例如如下SQL： 12345678mysql&gt; explain SELECT org_parent_id FROM org_copy WHERE org_copy=1;+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | org_copy | NULL | ALL | NULL | NULL | NULL | NULL | 4325 | 10.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 由于org_copy字段非索引，所以此时只能是通过回表的方式进行查询。 总结 由于mysql会在数据量大于30%的时候进行全表扫描，所以最好就是不要在区分度低的字段上建立索引，避免进行了全表扫描。 对于过滤字段的使用，应该是尽最大的努力让SQL采用索引覆盖，如果无法避免的话就尽量让最左原则生效。进而加快查询速度。","link":"/2019/06/09/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%B0%83%E4%BC%98-%E4%B8%80/"},{"title":"浅谈工厂模式","text":"简介工厂模式解决的是频繁的修改某一些 new 操作，隐藏真实的创建过程，方便以后更加快速的新增和扩展，简单来说就是维护一类关系。 简单工厂：把对象的创建放到一个Util中，通过不同的入参来创建不同的类。这也是日常编码中经常用到的，不过缺点就是每次新增一个类的时候，都需要修改if/else判断，有点繁琐。 工厂方法：将含有相同属性的类抽象成一个工厂，这一个工厂只负责自己的对象的创建，对修改关闭，对扩展开放。 抽象工厂：对类的操作进一步的划分，将某种类的操作再次进行抽象化。 代码假设现在要开发一个餐厅点餐系统，首先肯定是有顾客以及食物。为了以后的扩展性，在这里通过三种工厂方法来实现不同的需求 简单工厂现在有两个类，User 和 Food，此时有一个需求是人需要吃食物，可能你毫不犹豫的写下了如下代码： 12345678910111213141516171819202122232425262728293031323334public class Potato { private int temperaturel; private String name; public Potato(String name) { this.name = name; } public void cook(){ System.out.println(&quot;加热中&quot;); this.temperaturel = 100; } public String getName() { return name; } public void setName(String name) { this.name = name; }}public class User { public static void main(String[] args) { new User().dinner(); } private void dinner(){ Potato potato = new Potato(&quot;土豆&quot;); potato.cook(); System.out.println(&quot;吃晚餐&quot; + potato.getName()); }} 后来有一天需求变了，现在需要再加一个食物以供客户选择，后来你想了下，决定写一个 Util 类。如下： 123456789public static Object getFood(String name){ if(name != null){ if(&quot;Potato&quot;.equals(name)){ return new Potato(&quot;土豆&quot;); } else if(&quot;Tomato&quot;.equals(name)){ return new Tomato(&quot;西红柿&quot;); } } } 这段代码明显的有一个问题就是，返回的 Object 在另一个地方进行强制转换的时候，由于不知道返回的是 Potato 还是 Tomato，所以很容易抛出转换异常，所以此时，再继续优化这段代码，由于土豆和西红柿都是食物的一种，于是可以直接抽象一个接口出来。这样土豆和马铃薯都实现这个接口。如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public interface Food { void cook(); String getName();}public class Potato implements Food{ private int temperaturel; private String name; public Potato(String name) { this.name = name; } @Override public void cook(){ System.out.println(&quot;加热中&quot;); this.temperaturel = 100; } @Override public String getName() { return name; } public void setName(String name) { this.name = name; }}public class Tomato implements Food{ private int temperaturel; private String name; public Tomato(String name) { this.name = name; } @Override public void cook(){ System.out.println(&quot;加热中&quot;); this.temperaturel = 100; } @Override public String getName() { return name; } public void setName(String name) { this.name = name; }} 这个时候呢，修改 Util 类，让它返回 Food 这个抽象，然后通过实例化子类，来实现模版的调用，于是继续修改 Util 类。如下： 12345678910111213141516171819202122232425public class FoodUtil { public static Food getFood(String name){ if(name != null){ if(&quot;Potato&quot;.equals(name)){ return new Potato(&quot;土豆&quot;); } else if(&quot;Tomato&quot;.equals(name)){ return new Tomato(&quot;西红柿&quot;); } } return null; }}public class User { public static void main(String[] args) { new User().dinner(&quot;Potato&quot;); new User().dinner(&quot;Tomato&quot;); } private void dinner(String name){ Food food = FoodUtil.getFood(name); food.cook(); System.out.println(&quot;吃晚餐&quot; + food.getName()); }} 但是随着食物种类的不断增加，每一次新增一种食物，你都要去修改getFood这个方法，每次修改完毕之后，你还必须对其进行回归测试，万一改错了，或者equals方法调用者的key重复了，那就会影响到系统的运行了。而且这也不符合设计模式里面的开闭原则，对扩展开放，对修改关闭。所以此时继续对这个 Util 进行修改。 1234567public static Food getFood(Class&lt;? extends Food&gt; clazz) throws IllegalAccessException, InstantiationException { if(clazz != null){ // 在这里方便演示是设计模式，所以在Tomato 和 Potato加入了无参的构造器 return clazz.newInstance(); } return null; } 这就是一个简单工厂模式，简单工厂模式严格意义上来讲并不属于设计模式其中之一，但是这也是我们在开发过程中经常使用的一种方式，用于避免一些无意义的 new 操作。接下来就来讲下工厂方法模式。 工厂方法模式1234567891011121314151617181920//抽象的一个工厂public interface FoodFactory { Food createFood();}// 西红柿工厂public class PotatoFactory implements FoodFactory { @Override public Food createFood() { return new Potato(); }}// 土豆工厂public class TomatoFactory implements FoodFactory { @Override public Food createFood() { System.out.println(&quot;清洗干净&quot;); System.out.println(&quot;切好&quot;); return new Tomato(); }} 这个时候，假设我们要使用的话，则直接通过 new 一个子类即可，如下代码： 12345678public class User { public static void main(String[] args) { FoodFactory potatoFactory = new PotatoFactory(); FoodFactory tomatoFactory = new TomatoFactory(); tomatoFactory.createFood().cook(); potatoFactory.createFood().cook(); }} 这样做的好处是，对扩展开放，当需要添加其他食物的时候，直接新建一个Factory类然后实现FoodFactory接口即可，这就是工厂模式的好处。 抽象工厂模式抽象工厂模式是对工厂的更进一步抽象，例如 土豆 和 西红柿是一组产品，因为它们都同属于食物。但是现在有两个地方都可以生产 土豆 和 西红柿，A地的是金色的，B地生产的是银色的，于是此时 A 和 B 就产生了一个产品等级。但是A 和 B都是具有一组通用的特性，即使可以生产不同的颜色 土豆 和 西红柿。 12345678910111213141516171819202122232425262728293031323334353637383940public interface PlantFactory { Tomato plantTomato(); Potato plantPotato();}public class AFactory implements PlantFactory{ @Override public Tomato plantTomato() { System.out.println(&quot;plantTomato By A&quot;); return new Tomato(); } @Override public Potato plantPotato() { System.out.println(&quot;plantPotato By A&quot;); return new Potato(); }}public class BFactory implements PlantFactory { @Override public Tomato plantTomato() { System.out.println(&quot;plantTomato By B&quot;); return new Tomato(); } @Override public Potato plantPotato() { System.out.println(&quot;plantPotato By B&quot;); return new Potato(); }}public class User { public static void main(String[] args) { PlantFactory plantFactory = new AFactory(); plantFactory.plantPotato().cook(); }} 总的来说，这三种设计模式中，除了抽象工厂模式很少使用到，其他两种在平常开发的项目中基本上都可以看到。","link":"/2019/12/27/%E6%B5%85%E8%B0%88%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"title":"用FastThreadLocal替代ThreadLocal","text":"在多线程编程的环境中，一个变量是可以被多个线程访问并修改的，如果想让一个线程，在不影响到其他线程的情况下，修改此变量，那么就需要将该变量改成自己私有的，这就是 ThreadLocal 的作用了。 ThreadLocalThreadlocal 可以将一个变量作为自己的私有变量，可以在本线程内随意修改并且不影响到其他线程，如下 Demo： 可以看到 t1 打印出 Thread1，t2 打印出 Thread2，两个线程的 ThreadLocal 都不受影响。 多个 ThreadLocal在项目中，需要保存的值可能不仅仅一个，所有同一个 Thread 可以拥有多个 ThreadLocal，例如： 怎么实现的呢？在 JDK 中有一个 ThreadLocalMap，这个类的作用是将每一个ThreadLocal 作为 Key 保存于 Map 中，而 Map 的 value 就是我们在 ThreadLocal 中 set 的值了。 ThreadLocalMap在 Thread 中，有一个变量 threadLocals，其生命周期随着 Thread 的结束而结束。 Map如果大家的 JDK 八股文背的熟的话，应该可以记得在 Map 的接口中，有一个 Entry 的接口。 没错，就是这个，每一个实现 Map 接口类都必须实现这个 Entry 这里顺带提一句，接口里面是可以有内部接口的，跟内部类实现是一样的 然而 ThreadLocalMap 的 Entry 却跟这个没关系，在这里只是提一下这个 ThreadLocalMap 的 Entry ThreadLocalMap 的 Entry 是一个单独的类，同时是一个 WeakReference，关于 WeakReference，因为跟本文关系涉及关系不大，在这里就可以简单的理解为是下一次发生 GC 的时候，WeakReference 所持有的对象就会被回收了。 ThreadLocal 进行新增当我们向一个 ThreadLocal 进行赋值的时候，最终都会包装成一个 Entry，而 Key 就是 ThreadLocal， 所以 JDK 中的 ThreadLocal，其实就是作为一个 Key 存放于 ThreadLocalMap 中，而 ThreadLocal 的 value 就做为 ThreadLocalMap 的 value。 弊端当 ThreadLocalMap 用 Hash 来计算下标的时候，就不可避免会带来 Hash 碰撞问题，在 ThreadLocalMap 中解决 Hash冲突的方法是 线性探测 法，即通过 Hash 计算下标，如果当前下标已经有值，那么就直接寻找下一个空下标，然后赋值。 如果在大量的碰撞，就会浪费大量的 CPU 资源。 FastThreadLocal这是 Netty 包里面的另一个 ThreadLocal 实现，它的功能和 ThreadLocal 类似，都是可以将一个变量设置为线程的副本，而装载 FastThreadLocal 的容器是 InternalThreadLocalMap。 Netty 中的实现，整体上和 JDK 中 ThreadLocal 没什么区别，但是具体细节上却是千差万别，先看 Netty 中的 InternalThreadLocalMap。 InternalThreadLocalMap这是由 netty 实现的一个 FastThreadLocal 的容器，类似于 JDK 中的 ThreadLocalMap，虽然是 Map，但是其内部是由数组来实现的。 UnpaddedInternalThreadLocalMap 是 InternalThreadLocalMap 的一个父类，定义了一些基本的变量。 这里的 indexedVariables 就是用来存放 FastThreadLocal 的下标，在 Netty 中，每一个 FastThreadLocal 都会有一个唯一的下标，从而在查找 value 的时候，直接通过下标在数组中进行定位。 而下标的产生就跟上图的 nextIndex 有关系了。 创建一个 FastThreadLocal 的流程如下： 通过 nextIndex 变量计算得到下一个下标 如果当前下标小于 indexedVariables 的长度， 那么会进行扩容（一个损耗点） 如果当前下标小于 indexedVariables 的长度，则直接进行赋值 开销FastThreadLocal 的开销主要是在创建的时候分配 index 所产生的 CAS 竞争，后续就没有任何大的开销了，查询的时候也是直接通过下标就可以定位。 使用对于一般的业务开发，ThreadLocal 已经够用了，没有必要更换为 FastThreadLocal，相比于 Hash 计算产生的那点影响，网络、IO 产生的性能消耗才是重点。 而 FastThreadLocal 的使用场景，一般事在一些需要处理大量数据的项目中，例如一个项目需要实时调用外部接口来获取数据，同时又用到了 ThreadLocal 来进行存储一些基本的信息。 那么这个时候是可以考虑将 ThreadLocal 替换为 FastThreadLocal，首先是这种项目 GC 非常的频繁，一旦处理不好就会导致 Entry 中的 ThreadLocal 被回收，其次这种项目大部分会用 Netty 来进行网络传输，自带 FastThreadocal，不会担心额外的引入第三方包产生的风险。 杂谈内存泄漏在使用 JDK 自带的 ThreadLocal 的时候，使用不当可能会产生内存泄漏： 因 GC 导致 Entry 的 ThreadLocal 被回收 线程长时间的运行，并且 ThreadLocal 没有被及时的 remove 因 GC 导致 Entry 中的 ThreadLocal 被回收主要原因是 ThreadLocalMap 的生命周期和 Thread 是一致的，而 ThreadLocalMap 中的 Entry 的 Key 是 WeadReference，一旦发生GC，Key 可能被直接释放，但是 value 因为一直被 Entry 引用，所以导致无法被及时释放。 线程长时间的运行，并且 ThreadLocal 没有被及时的 RemoveThread 在使用完以后没有及时的 remove ThreadLocal，这种情况多发生于线程池的使用，如果项目中有一个通用线程池，有人在使用以后忘记进行将本次任务的 ThreadLocal 进行 remove，就会导致这部分的 ThreadLocal 也无法释放，引发内存泄漏。 FastThreadLocalFastThreadLocal 的做法是直接将 Object 数据强引用，所以除非是真正的内存不足，否则 FastThreadLocal 中的值不会被回收。 并且 FastThreadlocal 在每一次线程执行完毕以后，都会主动的进行 remove 操作，可以避免第二点导致的内存泄漏，但是对于第一点需要开发人员注意。","link":"/2022/10/10/%E7%94%A8FastThreadLocal%E6%9B%BF%E4%BB%A3ThreadLocal/"},{"title":"类的初始化以及静态变量优化","text":"前言JVM 规范中写道：在加载类的时候，分为如下几个大类： Creation and Loading Linking Initialization 每一步在 JVM 中都规定了具体的几个小节，但是今天本文的重点在于链接阶段，JVM 对一些静态变量做的一些优化，因此对于这里面的每一步具体是做什么的不展开讨论了。 类的初始化首先上一段代码： 123456789101112131415161718public class Test1 { static { System.out.println(&quot;Test1&quot;); } public static void main(String[] args) { System.out.println(Test2.test2); }}public class Test2 { public static final String test2 = &quot;static final test2&quot;; static { System.out.println(&quot;Test2&quot;); }} 以上代码的运行结果你知道吗，如果知道的话那么就可以不用往下看了，如果不知道的话可能就涉及到你的知识盲区了。上面的代码运行结果如下： 12Test1static final test2 是不是觉得很奇怪，为什么 Test2 的静态代码块没有执行，按照 JVM 的规范，一个类在加载的过程中，会对变量、代码块进行一些初始化赋值，那么 Test2 的代码块为什么不会被执行呢？那么不妨将 Test2 进行调整，将 test2 的类型由 String 改成 Integer： 123456789101112131415161718public class TestInteger1 { static { System.out.println(&quot;TestInteger1&quot;); } public static void main(String[] args) { System.out.println(TestInteger2.testInteger2); }}public class TestInteger2 { public static final Integer testInteger2 = 1; static { System.out.println(&quot;TestInteger2&quot;); }} 而此时的打印结果如下： 123TestInteger1TestInteger21 看到这里是不是觉得奇怪，为什么一个是 String，一个是 Integer 就会导致两种结果。 静态变量被 static 关键字修饰的变量称之为静态变量，通常一个类调用另一类的静态变量在 JVM 中是通过 getstatic 指令来实现的，而 getstatic 又是一个会触发类初始化的指令，参照 JVM规范5.5小节明确提到了 getstatic 会使一个类进行初始化，而调用一个静态变量的字节码指令就是 getstatic，那么不妨反编译 Test1 这个类看下： 反编译出来的竟然是 ldc，ldc 表示的是从常量池中加载值，那么自然就不会引起 Test2 进行初始化了，所以就不会打印出 Test2 了，那么 TestInteger1 类又是做了什么操作呢？ 看 TestInteger1 编译成的字节码发现，在调用 TestInteger2 类的时候，使用的是 getstatic，因为此时系统内 TestInteger2 还没有被初始化，所以此时 TestInteger2 就会首次初始化初始化，因此就会导致 TestInteger2 被打印出来。 不使用 final既然使用 final 会导致两种结果，那么通过控制变量法，将 Test2 和 TestInteger2 都进行修改，将修饰变量的 final 去掉，代码就不在这里贴出来了，直接说结果： Test2 和 TestInteger2 都会被打印出来 那么这个说明这个优化跟 final 以及 String 有关？ finalfinal 在 Java 中有如下两个作用： 修饰的基本变量值不可变 修饰的对象其引用不可变 同样是由 final 修饰的 String 和 Integer，会产生如此大的差异呢？ 常量池在 Java 中，String a = “a”，会产生一个对象，但是这个对象不是在堆上，而是直接在常量池中分配的，而 jvm 加载一个常量池中的对象就是直接采用 ldc。而 TestInteger2 这个类的 testInteger2 是一个 Integer 类型，所以是直接在堆中进行对象的分配，从而导致加载其的命令必须是 getstatic。 这时候可能会有小伙伴有疑问了，既然加不加 final 都可以进常量池，那么为什么还需要 final 修饰？ 这是因为 final 可以保证其引用不可变，如果不加 final，此时另外一个对象将 Test2 的 test2 修改为 Test3，如果 Test1 此时还用 ldc 去常量池中加载，那么肯定是会得到一个错误的结果，因此 JVM 放弃了对这种优化。如果 String 是由 final 修饰的，那么 JVM 判断这个变量已经没有地方可以修改其引用了，因此直接在编译期间直接该引用替换为常量，具体如下： Integer 的包装类既然 Integer 是 int 的包装类，而基本类型也是在常量池中进行分配的，是不是只要将 Integr 修改为 int，就可以避免 TestInteger2 的初始化呢？答案是确实可以的，具体就不上代码了。 ldc在这里顺带说一下 ldc，ldc 代表的是从常量池中加载数据，如果 Test2.test2 不是通过 String test2 = &quot;static final test2&quot; 这种方式赋值的话，同样也是会引起 Test2 的初始化的。 1public static final String test2 = new String(&quot;static final test2&quot;); 这种方式由于 test2 指向的并不是常量池，所以并不会采用 ldc 加载，而是通过 getstatic，所以这种情况下 Test2 依然被打印出来了。 最后其实这些都是 JVM 的小细节，在平时的业务代码中如果遇到这种静态变量，不妨多用基本类型以及final String 来进行优化下，说不定会对系统有那么丢丢的提升。","link":"/2021/08/01/%E7%B1%BB%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E4%BB%A5%E5%8F%8A%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8F%E4%BC%98%E5%8C%96/"},{"title":"线程池是如何关闭非核心线程的","text":"在 Java 中，多线程的核心实现类是 ThreadPoolExecutor，该类提供了多线程的几个参数，用于开发人员自定义自己的线程池。 线程池的参数12345public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize,long keepAliveTime, TimeUnit unit,BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory, RejectedExecutionHandler handler) { //... } 线程池一共有 7 个参数，其中跟本次相关的有三个，分别是 corePoolSize、maximumPoolSize、keepAliveTime，这三个参数代表的意思如下： corePoolSize：当线程池中的任务数小于 corePoolSize 或者线程池中的任务数大于 corePoolSize 但是小于阻塞队列的最大长度，那么线程池中的核心线程数就睡 &lt;= corePoolSize maximumPoolSize：当线程池中的任务数已经达到队列上限并且线程池中的线程数 &lt; maximumPoolSize，此时线程池就会将线程数增加至 maximumPoolSize keepAliveTime：代表线程的空闲时间，也就是线程等待多久以后可以被销毁 如果一个线程池中，线程数已经达到了 maximumPoolSize，如果后续任务数减少，此时就会销毁多余的线程，具体保留多少线程数还是需要依据线程池的具体参数，例如如果配置了 allowCoreThreadTimeOut，则所有线程都有被回收的可能。 线程池的状态线程池一共有五种状态，按照源代码中的定义分别如下： 12345private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;-536870912private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;0private static final int STOP = 1 &lt;&lt; COUNT_BITS;536870912private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;1073741824private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;1610612736 其中状态的变化如下图： RUNNING 表示该线程池一切正常，可以接受新的任务并且队列中的任务也会执行 SHUTDOWN 表示该线程池拒绝接受新的任务，处于队列中的任务也会执行完毕 STOP 表示该线程池拒绝接受新的任务，而且队列中的任务不会执行，并且会向正在执行中的任务发送一个中断指令 TIDYING 表示所有线程已经被终止，而且工作线程已经为 0，线程池即将去执行 terminated 钩子函数 TERMINATED terminated() 函数已经执行完毕，线程池被销毁 上面铺垫这么多了，现在可以回到主题了 线程池是如何关闭非核心线程的首先，还记得第一个标题里面的 keepAliveTime、workQueue 吗，线程池中的线程如果需要被销毁，满足下面任意一个条件即可： 线程中的业务代码执行出现了异常 线程池中的线程在等待了 keepAliveTime 后还是拿不到任务 下面从线程开始获取任务的地方开始： 12345678910111213141516171819202122232425262728293031323334private Runnable getTask() { boolean timedOut = false; // Did the last poll() time out? for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } int wc = workerCountOf(c); // Are workers subject to culling? // 如果允许核心线程被销毁或者此时总线程数 &gt; core，可以进行线程的销毁 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) { if (compareAndDecrementWorkerCount(c)) return null; continue; } try { // 当 timed 为 true，那么会等待 timeout 时间，如果为 false，则直接阻塞，除非拿到任务 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } } 在 getTask 方法中，第一步会判断线程池的状态，如果是处于 SHUTDOWN &amp;&amp; 队列为空，或者是处于 STOP 状态，则可以直接减少当前的工作数量了，因为线程池的状态流转到 TIDYING 的条件是：线程数为 0 且队列为空，而此时队列已经是空了，所以只需要减少当前的工作线程数就可以完成状态的流转。 如果此时的线程池状态为 RUNNING，那么会进行两个判断： 是否允许 core 线程销毁或 当前的线程数是否大于 corePoolSize 这两个判断就决定当前线程是否会被销毁，线程池中的线程通过 poll(long timeout, TimeUnit unit) 这个函数来获取任务，在等待 timeout 时间以后，就会将 timedOut 设置为 true，表示已经等待了 keepAliveTime 时间了，但是还没有拿到任务。 此时 (wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty()) 这个判断就很容易通过了，于是会通过 CAS 操作原子性的减少工作线程数，一旦设置成功，则会直接返回 null，然后就会进行线程的销毁。 processWorkerExit当拿不到任务的时候，就会执行 processWorkerExit，如果线程执行的过程中出现了异常也是从这个方法进入并进行销毁的，不过线程执行出错的情况下 completedAbruptly 是 true。 12345while (task != null || (task = getTask()) != null) { // ... } finally { processWorkerExit(w, completedAbruptly); } processWorkerExit 主要是做了以下几个操作： 上报该 worker 已经执行完毕的任务数，并且从 workers 集合中移除该 worker。 执行 tryTerminate() 判断是否是异常导致的线程销毁，如果是的则补充一个 worker，如果不是的，则判断当前 worker 数量是否大于线程池允许的最小数，如果小于的话也会补充 worker。 其中最关键是这里面的第2步 tryTerminate 方法 tryTerminatetryTerminate 会判断当前线程池的状态，如果符合一定的条件，那么就会让线程池向 TERMINATED 进行转换。 如果此时的线程池满足以下任意一个条件，则不会做任何的操作，直接返回： 线程池处于 RUNNING、TIDYING、TERMINATED。 正在处于 SHUTDOWN，但是队列中还有等待执行的任务。 当一个线程池不满足以上的情况的时候，那么剩下的就是： 处于 STOP 处于 SHUTDOWN，但是队列已空 从上面得知，STOP 状态下是队列中的任务是不会进行处理的，所以此时将线程池状态由 STOP 向 TERMINATED 是合理的。而如果处于 SHUTDOWN 且队列为空，此时也是可以向 TERMINATED 转移的。而且在转移的过程中，如果判断此时线程池中的 worker 数量大于 0，那么会直接随机中断一个线程，然后返回，保留线程池的当前状态。 这里之所以尝试中断一个线程，是因为假设线程池在执行了 shutdown 之后，如果队列中还有任务，此时并不会向 TERMINATED 转换，但是此时由于 shutdown 中的中断方法已经执行了，所以后续就需要再次中断 worker，而这里加一个判断，就是处理这种情况。 而如果上述的判断都是 false，此时证明已经可以转为 TERMINATED 了。转换的代码如下： 123456789101112131415final ReentrantLock mainLock = this.mainLock;mainLock.lock();try { if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) { try { terminated(); } finally { ctl.set(ctlOf(TERMINATED, 0)); termination.signalAll(); } return; }} finally { mainLock.unlock();} 修改线程状态为 TIDYING，然后执行 terminated() 函数，最后再唤醒所有的 termination() terminationtermination 这个变量是一个 condition，在 ThreadPoolExecutor 中只有两个地方出现过，一个是在 awaitTermination 中，另一个就是在 termination，不过在 awaitTermination 是等待获取线程池的状态： 1234567891011121314151617public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException { long nanos = unit.toNanos(timeout); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { for (;;) { if (runStateAtLeast(ctl.get(), TERMINATED)) return true; if (nanos &lt;= 0) return false; nanos = termination.awaitNanos(nanos); } } finally { mainLock.unlock(); }} 而线程池已经到了 TERMINATED，自然可以唤醒 termination了。","link":"/2021/07/21/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%98%AF%E5%A6%82%E4%BD%95%E5%85%B3%E9%97%AD%E9%9D%9E%E6%A0%B8%E5%BF%83%E7%BA%BF%E7%A8%8B%E7%9A%84/"},{"title":"记一次parallelStream错误使用导致的NullPointerException","text":"parallelStream在 Java8 中，新增了一个很有用的功能就是 流，这个功能可以使我们可以快速的写出优雅的代码，其中 stream 是一个串行流（说法可能有误…就是不会采取多线程来进行处理）。还有一种就是 parallelStream 采用 ForkJoinPool 来实现并发，加快执行效率。 所以在使用 parallelStream 的时候一定要注意线程安全的问题，首先看一段代码： 在这段代码中，是首先判断 dataListList 里面对象的 name 属性是否是偶数，是的话则添加至偶数List，反之则添加至奇数List。 然后开始测试这段代码： 12345public static void main(String[] args) { for(;;){ DataListTest.test(); }} 运行一段时间你就会发现，会出现 NullPointerException，这是因为在之前的 forEach 里面 ArrayList 是一个非线程安全的集合，而 parallelStream 是一个多线程的流，所以就会导致 ArrayList 在并发插入的时候，会出现部分元素是null的情况。具体原因如下： ArrayList并发不安全ArrayList并发不安全的点在于 add 方法里面没有使用锁来保证线程安全，下面是 add 的代码： 12345public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;} 假设不慎将 ArrayList 用于并发的环境，那么当第一个线程读取到 size 是 0，第二个线程也读取到该 size 也是0，然后都执行完了 ensureCapacityInternal 方法，此时线程一执行完 size++ 后被挂起，然后线程二也执行了 size++，那么此时无论哪一个线程先执行数组的赋值操作，它的值一定会被另一个线程所覆盖。 回到上面并发流空指针那一个例子 出现异常情况的 List 如下： 然后在 Collectors.toMap() 方法里面最终会调用到 HashMap 的 merge 方法，而 HashMap 的 merge 方法第一行就是判断 value 是否为空，所以就导致了 NullPointerException 总结在使用 parallelStream 一定需要注意并发的安全，同时注意在重构代码的时候如果是这种并发流的话，一定要注意线程安全。","link":"/2020/05/11/%E8%AE%B0%E4%B8%80%E6%AC%A1parallelStream%E9%94%99%E8%AF%AF%E4%BD%BF%E7%94%A8%E5%AF%BC%E8%87%B4%E7%9A%84NullPointerException/"},{"title":"记一次springboot-mybatis找不到dao层的错误","text":"写了三个多月的Python，今天再写一个springboot-mybatis的项目的时候好多东西都忘记了，尤其是在今天下午遇到了一个关于mybatis的错误： 12345678Description:Field peopleDao in com.example.serviceImpl.PeopleServiceImpl required a bean of type 'com.example.dao.PeopleDao' that could not be found.Action:Consider defining a bean of type 'com.example.dao.PeopleDao' in your configuration 很明显就是dao层无法被扫描到，在一下午的尝试中，首先检查了包结构，如下:com—example——controller————XXX.java——dao————XXX.java——–***——Application.java 也就是说这个包结构是完全符合springboot的规范的，也就表示在启动类上面的注解是完全可以扫描到dao包下面的那个接口的。所以注解不存在问题。 然后尝试了第二种方法。修改启动类的扫描结构： 1234567@SpringBootApplication@ComponentScan(basePackages = {com.example.dao})public class IpApplication { public static void main(String[] args) { SpringApplication.run(IpApplication.class, args); }} 修改之后确实不会报错了但是有一个问题就是除了这个dao下面的包可以被扫描之后其他的例如controller包都无法被扫描进来。所以也是无法根本解决问题， 最后无意间注意到了在Springboot的启动日志中出现了一句话：NO Mybatis mapperXXX后面的具体忘了，也就是说在这里根本就没有扫描到mapper，后来发现是查了一个jar包 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 也就是这个jar包导致了那个mapper一直找不到，不过具体原因待后面时间再补充","link":"/2018/03/05/%E8%AE%B0%E4%B8%80%E6%AC%A1springboot-mybatis%E6%89%BE%E4%B8%8D%E5%88%B0dao%E5%B1%82%E7%9A%84%E9%94%99%E8%AF%AF/"},{"title":"springcloud使用feign导致的Excepted authority 错误(三)","text":"今天在用Feign的时候遇到了一个BUG，这个BUG虽然不是很难，但是由于网上没什么解决办法，而是自己的DEBUG解决的，所以暂且记录下： 异常就是java.net.URISyntaxException: Expected authority at index 7: http://，刚开始的时候一头雾水，在Google了一遍之后并未找出解决办法，后来又尝试了在代码中进行了DEBUG，发现代码嵌套的太深了，所以没办法走下去，之后便去StackOverflow中查看了下也没有什么头绪在这里也检查过了是不是服务没注册还是Feign的注解微服务名称是不是有问题，都显示是正常的，并且可以通过这个名称获取其对应的地址 后来今早起来的时候再次逛StackOverflow看到有人提示try to debug into the LoadBalancerFeignClient.cleanUrl() 。查看了下这个方法的代码： 123static URI cleanUrl(String originalUrl, String host) { return URI.create(originalUrl.replaceFirst(host, &quot;&quot;)); } 猜测了下应该是负责创建Feign相关URL的一个类，所以尝试在这里DEBUG，然后再开启一个正常的可以使用Feign的服务，最后发现这两个服务的区别是，在这个出错的微服务里面发出的请求是http://xxx,而正常的微服务是http://xxx/user/name/XX,所以问题马上就定位出来了，就是请求的问题， 查看Feign的接口，发现在方法上面的@RequestMapping出了问题，出错的配置如下:@RequestMapping(name = &quot;movie/user/{username}&quot; ,method = RequestMethod.GET),这里其实应该是value而不是name，至于name的作用参照网上的说法如下(自己并未证实):假设在UserController中有一个getUser()方法，那么此时如下： 12345public class UserController{ @RequestMapping(name=&quot;ceshifangfa&quot;,value=&quot;/getuser&quot;,method=RequestMethod.GET)(){ }} 在jsp页面是可以通过name属性来访问这个接口： 1&lt;a href=&quot;${s:mvcUrl('UC#ceshifangfa').build()}&quot;&gt;获取用户&lt;/a&gt; 那么此时在这里使用name的话导致了Feign找不到了访问的URL，所以直接抛出异常。最后修改为value，一切正常 总结在碰到问题的时候自己的思路一开始是没错的，但是当时不知道如何找出那个Feign的发请求方法，所以一直卡在这里了。提示：以后再StackOverflow看到别人的回答的时候可以稍微仔细的思考下","link":"/2018/04/21/%E8%AE%B0%E4%B8%80%E6%AC%A1springcloud%E4%BD%BF%E7%94%A8feign%E5%AF%BC%E8%87%B4%E7%9A%84Excepted-authority-%E9%94%99%E8%AF%AF-%E4%B8%89/"},{"title":"记一次sql和时间段查询有关的语句","text":"前言由于自己有点需求需要在mysql中按照时间段进行查询，而自己又对这些不太了解，所以趁着这次机会将mysql和时间段相关的查询语句做一个记录： 常用函数：DATE_SUB()函数DATE_SUB(date,INTERVAL expr type)这个函数的date是一个时间表达式，一般取得是数据库中的一个字段。后面的INTERVAL一般来讲是不变的，expr一般是一个时间段，代表过去的，比如是30天，那么这里就是30，若是60，这里就是60，type则表示的是一个时间属性(可能表达的不是很准确)，如下： 123456789101112131415161718192021MICROSECONDSECONDMINUTEHOURDAYWEEKMONTHQUARTERYEARSECOND_MICROSECONDMINUTE_MICROSECONDMINUTE_SECONDHOUR_MICROSECONDHOUR_SECONDHOUR_MINUTEDAY_MICROSECONDDAY_SECONDDAY_MINUTEDAY_HOURYEAR_MONTH 在这里我用YEAR_MONTH 和 MONTH 同时写了一个sqlSELECT DATE_SUB('2015-09-30 11:19:00',INTERVAL2 MONTH)在这个里面，将MONTH和YEAR_MONTH互换都是没问题的，这是目前暂时未发现有什么差异的。 FROM_UNIXTIME()函数这是一个非常有用的函数，其作用是将一串数字或者一个时间戳转换成指定格式的一个日期，在这个里面的话第一个参数需要是一个UNIX时间戳,而第二个参数则是一个format的字符串，注意观察下面的三个例子的不同，第二个sql在后面做了一个+0的操作结果日期格式直接被转换成了一串数字。 12345678mysql&gt; SELECT FROM_UNIXTIME(1447430881); -&gt; '2015-11-13 10:08:01'mysql&gt; SELECT FROM_UNIXTIME(1447430881) + 0; -&gt; 20151113100801mysql&gt; SELECT FROM_UNIXTIME(UNIX_TIMESTAMP(), -&gt; '%Y %D %M %h:%i:%s %x'); -&gt; '2015 13th November 10:08:01 2015' CONCAT()函数CONCAT()函数可以将一个字符串按照第二个参数，具体来说就是对应以下语句SELECT LEFT('2018-01-26 11:50:09',7)这条语句的执行结果结果是2018-01,正好是从头截取了7个字符。参数：第一个是字符串，第二个是截取的长度。 12345678mysql&gt; SELECT LEFT('2018-01-26 11:50:09',7);+-------------------------------+| LEFT('2018-01-26 11:50:09',7) |+-------------------------------+| 2018-01 |+-------------------------------+1 row in set (0.00 sec) LEFT()函数LEFT()函数可以将一个字符串作为mysql的语句来执行，类比到Python就是python中的eval函数，但是该函数若执行的字符串中包含一个null:注意这个null若是关键字则是会则会导致 1234567mysql&gt; select concat('Mysql',null);+----------------------+| concat('Mysql',null) |+----------------------+| NULL |+----------------------+1 row in set (0.00 sec) sum()和Count()在统计1和0的区别这两个函数的区别在一次使用统计的时候遇到了一点问题，例如一个字段是标识符，也就是一个字段的值除了0就是1，那么要统计为1的话一般来讲就是case when xxx=1 then 1 else 0 end这样就可以值选择为1的行，那么这里如果使用的是count的话，这里的这个规则是无法应用的，具体如下： 123456789101112mysql&gt; select * from testestatus;+-----------+---------------+| status_id | status_status |+-----------+---------------+| 1 | b'1'|| 2 | b'0'|| 3 | b'1'|| 4 | b'1'|+-----------+---------------+4 rows in set (0.00 sec) 因为当时没有仔细的区分这两种用法的区别，所以才导致了统计错误。在这里，count只是一个统计，他只是统计应用规则之后的的符合数据，但是sum不同，sum配合case when 则可以选择出相匹配的一些数据 DATE_FORMAT()函数这个函数可以将一个时间按照指定的格式输出，类似于Java中的dateformat函数，是可以将一个时间进行格式的。 1234567mysql&gt; select date_format(now(),'%Y');+-------------------------+| date_format(now(),'%Y') |+-------------------------+| 2018 |+-------------------------+1 row in set (0.00 sec) 一些常用函数组合：查询本月的数据：在这里是可以使用concat函数和left函数，首先可以使用left函数截取一个字符串长度。比如获取当月的话是可以通过left来获取然后再将1号拼接上去，最后用数据库的日期再减去1号就可以了。 1234567891011121314151617select left(now(),7);+---------------+| left(now(),7) |+---------------+| 2018-03 |+---------------+1 row in set (0.00 sec)mysql&gt; select concat(left(now(),7),'-01');+-----------------------------+| concat(left(now(),7),'-01') |+-----------------------------+| 2018-03-01 |+-----------------------------+1 row in set (0.00 sec) 最后本月的1号就被查询出来了，然后便可以进行数据操作了 查询指定天数的数据假设现在需要查询往期30天的数据，那么可以通过date_sub函数来进行查询： 12345678select date_sub(now(),INTERVAL 30 DAY );+----------------------------------+| date_sub(now(),INTERVAL 30 DAY ) |+----------------------------------+| 2018-02-22 22:03:22 |+----------------------------------+1 row in set (0.00 sec) 总结：目前关于时间方面的sql总结差不多就是这些了，其他的以后再进行补充。","link":"/2018/03/23/%E8%AE%B0%E4%B8%80%E6%AC%A1sql%E5%92%8C%E6%97%B6%E9%97%B4%E6%AE%B5%E6%9F%A5%E8%AF%A2%E6%9C%89%E5%85%B3%E7%9A%84%E8%AF%AD%E5%8F%A5/"},{"title":"记一次sql嵌套查询的使用方法","text":"记一次sql嵌套查询的使用方法一般来讲在sql中嵌套查询在where之后以便于查询范围的限制。现在有一个情况就是就是现在嵌套查询的话是需要查询出结果值然后返回为一个字段。 思路：思路一：既然需要返回的是一个字段，那么是需要一个嵌套查询，所以一般的表达式是：select &lt;表达式&gt;（select&lt;表达式&gt;）as name这样作为一个查询。首先在括号()里面的一个select语句是可以作为一个字段的，可以通过在后面加一个as 字段从而返回的是一个字段。、那么这样书写sql之后便可以作为一个字段然后返回结果了。但是这样写有一个弊端，就是若没有groupby，则会导致查询出来的数据会有多余重复得。类似下面这条sql： 123SELECT * ,(SELECT SUM(CASE WHEN car_information.`car_status` ='1' THEN 1 ELSE 0 END)FROM car_information WHERE car_information.`cat_place`='武汉') AS 'weifacishi' FROM car LEFT JOIN car_information ON car.`car_num`=car_information.`car_num` ; 查询结果： 可以看到重复了许多数据，所以表明需要实现这个查询集合需要修改sql语句。 思路二：当看到这种写法出现重复得时候，便决定这个sql必须进行groupby，首先写了一个sql来测试groupby得，发现可以。 12SELECT car_information.`car_num`,car_information.`cat_place`, SUM(CASE WHEN car_information.`car_status` = '1' THEN 1 ELSE 0 END) AS 'weifacishi' FROM car_information GROUP BY car_information.`car_num`; 查询结果： 发现，思路确实是对的，只不过需要对sql重新进行排序。 思路三：有了上面得两步之后便差不多知道如何进行操作了：首先应该进行一个左连接，以car表为左表，car_information为右表。连接之后再这个结果集中进行groupby然后统计。 1SELECT * FROM (SELECT car.`car_num`,car.`car_wearhouse`,car_information.`car_status` FROM car LEFT JOIN car_information ON car.`car_num`=car_information.`car_num`) AS t GROUP BY t.car_num; 结果如下： 总结： 在sql语句中，几个表之间得连接是可以做一个结果集得，这个结果集可以通过as命名为别名然后通过操作这个别名来操作； 不理解得地方： 在sql中select t.XXX这个t是后来才被命名得，但是在之前却是可以使用得，因此想知道sql得运行原理—以后再了解 在sql中统计得用法case when XX=='XX' then 1 else 0 end，并且嵌套得使用得() as ‘XXX’ 其余的以后再补充 补充一：在sql中左连接和内连接的优先选择问题，内连接会返回在两表中都包含的值，而左连接的话，当右边的表数据在左表中不存在的时候会出现null，而内连接则不会产生这种情况。还有就是UNION的使用，union在两张表返回的结果集是一样的话而且两表需要拼接的话是非常有效的","link":"/2018/03/16/%E8%AE%B0%E4%B8%80%E6%AC%A1sql%E5%B5%8C%E5%A5%97%E6%9F%A5%E8%AF%A2%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"},{"title":"记一次使用Spring5构件Web的过程","text":"由于当时在学习Spring的时候还是在一年前，那时候Spring才是刚到4.3还是4.5.然后做了一个项目之后便了解到了SpringBoot，于是一直在用SpringBoot，所以导致现在配置起来就有点忘记了。所以现在记录下此次配置的过程中所遇到的坑。 踩过的坑：遇到在web.xml中分发请求的类找不到第一个坑就是org.springframework.web.servlet.DispatcherServlet这个类一直找不到；于是在POM中添加了个各种依赖终于发现缺少Spring Web MVC这个依赖包。。。 12345&lt;servlet&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;&lt;!-- 需要webmvc这个jar包--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; 开启Tomcat的时候一直提示什么Cache错误错误提示：CacheManager No Bean Found - Not Trying to setup any Cache 当时一直在到处查找是否开启数据库什么的，后来在StackOverFlow中查询到了问题的解决办法：StackOverFlow中关于Cache的错误根据他人的答案做一个总结就是：在配置文件中发现若文件中含有&lt;tx的话，IDEA会自动的将引入类似***/cache等的xsd文件，所以导致了这个异常的出现。所以一般来说又两种解决办法。 用IDEA的模板直接创建spring的配置文件：过程：File -&gt; new -&gt; XMLConfigurationFile -&gt; Spring Config 然后Spring自动的将所需要的命名空间添加到了新建的xml文件中。此时只需要添加相应的标签即可。 删除导致异常的xsd步骤：在xml配置文件中删除掉带有cache的的xsd 静态资源的配置和引用在去年配置静态资源的时候与今年的相差无几，但是在引用的时候却出现了一些变化。去年做题的网站的代码： 123456789101112131415161718192021去年获取静态资源的配置： &lt;mvc:resources mapping=&quot;/static/**&quot; location=&quot;/static/&quot; /&gt; &lt;mvc:resources mapping=&quot;/static/**&quot; location=&quot;/static/&quot; /&gt; &lt;mvc:resources mapping=&quot;/uploadimg/**&quot; location=&quot;/uploadimg/&quot; /&gt;去年的jsp文件中的引用：&lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;utf-8&quot;%&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;解题目录&lt;/title&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;${pageContext.request.contextPath}/static/js/jquery-3.1.1.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;${pageContext.request.contextPath }/static/js/bootstrap.js&quot;&gt;&lt;/script&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;${pageContext.request.contextPath }/static/css/bootstrap.css&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;${pageContext.request.contextPath }/static/css/bootstrap-theme.css&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;${pageContext.request.contextPath }/static/css/examination.css&quot;&gt; 很明显可以看到：在jsp中是首先获取到项目目录然后以绝对路径来获取静态资源 今天新建的项目： 1234567891011121314xml配置文件： &lt;mvc:resources mapping=&quot;/static/**&quot; location=&quot;/WEB-INF/static/&quot; /&gt; &lt;mvc:resources mapping=&quot;/static/**&quot; location=&quot;/WEB-INF/static/&quot; /&gt; &lt;mvc:resources mapping=&quot;/uploadimg/**&quot; location=&quot;/WEB-INF/uploadimg/&quot; /&gt;jsp文件中的引用：&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link href=&quot;../static/css/TestPage.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;/head&gt; 在这里会发现今天在配置文件中的引用多了一个WEB-INF然后再是static文件夹 关于为什么xml现在默认在WEB-INF中：在去年的时候记得spring的配置文件都是在resources中的，但是今天却发现在resources中配置的话启动tomcat会导致一条提示就是IOException parsing XML document from ServletContext resource [/WEB-INF/XXXXXX]后来查询到发现是可以修改的，默认的话直接在WEB-INF中按照前面创建xml的方式创建一个即可，但是若需要修改的话可以参照一下配置： 12345678910111213141516171819这个xml配置表示的是基本配置的Application.xml的位置&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:ApplicationContext.xml &lt;/param-value&gt;&lt;/context-param&gt;这个是配置分发器的配置文件的位置：&lt;servlet&gt; &lt;servlet-name&gt;springDefault&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:spring-mvc.xml &lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; 可以发现在去年的时候都是制定了配置文件的路径的，所以才可以在resources中配置xml，但是再看今年的配置： 12345678910&lt;servlet&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;&lt;!-- 需要wenmvc这个jar包--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 会发现都没配置好，所以会出现需要配置在默认的位置上。StackOverFlow中默认WEB-INF配置的解释","link":"/2018/03/20/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8Spring5%E6%9E%84%E4%BB%B6Web%E7%9A%84%E8%BF%87%E7%A8%8B/"},{"title":"记踩过的SpringMvc的一些坑","text":"时隔一年多，再次在新公司期间接触了SpringMvc，由于之前一段时间再用Python和SpringBoot做项目，所以一时间导致SpringMvc配置中出现了好多坑，遂逐一记录： 关于Dao层找不到的异常在配置的过程中这个异常出现的次数是最多的，也是最烦人的，一般是由于在Controller层中找不到Service层，然后Service层的Impl在自动装配dao的时候找不到dao，所以异常就会沿着service到达contrller层，但是总结起来，在今天的配置中遇到的情况主要又以下几种： web.xml中的配置出现了错误：在Spring5中默认xml文件是在WEB-INF中的，于是也就想着少配置一点是一点的原则，所以在web.xml中只是配置了分发器。但是今天却在其中发现了一些可能会导致Dao层找不到的原因，如下所示是我之前在web.xml中配置的一个详情： 12345678910&lt;servlet&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;&lt;!-- 需要wenmvc这个jar包--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 对，就是这么简单，所以也才导致了Dao层出错了，具体解释如下：在StackOverFlow中发现有一个人的解释十分的独特，大意是web.xml中的配置有问题： 1It is because the servlet-context.xml is placed inside the dispatcher servlet. Since dispatcher servlet is child of parent context, the parent context does not have the dependencies of child, So if you put the servlet-context.xml inside context param, and you must have the appServlet-servlet.xml inside the init param it will work fine. 于是我自己就修改了web.xml这个文件，修改之后的web.xml配置如下: 1234567891011121314151617181920212223&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:ApplicationContext.xml &lt;/param-value&gt; &lt;/context-param&gt; &lt;servlet&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:spring-mvc.xml &lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;spring-dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 然后马上就运行成功了。。。 这个就属于自己粗心了在mapper配置文件中我这边是返回的一个集合。然后标签打错了，就成了resultType于是一直出错，后来换成了正确的resultMap就正确了。 修改web.xml之后导致的命名空间出错了","link":"/2018/03/22/%E8%AE%B0%E8%B8%A9%E8%BF%87%E7%9A%84SpringMvc%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/"},{"title":"重温Java中String","text":"本文的内容都是基于 JDK1.8 来写的，主要是复习下 String 类的设计。 简介String 是一个用于存储字符串的类，其内部是通过 char 数组来实现，在 Java 中，1byte = 8bit，1char = 2byte, 所以在 Java 中，String 的code point是16位。String 类是由 final 关键字来修饰的，因此表明该类不可以被继承，同时 String 又实现了 Serializable、Comparable、CharSequence接口，表明 String 可以被序列化，以及使用cpmpareTo来比较两个字符串的值。 字符集编码内码内码指的是程序内部自己使用的字符集，java中是以 UTF-16 来表示的 The Java programming language represents text in sequences of 16-bit code units, using the UTF-16 encoding. UTF-16最多可以表示 65535 种字符，那么不在 65535 之内的字符，该如何表示呢，这时候就需要用两个字节来表示这个字符。以😁这个emoje为例子：其Code的编码是1F601。 12345具体方法是：Code Point减去0x10000， 得到的值是长度为20bit（不足的补0）；将得到数值的高位的10比特的值 加上0xD800的前6位得到第一个Code Unit。步骤1得到数值的低位的10比特的值 加上0xDC00的前6位得到第二个Code Unit。 于是计算方法如下： 12345678x = 1F601 - 0x10000 = 1111011000000001不满20位的补0；所以得到：00001111011000000001。0xD800：11011000000000000xDC00：1101110000000000于是高位的代理就是：1101100000111101于是低位的代理就是：1101111000000001最终得到：d83d,de01 在 String 的代码中体现如下： 1234567891011121314151617181920 public static int codePointAt(CharSequence seq, int index) { char c1 = seq.charAt(index); if (isHighSurrogate(c1) &amp;&amp; ++index &lt; seq.length()) { char c2 = seq.charAt(index); if (isLowSurrogate(c2)) { return toCodePoint(c1, c2); } } return c1;}public static int toCodePoint(char high, char low) { // Optimized form of: // return ((high - MIN_HIGH_SURROGATE) &lt;&lt; 10) // + (low - MIN_LOW_SURROGATE) // + MIN_SUPPLEMENTARY_CODE_POINT; return ((high &lt;&lt; 10) + low) + (MIN_SUPPLEMENTARY_CODE_POINT - (MIN_HIGH_SURROGATE &lt;&lt; 10) - MIN_LOW_SURROGATE);} 注意这个high &lt;&lt; 10是因为code转成16进制的时候，还补了4个0当 String 判断是否是一个 code point 的时候，首先会判断是否是高位代理，是的话在判断下一个字节是否是低位代理，是的就通过toCodePoint来推导出原来的code。 readResolve方法这个方法是为了保证序列化的时候，避免生成多个 String 对象，首先当一个对象被反序列化的时候，其调用链如下: ObjectInputStream -&gt; readObject() -&gt; readObject0(boolean unshared) -&gt; readOrdinaryObject(boolean unshared) 在readOrdinaryObject中，主要是需要注意如下两段代码： 12345678910111213141516171819202122232425262728293031Object obj;try { obj = desc.isInstantiable() ? desc.newInstance() : null;} catch (Exception ex) { throw (IOException) new InvalidClassException( desc.forClass().getName(), &quot;unable to create instance&quot;).initCause(ex);}if (obj != null &amp;&amp; handles.lookupException(passHandle) == null &amp;&amp; desc.hasReadResolveMethod()){ Object rep = desc.invokeReadResolve(obj); if (unshared &amp;&amp; rep.getClass().isArray()) { rep = cloneArray(rep); } if (rep != obj) { // Filter the replacement object if (rep != null) { if (rep.getClass().isArray()) { filterCheck(rep.getClass(), Array.getLength(rep)); } else { filterCheck(rep.getClass(), -1); } } handles.setObject(passHandle, obj = rep); }}ObjectStreamClass：readResolveMethod = getInheritableMethod(cl, &quot;readResolve&quot;, null, Object.class); 如果没有重写readResolve的话，那么此时便会直接返回newInstance 生成的新对象了。 final 的使用在 String 中，可以看到需要地方使用到了 final 字段，例如： 12private final char value[];private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; 这些都好理解，但是其中有一个方法里面的变量名却也使用了final，如下： 1234567891011121314private int indexOfSupplementary(int ch, int fromIndex) { if (Character.isValidCodePoint(ch)) { final char[] value = this.value; final char hi = Character.highSurrogate(ch); final char lo = Character.lowSurrogate(ch); final int max = value.length - 1; for (int i = fromIndex; i &lt; max; i++) { if (value[i] == hi &amp;&amp; value[i + 1] == lo) { return i; } } } return -1;} 在这里我个人感觉其实是没什么作用的，因为方法内部的value会随着this.value的变化而变化(一般不可能)，所以这里的final，也就hi和lo以及max有作用，可以保证同一个方法内部，在多线程的竞态条件下不改变。","link":"/2020/04/06/%E9%87%8D%E6%B8%A9Java%E4%B8%ADString/"},{"title":"重温生产者和消费者模型","text":"生产者和消费者模型，主要解决的是数据的同步问题，生产者将数据放置一个存储区域，然后消费者过来取数据。这种模式类似于一个中间件，可以使得生产者不需要关心消费者什么时候来拿数据，同时在这种模式下，还可以控制两边的处理速率，避免数据的丢失。 下面以 Java 为例，来写一个生产者和消费者模型。 当队列满了的时候，生产者自己进行阻塞。而当消费者发现队列为空，则将自己阻塞。 所以要实现这个生产者消费者模型，首先必须有以下条件： 生产者或者消费者必须支持可阻塞 在多线程的情况下，必须保证并发安全（即插入不能产生数据错误），取数据不可以重复取 阻塞队列在Java 中，常用的阻塞队列有 LinkedBlockingQueue 或者 ArrayBlockingQueue，这两个阻塞队列的实现都是基于 ReentrantLock ，通过可重入锁来控制并发情况下的插入操作。 所以，如下便是在 Java 中的生产者和消费者模型 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class BlockModel { static ThreadPoolExecutor product = new ThreadPoolExecutor(5,10,1000, TimeUnit.MICROSECONDS,new LinkedBlockingQueue&lt;Runnable&gt;(100)); static ThreadPoolExecutor consumer = new ThreadPoolExecutor(5,10,1000, TimeUnit.MICROSECONDS,new LinkedBlockingQueue&lt;Runnable&gt;(100)); public static void main(String[] args) throws InterruptedException { LinkedBlockingQueue linkedBlockingQueue = new LinkedBlockingQueue(10); for(int i =0 ;i&lt; 5 ;i++){ product.submit(new Producer(100,linkedBlockingQueue,i)); consumer.submit(new Consumer(linkedBlockingQueue)); } }}class Consumer implements Runnable{ LinkedBlockingQueue linkedBlockingQueue; public Consumer(LinkedBlockingQueue linkedBlockingQueue) { this.linkedBlockingQueue = linkedBlockingQueue; } public void run() { try { consumerStart(linkedBlockingQueue); } catch (InterruptedException e) { e.printStackTrace(); } } private void consumerStart(LinkedBlockingQueue&lt;Integer&gt; queue) throws InterruptedException { while(true){ System.out.println(&quot;消费者-----&quot;+queue.take()); Random random = new Random(47); Thread.currentThread().sleep(random.nextInt(1000)); } }}class Producer implements Runnable{ int total; LinkedBlockingQueue linkedBlockingQueue; int number; public Producer(int total, LinkedBlockingQueue linkedBlockingQueue, int number) { this.total = total; this.linkedBlockingQueue = linkedBlockingQueue; this.number = number; } public void run() { try { productStart(linkedBlockingQueue); } catch (InterruptedException e) { e.printStackTrace(); } } private void productStart(LinkedBlockingQueue&lt;Integer&gt; queue) throws InterruptedException { while(true){ System.out.println(number + &quot;生产者----&quot;+queue.size()); queue.put(number); } }} 非阻塞队列如果考虑在 Java 中使用 LinkedList 来实现阻塞队列，那么第一点，需要实现入队和出队的原子性，因为 LinkedList 是基于双向链表来实现的，所以在这里必须保证其原子性的操作。 在Java 中如果要实现对于链表的原子性操作，首先是加锁，考虑到加锁和释放锁导致的性能开销，决定使用可重入锁 有两种锁，一个是 synchronized，一个则是 ReentrantLock 锁的选型synchronized 的加锁逻辑依赖于 JVM ，同时也是支持可重入。并且JDK1.6 以后对其做了大量的优化，所以一般情况下可以直接用synchronized。 ReentrantLock 是 Java 语言自带的一种可重入锁，相较于 synchronized ，它含有公平锁和非公平锁两种模式，并且支持 Condition 。 在这里由于LinkedBlockingQueue 采用的 ReentrantLock ，所以在这里也是采用 ReentrantLock 阻塞以及唤醒当没有数据的时候，需要消费者阻塞，同时队列已经满了的情况下，需要生产者进行阻塞，而配合这些操作的就是将这些线程阻塞，在java 中可以通过 wait 和 notify 方法来进行阻塞和唤醒， 如果使用的是 ReentrantLock，也可以使用自己的 Condition。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public class NonBlockModel { static ReentrantLock reentrantLock = new ReentrantLock(); static Condition produceCondition = reentrantLock.newCondition(); static Condition consumerCondition = reentrantLock.newCondition(); static ThreadPoolExecutor product = new ThreadPoolExecutor(5,10,1000, TimeUnit.MICROSECONDS,new LinkedBlockingQueue&lt;Runnable&gt;(100)); static ThreadPoolExecutor consumer = new ThreadPoolExecutor(5,10,1000, TimeUnit.MICROSECONDS,new LinkedBlockingQueue&lt;Runnable&gt;(100)); static int total =10; public static void main(String[] args) throws InterruptedException { LinkedList linkedList = new LinkedList(); for(int i =0;i&lt; 5 ;i++){ product.submit(new NonBlockProducer(reentrantLock,linkedList,i,produceCondition,consumerCondition)); consumer.submit(new NonBlockConsumer(reentrantLock,linkedList,produceCondition,consumerCondition)); } Thread.currentThread().sleep(1000000); }}class NonBlockProducer implements Runnable{ ReentrantLock produceReentrantLock; LinkedList&lt;Integer&gt; linkedList; int number; Condition produceCondition; Condition consumerCondition; public NonBlockProducer(ReentrantLock produceReentrantLock, LinkedList&lt;Integer&gt; linkedList, int number, Condition produceCondition, Condition consumerCondition) { this.produceReentrantLock = produceReentrantLock; this.linkedList = linkedList; this.number = number; this.produceCondition = produceCondition; this.consumerCondition = consumerCondition; } @Override public void run() { while (true) { produceReentrantLock.lock(); try { if (10 == linkedList.size()) { System.out.println(&quot;队列已满，生产者被阻塞&quot; + number + &quot;--&quot; + Thread.currentThread().getName()); produceCondition.await(); } if(linkedList.size() +1 &lt; 10){ linkedList.push(number); System.out.println(&quot;添加元素&quot; + linkedList.size()); } consumerCondition.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { produceReentrantLock.unlock(); } } }}class NonBlockConsumer implements Runnable{ ReentrantLock consumerReentrantLock; LinkedList&lt;Integer&gt; linkedList; Condition produceCondition; Condition consumerCondition; public NonBlockConsumer(ReentrantLock consumerReentrantLock, LinkedList&lt;Integer&gt; linkedList, Condition produceCondition, Condition consumerCondition) { this.consumerReentrantLock = consumerReentrantLock; this.linkedList = linkedList; this.produceCondition = produceCondition; this.consumerCondition = consumerCondition; } @Override public void run() { while (true) { consumerReentrantLock.lock(); try { if (0 == linkedList.size()) { System.out.println(&quot;队列已空，消费者已阻塞&quot; + Thread.currentThread().getName()); consumerCondition.await(); } linkedList.pollLast(); System.out.println(&quot;移除元素&quot; + linkedList.size()); if(linkedList.size() &gt; 1){ consumerCondition.signal(); } produceCondition.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { consumerReentrantLock.unlock(); } } }}","link":"/2020/07/26/%E9%87%8D%E6%B8%A9%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"title":"阿里云SDK使用代理的一个坑","text":"由于项目中需要使用阿里云的短信平台，所以直接引用了最新的SDK，版本号为 4.5.1。但是由于机器在内网环境，如果需要访问外部网络的话，需要代理机器。于是去看下 阿里的SDK 官方文档，如何支持代理访问，于是找到以下内容： 坑就坑在这个文档里面的设置方法，设置了并没有什么用。于是自己研究了下这种设置为什么不生效。 System.setProperty这个命令和在启动参数中加 -DXXX=XXX 是一样的效果，例如： 12System.setProperty(&quot;http.proxyHost&quot;, &quot;127.0.0.1&quot;); System.setProperty(&quot;http.proxyPort&quot;, &quot;8888&quot;); 就等价于 -Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort = 8888，但是这种设置有一个限制，那就是只对 JDK 自带的 HttpURLConnection 有效，如下Demo： 当我们执行这段代码的时候，你会发现确实走了代理（可以本地随便设置一个IP加端口，你会发现一直卡在那里），那么既然这是有效的，就说明了阿里云的 Http 请求一定不是通过 JDK 的 HttpURLConnection 发送的。 Debug在阿里 doCommonResponse 的调用链路中，发现有一处代码 com.aliyuncs.DefaultAcsClient#doRealAction 如下： 此时阿里的SDK会通过 System.getenv(&quot;HTTPS_PROXY&quot;) 和 System.getenv(&quot;HTTP_PROXY&quot;) 来判断系统的环境中是否有如下两个变量。有的话就设置到 HttpClientConfig 中，没有的话则直接 return，既然我们系统环境里面没有这两个字段，那么肯定不会设置代理，于是继续往下跟代码。 最终发送 Http 请求的代码如下： 1234private IHttpClient httpClient;...省略相关代码// com.aliyuncs.DefaultAcsClient#doRealAction 第330行response = this.httpClient.syncInvoke(httpRequest); httpClient 最终对应的是 IHttpClient，它是阿里 SDK 里面的一个类。 IHttpClient首先看下它的结构。 在这边有两个实现类，ApacheHttpClient 是 apache 下面的一个包，而 CompatibleUrlConnClient 则是 JDK 自带的 http 请求类，那么阿里的SDK到底是初始化那一个SDK呢？ 首先查看官方的发送短信Demo： 1234... 省略相关代码IClientProfile profile = DefaultProfile.getProfile(&quot;cn-hangzhou&quot;, accessKeyId,accessKeySecret);IAcsClient acsClient = new DefaultAcsClient(profile); new DefaultAcsClient(profile) 这行代码最终会调用com.aliyuncs.DefaultAcsClient#DefaultAcsClient(com.aliyuncs.profile.IClientProfile, com.aliyuncs.auth.AlibabaCloudCredentialsProvider) 这个构造器。 而且在这一行代码里面会进行 HttpClientConfig 的初始化，如下所示： 12345public DefaultAcsClient(IClientProfile profile, AlibabaCloudCredentialsProvider credentialsProvider) {... 省略相关代码 this.httpClient = HttpClientFactory.buildClient(this.clientProfile);... 省略相关代码} 而在 HttpClientConfig.getDefault() 里面，最终会默认初始化一个 Apache 的 Httpclient。 12345public static HttpClientConfig getDefault() { HttpClientConfig config = new HttpClientConfig(); config.setClientType(HttpClientType.ApacheHttpClient); return config;}至此为什么官方文档上写的 System.setProperty 不生效的原因终于找到了。也就是说，如果你是按照官方文档来写的代码，那么你通过 System.setProperty 来设置代理是肯定不是生效的。 解决办法 将HTTPS_PROXY 或者 HTTP_PROXY 设置为系统环境变量（可以生效，但是不推荐） 在 buildClient 方法里面，可以发现只有当 HttpClientConfig 为空的情况下才会创建默认的 config，那么我们可以在 IClientProfile 里面，手动的将 HttpClientConfig 设置进去，从而避免创建默认的HttpClientConfig。 用 JDK 的 HttpURLConnection 发请求，通过 System.setProperty 设置代理。","link":"/2020/05/16/%E9%98%BF%E9%87%8C%E4%BA%91SDK%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91/"},{"title":"页面布局以及JS解析json的总结","text":"关于页面的水平垂直居中：页面的水平垂直居中布局的话目前就我这里了解的话是又两种方法，一种是盒子布局，一种是流式布局：盒子布局 关于Jquery解析JSON格式的问题JSON.parse()方法在使用这个方法解析Json格式的时候一直会报错，但是传入的值却又明明是JSON格式的，所以一直在排查： 1234a='{&quot;first&quot;:&quot;111&quot;}';b=&quot;{'first':'111'}&quot;;console.log(JSON.parse(a));console.log(JSON.parse(b)) //在这里会报错，提示Unexpected token ' in JSON at position 1 这就引出了在JS中使用JSON解析Json字符串的问题了，下图是解析字符串的顺序了，在下图可以看到解析是以&quot;为起点，然后是/，若在开始的位置没有发现这两个起始符号的话那么js会跳过这次解析直接到达末尾，然后报错，这也就解释了为什么在解析b的时候会直接抛出异常Unexpected token ' JSON.parse()方法这个方式是将一个Js的Object解析成一个Json格式的字符串，如下: 1234567var obj={ 'a':123, &quot;b&quot;:&quot;qq&quot;}console.log(JSON.stringify(obj))输出{&quot;a&quot;:123,&quot;b&quot;:&quot;qq&quot;} 在这里需要注意的是在这个对象中的话，单引号会被转成双引号。 JSON格式：JSON格式的规范如下，而使用JSON.parse()来解析单引号的内容的话是不符合规范的。 JSON 名称/值对JSON 数据的书写格式是：名称/值对。 名称/值对包括字段名称（在双引号中），后面写一个冒号，然后是值：","link":"/2018/03/28/%E9%A1%B5%E9%9D%A2%E5%B8%83%E5%B1%80%E4%BB%A5%E5%8F%8Ajquery%E8%A7%A3%E6%9E%90json%E7%9A%84%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"周记","slug":"周记","link":"/tags/%E5%91%A8%E8%AE%B0/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"web后端","slug":"web后端","link":"/tags/web%E5%90%8E%E7%AB%AF/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Json","slug":"Json","link":"/tags/Json/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"rabbitmq","slug":"rabbitmq","link":"/tags/rabbitmq/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"MySql","slug":"MySql","link":"/tags/MySql/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"plotly","slug":"plotly","link":"/tags/plotly/"},{"name":"Rabbitmq","slug":"Rabbitmq","link":"/tags/Rabbitmq/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"SSO","slug":"SSO","link":"/tags/SSO/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/tags/SpringCloud/"},{"name":"Springboot","slug":"Springboot","link":"/tags/Springboot/"},{"name":"Servlet","slug":"Servlet","link":"/tags/Servlet/"},{"name":"Mongo","slug":"Mongo","link":"/tags/Mongo/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"web前端","slug":"web前端","link":"/tags/web%E5%89%8D%E7%AB%AF/"},{"name":"css","slug":"css","link":"/tags/css/"},{"name":"Angular","slug":"Angular","link":"/tags/Angular/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"maven","slug":"maven","link":"/tags/maven/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"dubbo","slug":"dubbo","link":"/tags/dubbo/"},{"name":"java基础","slug":"java基础","link":"/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"mongo","slug":"mongo","link":"/tags/mongo/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"shiro","slug":"shiro","link":"/tags/shiro/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"MyBatis","slug":"MyBatis","link":"/tags/MyBatis/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"计算机基础","slug":"计算机基础","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"Leetcode","slug":"Leetcode","link":"/tags/Leetcode/"},{"name":"动态规划","slug":"动态规划","link":"/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"字符串","slug":"字符串","link":"/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"思考","slug":"思考","link":"/tags/%E6%80%9D%E8%80%83/"},{"name":"jquery","slug":"jquery","link":"/tags/jquery/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"Shiro","slug":"Shiro","link":"/tags/Shiro/"},{"name":"Jwt","slug":"Jwt","link":"/tags/Jwt/"},{"name":"个人记录","slug":"个人记录","link":"/tags/%E4%B8%AA%E4%BA%BA%E8%AE%B0%E5%BD%95/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"}],"categories":[{"name":"周记","slug":"周记","link":"/categories/%E5%91%A8%E8%AE%B0/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/categories/ElasticSearch/"},{"name":"web前端","slug":"web前端","link":"/categories/web%E5%89%8D%E7%AB%AF/"},{"name":"JDK","slug":"Java/JDK","link":"/categories/Java/JDK/"},{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Vue","slug":"web前端/Vue","link":"/categories/web%E5%89%8D%E7%AB%AF/Vue/"},{"name":"Json","slug":"Java/Json","link":"/categories/Java/Json/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"leetcode","slug":"算法/leetcode","link":"/categories/%E7%AE%97%E6%B3%95/leetcode/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"消息队列","slug":"消息队列","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"SpringBoot","slug":"Java/SpringBoot","link":"/categories/Java/SpringBoot/"},{"name":"SpringCloud","slug":"Java/SpringCloud","link":"/categories/Java/SpringCloud/"},{"name":"MySql","slug":"数据库/MySql","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySql/"},{"name":"Spring","slug":"Java/Spring","link":"/categories/Java/Spring/"},{"name":"线程","slug":"Java/线程","link":"/categories/Java/%E7%BA%BF%E7%A8%8B/"},{"name":"回溯","slug":"算法/回溯","link":"/categories/%E7%AE%97%E6%B3%95/%E5%9B%9E%E6%BA%AF/"},{"name":"Angular","slug":"web前端/Angular","link":"/categories/web%E5%89%8D%E7%AB%AF/Angular/"},{"name":"Rabbitmq","slug":"消息队列/Rabbitmq","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Rabbitmq/"},{"name":"css","slug":"web前端/css","link":"/categories/web%E5%89%8D%E7%AB%AF/css/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"Redis","slug":"数据库/Redis","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"},{"name":"dubbo","slug":"dubbo","link":"/categories/dubbo/"},{"name":"JVM","slug":"Java/JVM","link":"/categories/Java/JVM/"},{"name":"layui","slug":"web前端/layui","link":"/categories/web%E5%89%8D%E7%AB%AF/layui/"},{"name":"第三方组件","slug":"第三方组件","link":"/categories/%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BB%84%E4%BB%B6/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"设计模式","slug":"Java/设计模式","link":"/categories/Java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"集合","slug":"Java/集合","link":"/categories/Java/%E9%9B%86%E5%90%88/"},{"name":"计算机基础","slug":"计算机基础","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"动态规划","slug":"算法/动态规划","link":"/categories/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"思考","slug":"思考","link":"/categories/%E6%80%9D%E8%80%83/"},{"name":"设计模式","slug":"设计模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Shiro","slug":"Java/Shiro","link":"/categories/Java/Shiro/"},{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"Netty","slug":"第三方组件/Netty","link":"/categories/%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BB%84%E4%BB%B6/Netty/"},{"name":"Lambda","slug":"Java/Lambda","link":"/categories/Java/Lambda/"},{"name":"nginx","slug":"第三方组件/nginx","link":"/categories/%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BB%84%E4%BB%B6/nginx/"},{"name":"计算机基础","slug":"Java/计算机基础","link":"/categories/Java/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"阿里云","slug":"阿里云","link":"/categories/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"MyBatis","slug":"第三方组件/MyBatis","link":"/categories/%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BB%84%E4%BB%B6/MyBatis/"},{"name":"json","slug":"web前端/json","link":"/categories/web%E5%89%8D%E7%AB%AF/json/"}]}